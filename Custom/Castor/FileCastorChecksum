#!/usr/bin/env perl

##H Add checksums for files that don't have the information.
##H
##H This agent is an export-side agent that ensures the files have a
##H a checksum, ideally priort to transfer but we do not actually
##H guarantee that.  The agent checks for staged-in files and if the
##H file checksum is -1, it calculates the checksum and updates the
##H database.  The agent doesn't require great disk or cpu capacity,
##H but it will use all the network bandwidth it can get -- set the
##H number of parallel "cksum" jobs according to available network.
##H
##H Usage:
##H   FileCastorChecksum
##H      -state DIRECTORY -node NAME -db FILE[:SECTION] [-wait SECS]
##H      -pfnquery COMMAND[,ARGS...] [-rfcat COMMAND[,ARGS...]]
##H      [-jobs NJOBS]
##H
##H -state     agent state directory
##H -node      the node where this agent runs
##H -db        database connection configuration parameter file
##H -wait      time to wait in seconds between work scans
##H -pfnquery  command to query pfns from catalogue
##H -rfcat     command to replace "rfcat" (e.g. "cat")
##H -jobs      number of parallel subjobs to run (default: 4)

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
use UtilsHelp;
my %args = (NJOBS => 4, CAT_PROG => ["rfcat"]);
while (scalar @ARGV)
{
    if ($ARGV[0] eq '-wait' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WAITTIME} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-state' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DROPDIR} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-db' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBCONFIG} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-node' && scalar @ARGV > 1)
    { shift (@ARGV); $args{MYNODE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-pfnquery' && scalar @ARGV > 1)
    { shift (@ARGV); @{$args{PFN_QUERY}} = split (/,/, shift(@ARGV)); }
    elsif ($ARGV[0] eq '-jobs' && scalar @ARGV > 1)
    { shift (@ARGV); $args{NJOBS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-rfcat' && scalar @ARGV > 1)
    { shift (@ARGV); @{$args{CAT_PROG}} = [ split (/,/, shift(@ARGV)) ]; }
    elsif ($ARGV[0] eq '-h')
    { &usage(); }
    else
    { last; }
}

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG} || !$args{MYNODE}
    || !$args{PFN_QUERY} || !$args{NJOBS} || !$args{CAT_PROG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FileCastorChecksum (%args))->process();

######################################################################
# Routines specific to the agent.
package FileCastorChecksum; use strict; use warnings; use base 'UtilsAgent';
use File::Path;
use Data::Dumper;
use UtilsCommand;
use UtilsLogging;
use UtilsTiming;
use UtilsCatalogue;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
	  	  MYNODE => undef,		# My TMDB node name
	  	  PFN_QUERY => undef,		# Program to query PFNs
	  	  CAT_PROG => undef,		# Program to "cat" the file
	  	  AGENTID => "Checksum");	# Identity for activity logs
    my %args = (@_);
    map { $self->{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Start processing a file
sub fileStart
{
    my ($self, $guid, $filesize) = @_;

    # Get the PFN for the GUID.
    my $now = &mytimeofday();
    my $pfn = &guidToPFN ($guid, "direct", "local", @{$self->{PFN_QUERY}});
    do { &alert ("failed to look up pfn for $guid"); return 0; } if ! $pfn;

    my $dir = "$self->{WORKDIR}/$guid";
    do { &alert ("$dir: exists already"); return 0 } if -d $dir;
    eval { &mkpath ($dir); };
    do { chomp ($@); &alert ("$dir: cannot create: $@"); return 0 } if $@;

    my $file = {
	GUID		=> $guid,
	PFN		=> $pfn,
	WORKDIR		=> $dir,
	FILESIZE	=> $filesize,
	TIME_STARTED	=> $now
    };

    $self->{PENDING}{$guid} = 0;
    $self->addJob (sub { $self->fileChecksummed ($file, "$dir/checksum", @_) },
	{ }, "sh", "-c", "@{$self->{CAT_PROG}} $pfn | cksum > $dir/checksum");
    return 1;
}

# Respond to completed checksum.  Collect file checksum and size.
sub fileChecksummed
{
    my ($self, $file, $output_file, $job) = @_;
    my $output = &input ($output_file);
    unlink ($output_file);
    chomp ($output);

    if ($job->{STATUS}) {
	$file->{FAILURE} = "exit code $job->{STATUS} from @{$job->{CMD}}";
    } elsif (! $output) {
	$file->{FAILURE} = "no output from @{$job->{CMD}}";
    } elsif ($output !~ /^(\d+) (\d+)$/) {
	$file->{FAILURE} = "malformed output from @{$job->{CMD}}";
    } else {
	$file->{CHECKSUM} = $1;
	$file->{NEWSIZE} = $2;
    }
    $self->fileComplete ($file);
}

# Update checksum in the database if the file was succefully processed.
sub fileComplete
{
    my ($self, $file) = @_;

    # Dig out the report.
    my $pfn = $file->{PFN};
    my $guid = $file->{GUID};
    my $success = $file->{FAILURE} ? 1 : 0;
    my $checksum = $file->{CHECKSUM};
    my $oldsize = $file->{FILESIZE};
    my $newsize = $file->{NEWSIZE};
    my $mynode = $self->{MYNODE};
    my $dbh = undef;
    my $op;

    # Update the database as appropriate.
    eval
    {
	# Check preceding failures
	$op = "calculate checksum";
	die "$file->{FAILURE}\n" if $file->{FAILURE};

	# Nuke working directory
	$op = "remove working directory";
	&rmtree ($file->{WORKDIR});

	# Check file size matches
	$op = "match file size";
	die "$oldsize (database) != $newsize (calculated)\n" if $oldsize != $newsize;

	# Ok, it's worth talking to database.  Get connected.
	$op = "connect to the database";
	$dbh = &connectToDatabase ($self, 0) or die "failed to connect\n";

	# Check for an existing checksum; if there is one, make sure
	# it's the same we got this time.  Add if there isn't one.
	$op = "match old checksum";
	my $oldsum = &dbexec($dbh, qq{
		select checksum from t_file where guid = :guid},
		":guid" => $guid)->fetchrow_array();
	die "$oldsum (database) != $checksum (calculated)\n"
	    if defined $oldsum && $oldsum != -1 && $oldsum ne $checksum;

	$op = "update checksum";
	&dbexec($dbh, qq{
	  update t_file set checksum = :checksum where guid = :guid},
	  ":guid" => $guid, ":checksum" => $checksum);

  	$op = "commit";
	$dbh->commit();
    };
    do { chomp ($@);
	 &alert ("failed to $op for $guid ($pfn): $@");
	 $file->{FAILURE} = $op; $success = 1;
	 eval { $dbh->rollback() } if $dbh } if $@;

    # Disconnect from database
    &disconnectFromDatabase ($self, $dbh);

    # Log transfer delay stats
    my $now = &mytimeofday ();
    &logmsg ("xstats: $guid $mynode $success "
	     . sprintf ('%.2f', $now - $file->{TIME_STARTED})
	     . " $file->{FILESIZE}");

     # Mark this file no longer known to us, except if there was a
     # failure, remember it for a while longer so we back off on bad
     # files for a while.  We remove memory of such files in &idle().
     if (! $file->{FAILURE}) {
	 # Success, remove memory
	 delete $self->{PENDING}{$guid};
     } else {
	 # Failed, ignore for at least an hour.
	 $self->{PENDING}{$guid} = $now + 3600 + rand(3600);
     }
}

# Get N guids that should be checksummed.  Returns a list of arrays
# with members GUID, FILESIZE, TIMESTAMP, CATALOGUE, HOST_KEY.
sub nextFiles
{
    my ($self, $dbh, $n) = @_;
    my @result = ();
    eval
    {
	# Select files that need to be checksummed.  This is everything
	# staged in at this node but without checksum.
	my $stmt = &dbexec ($dbh, qq{
		select rs.guid, f.filesize
		from t_replica_state rs
		join t_file f on f.guid = rs.guid
		where rs.node = :node
	  	  and rs.state = 1
	  	  and f.checksum = -1},
		":node" => $self->{MYNODE});
	while (my $row = $stmt->fetchrow_arrayref())
	{
	    next if exists $self->{PENDING}{$row->[0]};
	    last if ! $n--;
	    push (@result, [ @$row ]);
	}
    };
    do { chomp ($@); &alert ("failed to select next files: $@") } if $@;
    return @result;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and start their processing.
sub idle
{
    my ($self, @pending) = @_;
    my $now = time();
    my $dbh = undef;
    my $more = 0;

    eval
    {
	$dbh = &connectToDatabase ($self) or die "failed to connect\n";
        # FIXME: Pick up and process messages to me

	# If there are no pending jobs, remove excess work directories
	if (! @{$self->{JOBS}})
	{
	    my $workdir = $self->{WORKDIR};
	    &rmtree ([ <$workdir/*> ]);
	}

	# Purge memory of bad files that have cooled long enough.
	foreach my $guid (keys %{$self->{PENDING}})
	{
	    next if ! $self->{PENDING}{$guid}; # Skip if being checksummed
	    delete $self->{PENDING}{$guid} if $self->{PENDING}{$guid} < $now;
	}

	# Pick up new work
	my $files = $self->{FILES} ||= [];
	my $maxload = $self->{NJOBS} * 100;
        while (scalar @{$self->{JOBS}} < $self->{NJOBS})
        {
	    # Get more files if necessary
	    @$files = $self->nextFiles ($dbh, $maxload) if ! @$files;
	    last if ! @$files;

	    # Start processing this one if possible
	    last if ! ($more = $self->fileStart (@{shift @$files}));

	    # Move existing jobs
	    $self->pumpJobs ();
        }
    };
    do { chomp ($@); &alert("database error: $@");
	 eval { $dbh->rollback() } if $dbh } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Keep working, resting a bit at a time.  If we have available
    # job slots and more files to work on, break out of the loop to
    # initiate new jobs.  Otherwise if we have exhausted available
    # files, sleep here for a moment.
    my $target = time() + $self->{WAITTIME};
    while (time() < $target)
    {
        $self->maybeStop ();
        $self->pumpJobs ();
        last if $more && @{$self->{FILES}} && scalar @{$self->{JOBS}} < $self->{NJOBS};
        select (undef, undef, undef, .1);
    }
}
