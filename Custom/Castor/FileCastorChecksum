#!/usr/bin/env perl

## Add checksums for files that don't have the information.
##
## This agent works in tandem with FileCastorExport.  The former
## manages the file export castor stage pool based on file requests
## from downstream transfer agents.  Once the files are brought on
## disk, the export agent marks them as requiring checksum using a
## special node-specific state.  This agent looks for files in such
## a state, and if they don't have a checksum, calculates and adds
## it to the database.  Only once the checksum is available we mark
## the file available for transfer.
##
## This agent periodically checks TMDB for files in "needs checksum"
## state.  For each such file it creates a job packet for an internal
## slave that does the actual checksumming.  While the file is being
## checksummed, we mark it as "in checksum" state so several checksum
## agents can be ran in parallel on different computers.
##
## Note that inbox of this agent is used solely by the internal
## slaves.  We create job packets as drops for the slaves, which
## then pass the drops to our inbox when they are done.  The
## main agent then updates the database according to the status.
## The program itself is used both as a slave and a master, but
## there are two separate agents.

BEGIN {
  use strict; use warnings;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
$ENV{STAGE_HOST} ||= "stagecmsprod";	# Castor stage host
$ENV{STAGE_POOL} ||= "cms_prod2";	# Castor stage pool
my %args = (DBITYPE => "Oracle", NJOBS => 1);
while (scalar @ARGV)
{
    if ($ARGV[0] eq '-stagehost' && scalar @ARGV > 1)
    { shift (@ARGV); $ENV{STAGE_HOST} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-stagepool' && scalar @ARGV > 1)
    { shift (@ARGV); $ENV{STAGE_POOL} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-wait' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WAITTIME} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-state' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DROPDIR} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-db' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBNAME} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbi' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBITYPE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbuser' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBUSER} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbpass' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBPASS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-node' && scalar @ARGV > 1)
    { shift (@ARGV); $args{MYNODE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-prefix' && scalar @ARGV > 1)
    { shift (@ARGV); $args{RM_PATH_PREFIX} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-jobs' && scalar @ARGV > 1)
    { shift (@ARGV); $args{NJOBS} = shift(@ARGV); }
    else
    { last; }
}

if (scalar @ARGV || !$args{DROPDIR} || !$args{DBNAME} || !$args{DBUSER}
    || !$args{DBPASS} || !$args{DBITYPE} || !$args{MYNODE}
    || !$args{RM_PATH_PREFIX} || !$args{NJOBS})
{
    print STDERR
	"usage: $me -node TMDB-NODE -prefix PATH-PREFIX-TO-REMOVE\n",
	"    -db NAME -dbuser USER -dbpass PASSWORD [-dbitype TYPE]\n",
	"    -state IN-DROP-BOX [-jobs NUMBER] [-wait SECS-TO-WAIT]\n",
	"    [-stagehost STAGE-HOST] [-stagepool POOL]\n";
    exit (1);
}

my $agent = new FileCastorChecksum (%args);
# Recapture interrupt signal, oracle swallows it.
$SIG{INT} = sub { system "touch $agent->{STOPFLAG}"; $agent->maybeStop (); };
$agent->process ();

######################################################################
# Routines specific to the agent.
package FileCastorChecksum; use strict; use warnings; use base 'UtilsAgent';
use File::Path;
use Data::Dumper;
use UtilsCommand;
use UtilsLogging;
use UtilsTiming;
use UtilsCatalogue;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBITYPE => undef,		# Database driver binding
    		  DBNAME => undef,		# Database name
	  	  DBUSER => undef,		# Database user name
	  	  DBPASS => undef,		# Database user password
	  	  MYNODE => undef,		# My TMDB node name
	  	  RM_PATH_PREFIX => undef,	# Prefix to remove from PFNs
	  	  AGENTID => "Checksum");	# Identity for activity logs
    my %args = (@_);
    map { $self->{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Start processing a file
sub fileStart
{
    my ($self, $guid, $filesize, $assigned, $catalogue, $hostkey) = @_;

    # Get the PFN for the GUID.
    my $now = &mytimeofday();
    my $pfn = &guidToPFN ($guid, $catalogue, $hostkey);
    do { &alert ("failed to look up pfn for $guid"); return 0; } if ! $pfn;
    $pfn =~ s|$self->{RM_PATH_PREFIX}||;

    my $lfn = $pfn; $lfn =~ s|.*/||;
    my $dir = "$self->{WORKDIR}/$guid";
    do { &alert ("$dir: exists already"); return 0 } if -d $dir;
    eval { &mkpath ($dir); };
    do { &alert ("$dir: cannot create: $@"); return 0 } if $@;

    my $file = {
	GUID		=> $guid,
	PFN		=> $pfn,
	LFN		=> $lfn,
	WORKDIR		=> $dir,
	FILESIZE	=> $filesize,
	TIME_ASSIGNED	=> $assigned,
	TIME_STARTED	=> $now
    };

    $self->{PENDING}{$guid} = 0;
    $self->addJob (sub { $self->fileChecksummed ($file, @_) },
	{ OUTPUT_FILE => "$file->{WORKDIR}/checksum" },
	"sh", "-c", "rfcat $pfn | cksum > $file->{WORKDIR}/checksum");
    return 1;
}

# Respond to completed checksum.  Collect file checksum and size.
sub fileChecksummed
{
    my ($self, $file, $job) = @_;
    my $output = &input ($job->{OUTPUT_FILE});
    unlink ($job->{OUTPUT_FILE});
    chomp ($output);

    if ($job->{STATUS}) {
	$file->{FAILURE} = "exit code $job->{STATUS} from @{$job->{CMD}}";
    } elsif (! $output) {
	$file->{FAILURE} = "no output from @{$job->{CMD}}";
    } elsif ($output !~ /^(\d+) (\d+)$/) {
	$file->{FAILURE} = "malformed output from @{$job->{CMD}}";
    } else {
	$file->{CHECKSUM} = $1;
	$file->{NEWSIZE} = $2;
    }
    $self->fileComplete ($file);
}

# Update checksum in the database if the file was succefully processed.
sub fileComplete
{
    my ($self, $file) = @_;

    # Dig out the report.
    my $guid = $file->{GUID};
    my $success = $file->{FAILURE} ? 1 : 0;
    my $checksum = $file->{CHECKSUM};
    my $oldsize = $file->{FILESIZE};
    my $newsize = $file->{NEWSIZE};
    my $mynode = $self->{MYNODE};
    my $now = time();
    my $dbh = undef;
    my $op;

    # Update the database as appropriate.
    eval
    {
	# Check preceding failures
	$op = "calculate checksum";
	die $file->{FAILURE} if $file->{FAILURE};

	# Nuke working directory
	$op = "remove working directory";
	&rmtree ($file->{WORKDIR});

	# Check file size matches
	$op = "match file size";
	die "$oldsize != $newsize" if $oldsize != $newsize;

	# Ok, it's worth talking to database.  Get connected.
	$op = "connect to the database";
	$dbh = &connectToDatabase ($self, 0) or die "failed to connect";

	# Check for an existing checksum; if there is one, make sure
	# it's the same we got this time.  Add if there isn't one.
	$op = "match old checksum";
	my $oldsum = (&dbexec($dbh, qq{
		select value from t_replica_metadata
		where guid = :guid and attribute = 'checksum'},
		":guid" => $guid))[0]->fetchrow_array();
	die "$oldsum != $checksum" if defined $oldsum && $oldsum ne $checksum;

	$op = "update checksum";
	&dbexec($dbh, qq{
	  insert into t_replica_metadata (guid, attribute, value)
	    values (:guid, 'checksum', :checksum)},
	  ":guid" => $guid, ":checksum" => $checksum);

  	$op = "commit";
	$dbh->commit();
    };
    do { &alert ("failed to $op for $guid: $@"); $dbh->rollback() if $dbh } if $@;

    # Disconnect from database
    $dbh->disconnect() if $dbh;
    undef $dbh;

    # Log transfer delay stats
    $now = &mytimeofday ();
    &logmsg ("xstats: $guid $mynode $success "
	     . sprintf ('%.2f %.2f',
	    		$now - $file->{TIME_ASSIGNED},
	    		$now - $file->{TIME_STARTED})
	     . " $file->{FILESIZE}");

     # Mark this file no longer known to us, except if there was a
     # failure, remember it for a while longer so we back off on bad
     # files for a while.  We remove memory of such files in &idle().
     if (! $file->{FAILURE}) {
	 # Success, remove memory
	 delete $self->{PENDING}{$guid};
     } else {
	 # Failed, ignore for 5-10 minutes.
	 $self->{PENDING}{$guid} = $now + 300 + rand(300);
     }
}

# Get N guids that should be checksummed.  Returns a list of arrays
# with members GUID, FILESIZE, TIMESTAMP, CATALOGUE, HOST_KEY.
sub nextFiles
{
    my ($self, $dbh, $n) = @_;
    my @result = ();
    eval
    {
	# Select files that need to be checksummed.  This is everything
	# staged in at this node but without checksum.
	my $stmt = &dbexec ($dbh, qq{
		select rs.guid,
		       m2.value,
		       rs.time_stamp,
		       n.catalogue_contact,
		       n.host_string
		from t_replica_state rs
		left join t_replica_metadata m1
		  on m1.guid = rs.guid and m1.attribute = 'checksum'
		left join t_replica_metadata m2
		  on m2.guid = rs.guid and m2.attribute = 'filesize'
		left join t_nodes n
		  on n.node_name = rs.node
		where rs.node = :node
	  	  and rs.state = 1
	  	  and m1.value is null},
		":node" => $self->{MYNODE});
	while (my $row = $stmt->fetchrow_arrayref())
	{
	    next if exists $self->{PENDING}{$row->[0]};
	    last if ! $n--;
	    push (@result, [ @$row ]);
	}
    };
    &alert ("failed to select next files: $@") if $@;
    return @result;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and start their processing.
sub idle
{
    my ($self, @pending) = @_;
    my $now = time();
    my $dbh = undef;
    my $more = 0;

    eval
    {
	$dbh = &connectToDatabase ($self) or die "failed to connect";
        # FIXME: Pick up and process messages to me

	# Purge memory of bad files that have cooled long enough.
	my $pending = $self->{PENDING};
	map { delete $pending->{$_} }
	  grep ($pending->{$_} && $pending->{$_} < $now, keys %$pending);

	# Pick up new work
	my $files = $self->{FILES} ||= [];
	my $maxload = $self->{NJOBS} * 10;
        while (scalar @{$self->{JOBS}} < $self->{NJOBS})
        {
	    # Get more files if necessary
	    @$files = $self->nextFiles ($dbh, $maxload) if ! @$files;
	    last if ! @$files;

	    # Start processing this one if possible
	    last if ! ($more = $self->fileStart (@{shift @$files}));

	    # Move existing jobs
	    $self->pumpJobs ();
        }
    };
    do { &alert("database error: $@"); $dbh->rollback() if $dbh } if $@;

    # Disconnect from the database
    $dbh->disconnect() if $dbh;
    undef $dbh;

    # Keep working, resting a bit at a time.  If we have available
    # job slots and more files to work on, break out of the loop to
    # initiate new jobs.  Otherwise if we have exhausted available
    # files, sleep here for a moment.
    my $target = time() + $self->{WAITTIME};
    while (time() < $target)
    {
        $self->maybeStop ();
        $self->pumpJobs ();
        last if $more && @{$self->{FILES}} && scalar @{$self->{JOBS}} < $self->{NJOBS};
        select (undef, undef, undef, .1);
    }
}
