#! /usr/bin/env perl

##H Deactivate blocks when file-level information is not needed.
##H
##H Files belong to blocks; when the file replicas are no longer needed
##H they are collapsed to block replicas, which remember entire sets of
##H files.  This agent monitors completeness of blocks and triggers
##H removal of the file-level information in TMDB.
##H
##H Usage:
##H   BlockDeactivate
##H      -state DIRECTORY -node NAME -db FILE[:SECTION] [-wait SECS]
##H      [-holdoff SECS]
##H
##H -state     agent state directory
##H -node      the node where this agent runs
##H -db        database connection configuration parameter file
##H -wait      time to wait in seconds between work scans
##H -holdoff   time lag in seconds before deactivation (default: 3 days)

BEGIN {
  use warnings; use strict;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
use UtilsHelp;
use Getopt::Long;
my %args = (WAITTIME => 60);
&GetOptions ("state=s"		=> \$args{DROPDIR},
	     "node=s"           => \$args{MYNODE},
	     "db=s"		=> \$args{DBCONFIG},
	     "wait=f"		=> \$args{WAITTIME},
	     "help|h"		=> sub { &usage() },
	     "holdoff=f"        => \$args{HOLDOFF});

if (@ARGV || !$args{MYNODE} || !$args{DROPDIR} || !$args{DBCONFIG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new BlockDeactivate (%args))->process();

################################################
package BlockDeactivate; use strict; use warnings; use base 'UtilsAgent';
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My TMDB node
	  	  AGENTID => "BlockDeactivate", # Identity for activity logs
		  HOLDOFF => 3*86400);          # Hold-off time for pruning

    my %args = (@_);
    map { $self->{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Run the main loop of this agent.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    eval
    {
	$dbh = &connectToDatabase ($self) or die "failed to connect";

	# Guarantee full consistency.  We need to ensure that a) this
	# procedure is aborted if someone goes and makes block open
	# again and adds more files, b) we delete exactly as many
	# t_replica_state rows as we planned to.
	&dbexec ($dbh, q{set transaction isolation level serializable});

	# Deactivate complete blocks.  Get all blocks whose all replicas
	# are complete and active.  First of all, ignore blocks where
	# replicas are already inactive.  Secondly, ignore all blocks
	# which have been touched "recently" to allow things to settle.
	#
	# Consider remaining (active) blcok replicas.  If all replicas
	# of a block have as many files as the block has, deactive the
	# block (and all replicas).  We also require for extra safety
	# that there can be no files in transfer.
	#
	# We do *not* require that dest_files = node_files, as in nodes
	# which are file sources dest_files is usually zero, and we
	# still want to deactivate.  This should be relatively safe --
	# the only way we can deactivate a block on intermediate node
	# is if a) the entire block has already reached all current
	# destination nodes, b) either all or none of the files have
	# been removed in intermediate nodes, and c) at least several
	# days has passed like this.  It is unlikely that all these
	# criteria will be met simultaneously, and in any case the fix
	# is easy: reactivate the block.
	#
	# Do just one at a time to avoid overflowing undo segmens and
	# running the risk this whole transaction will fail.  This is
	# because we require transactional integrity for the entire
	# select/delete/update (see above).  It's OK to do work
	# incrementally here.
	my $qblocks = &dbexec ($dbh, qq{
	    select b.name
	    from t_block b
	    where b.isopen = 0
	      and b.timestamp < :limit
	      and (b.files, b.bytes, 0, 1, 1) = all
      	          (select br.node_files, br.node_bytes, br.xfer_files,
			  sign(:limit - br.last_update), br.isactive
		   from t_block_replica br
		   where br.name = b.name)},
	    ":limit" => &mytimeofday () - $self->{HOLDOFF});
        if (my ($name) = $qblocks->fetchrow())
	{
	    # Deactivate.  The last "update" has no effect except it
	    # guarantees the transasction will fail if the block is
	    # touched concurrently (e.g. to reopen to add new files).
	    my ($dr, $nr) = &dbexec ($dbh, qq{
		delete from t_replica_state where guid in
		(select guid from t_file where inblock = :block)},
		":block" => $name);
	    my ($db, $nb) = &dbexec ($dbh, qq{
		update t_block_replica set isactive = 0 where name = :block},
		":block" => $name);
	    &dbexec ($dbh, qq{update t_block set isopen = 0 where name = :block},
		":block" => $name);

	    &logmsg ("deactivated $name: $nr file replicas, $nb block replicas");
        }

	# Commit everything.  See above for comments why we don't
	# want to commit partially each block.
	$dbh->commit ();
    };
    do { &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Have a little nap
    $self->nap ($self->{WAITTIME});
}

sub removeInTMDB {
    my ($self, $dbh, $collection) = @_;
    my ($dataset, $owner) = split(':',$collection);

# we need the guids for the files belonging to the selected dataset-owner pairs
    my @guids = &getGUIDs($dbh, $dataset, $owner);
    
     foreach my $guid (@guids) {
# let's remove all t_transfer and t_replica info for the chosen guids
	&dbexec ($dbh, qq{
		delete from t_transfer_state where
	        guid = :guid},
	        ":guid"=>$guid);
	    
	&dbexec ($dbh, qq{
		delete from t_replica_state where
	        guid = :guid},
	        ":guid"=>$guid);
	$dbh->commit();
    }
	
}

sub getGUIDs {
    my ($dbh, $dataset, $owner) = @_;
    my @guids = ();

    # fetch GUIDs from TMDB for given dataset and owner
    my $guids_h = &dbexec ($dbh, qq{
	select f.guid from t_dsb_fileid f, t_dsb_dataset d
	where  d.dataset=:dataset and d.owner=:owner and d.id=f.id},
	":dataset"=>$dataset, ":owner"=>$owner);
    
    while (my ($guid) = $guids_h->fetchrow() ) {
	push (@guids, $guid);
    }
    return @guids;
}
