#!/usr/bin/env perl

##H Published completed received blocks to the local PubDB.
##H
##H This agent receives drops from BlockNotify and processes them to
##H ensure safe publishing into the PubDB database.  Both CMS Glide
##H publish service and the validation tools are supported.
##H
##H Usage:
##H   BlockPublish
##H      -state DIRECTORY -next DIRECTORY [-wait SECS]
##H      -script CMD[,ARGS...] -lfnquery CMD[,ARGS...]
##H      -published DIRECTORY
##H
##H -state     agent state directory
##H -next      state directory for the next agent; can be repeated
##H -wait      time to wait in seconds between work scans
##H -script    the publishing script to be invoked
##H -lfnquery  command to query lfns from catalogue
##H -published history directory for already published datasets

BEGIN {
  use warnings; use strict; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
use UtilsHelp;
use Getopt::Long;
my %args = (WAITTIME => 3600);
&GetOptions ("state=s"		=> \$args{DROPDIR},
	     "published=s"      => \$args{PUBLISHED},
	     "lfnquery=s"       => \$args{LFN_QUERY},
	     "script=s"         => \$args{SCRIPT},
	     "wait=f"		=> \$args{WAITTIME},
     	     "help|h"		=> sub { &usage() });

$args{SCRIPT} = [ split(/,/, $args{SCRIPT}) ] if $args{SCRIPT};
$args{LFN_QUERY} = [ split(/,/, $args{LFN_QUERY}) ] if $args{LFN_QUERY};
if (!$args{DROPDIR} || !$args{PUBLISHED} || !$args{LFN_QUERY} || !$args{SCRIPT})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new BlockPublish (%args))->process();

################################################
package BlockPublish; use strict; use warnings; use base 'UtilsAgent';
use File::Path;
use UtilsCommand;
use UtilsLogging;
use UtilsTiming;
use UtilsCatalogue;
use UtilsNet;
use UtilsTR;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (MYNODE     => undef,          # My TMDB node name
		  SCRIPT     => undef,          # path to script for publishing
		  LFN_QUERY  => undef,		# LFN query script
		  PUBLISHED  => undef);         # dir with treated owner/datasets

    my %args = (@_);
    map { $self->{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

sub processDrop
{
    my ($self, $drop) = @_;

    # Sanity checking
    return if (! $self->inspectDrop ($drop));
    delete $self->{BAD}{$drop};
    &timeStart($self->{STARTTIME});

    # Read back file information
    my $dropdir = "$self->{WORKDIR}/$drop";
    my $block = do { no strict "vars"; eval &input ("$dropdir/packet") };
    if ($@ || !$block || !$block->{DATASET} || !$block->{OWNER} || !$block->{FILES})
    {
	&alert ("corrupt packet in $drop");
	$self->markBad ($drop);
	return;
    }

    # Verify consistency of the packet, then publish.  We mark the
    # drop bad before going anywhere because publication procedure is
    # not atomic -- one has to clean up by hand if this is terminated
    # for any reason.  However this agent will normally be fed the
    # data several times (from block monitoring -- but is that also
    # true if this is used for publishing produced data?), so we can
    # in general expect to come around another time and finish the job
    # after a human has cleaned up the mess.

    my $start = &mytimeofday();
    return if ! $self->checkConsistency ($drop, $block);
    my $publishreturn = $self->publishBlock ($drop, $block);
    &touch ("$dropdir/done");

    # Log transfer delay stats
    my $delta = &mytimeofday() - $start;
    &logmsg ("xstats: $block->{OWNER}/$block->{DATASET} " . sprintf('%.2f', $delta));
    # distinguish between successful, failed, postponed and "nothing to do" rounds
    if ($publishreturn == 10)
    {
	&logmsg("stats: $drop @{[&formatElapsedTime($self->{STARTTIME})]} already tried");
    } elsif ($publishreturn == 20)
    {
	&logmsg("stats: $drop @{[&formatElapsedTime($self->{STARTTIME})]} failed !!");
    } elsif ($publishreturn == 30)
    {
	&logmsg("stats: $drop @{[&formatElapsedTime($self->{STARTTIME})]} postponed");
    } else
    {
	&logmsg("stats: $drop @{[&formatElapsedTime($self->{STARTTIME})]} success");
    }

    $self->relayDrop ($drop);
}


# Check that the number of files registered in local catalogue match
# the number of files in the block. Returns zero on failure, non-zero
# if the catalogue is consistent.
# The LFN query needs to be provided as argument to the agent
# Example: FClistLFN,-u,<POOL_CATALOG>,-q
sub checkConsistency
{
    my ($self, $drop, $block) = @_;
    my $cmd = "@{$self->{LFN_QUERY}}"
	. " \"dataset='$block->{DATASET}' and owner='$block->{OWNER}'\"";
    my @lfns = qx($cmd);
    my $zipmembers = grep($_ =~ m|Zipped|,@lfns);

    my $filesincatalog = 0;
    if ($zipmembers)
    {
	$filesincatalog = $zipmembers;
    } else {
	$filesincatalog = scalar @lfns;
    }


    if ($filesincatalog != $block->{FILES})
    {
	&alert("$drop: $block->{OWNER}/$block->{DATASET}: expected $block->{FILES}"
	       . " files, found $filesincatalog in catalogue");
	return 0;
    }
    return 1;
}

# Get dataset pre-requisite information.
# this information is gathered from RefDB. It contains the input owner
# and PU owner/dataset needed fo publishing Digi datasets
sub datasetRequirements
{
    my ($owner, $dataset) = @_;
    my @pending = ([ $owner, $dataset ]);
    my @history = ();
    while (@pending)
    {
	# FIXME: This should use CollectionTreeAndPU; see UtilsDBS.pm.
	my %info = ();
	my ($owner, $dataset) = @{shift (@pending)};
	my $data = &getURL ("http://cmsdoc.cern.ch/cms/production/www/cgi/data/"
			    ."AnaInfo.php?DatasetName=$dataset&OwnerName=$owner");
	die "no run data for $owner/$dataset\n" if ! $data;
	die "bad run data for $owner/$dataset\n" if $data =~ /ERROR.*SELECT.*FROM/s;
	foreach my $row (split("\n", $data))
	{
	    if ($row =~ /^(\S+)=(.*)/) {
		$info{$1} = $2;
	    }
	}

	if ($info{InputOwnerName})
	{
	    push (@history, [ $info{InputOwnerName}, $info{DatasetName} ]);
	    push (@pending, [ $info{InputOwnerName}, $info{DatasetName} ]);
	}

	if ($info{PUDatasetName} && $info{PUOwnerName})
	{
	    push (@history, [ $info{PUOwnerName}, $info{PUDatasetName} ]);
	    push (@pending, [ $info{PUOwnerName}, $info{PUDatasetName} ]);
	}
    }

    return @history;
}

# perform publication by invoking a site specific wrapper script
# using the following arguments:
# 1. dataset 2. owner 3. array with assignment IDs
sub publishBlock
{
    my ($self, $drop, $block) = @_;

    # Guess the type of the data we are processing.
    my $type = ($block->{DATASET} =~ /MBforPU/ ? "PileUp"
		: $block->{OWNER} =~ /Hit/ ? "Hit"
		: $block->{OWNER} =~ /DST/ ? "DST"
		: "Digi");

    # Check if we have already handled this one.  If so, skip it.
    return 10 if (-e "$self->{PUBLISHED}/$block->{OWNER}.$block->{DATASET}");

    # For Digi, Hit/PileUp are also required.  DST is stand-alone.
    my @history = eval { &datasetRequirements($block->{OWNER}, $block->{DATASET}) };
    do { chomp ($@); &alert ($@); return 1 } if $@;
    foreach my $item (@history)
    {
	next if (-e "$self->{PUBLISHED}/$item->[0].$item->[1]");
	&warn ("$drop: required pre-requisite $item->[0]/$item->[1] not yet published");
	return 30;
    }

    &logmsg("publishing $block->{OWNER}/$block->{DATASET}");
    &runcmd(("mkdir", "-p", "$self->{PUBLISHED}")) if (! -d $self->{PUBLISHED});
    &touch ("$self->{PUBLISHED}/$block->{OWNER}.$block->{DATASET}");
    my $failed = &runcmd(@{$self->{SCRIPT}}, $block->{DATASET}, $block->{OWNER},
		      &listAssignments($block));
    &alert("$drop: publishing failed with exit code @{[&runerror ($failed)]}") if $failed;
    return 20 if $failed;

    # return success.
    return 1;
}

