#!/usr/bin/env perl

##H Update node-to-node transfer performance statistics.  Aggregates
##H completed and pending transfers into historical histogram for
##H performance monitoring and latency estimation.
##H
##H Usage:
##H   PerfMonitor -state DIRECTORY -db FILE[:SECTION] -node NODE [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -node      the node where this agent runs
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
             "node=s"      => \$args{MYNODE},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG} || !$args{MYNODE})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new PerfMonitor (%args))->process();

######################################################################
# Routines for this agent.
package PerfMonitor; use strict; use warnings; use base 'UtilsAgent';
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My TMDB node name
	          WAITTIME => 120);		# Agent activity cycle
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Update database.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    eval
    {
	$dbh = &connectToDatabase ($self);

	# Use 5-minute binning.
	my $now = &mytimeofday();
	my $timewidth = 300;
	my $timebin = int($now/$timewidth)*$timewidth;

	# FIXME: Decide which periods should have "when matched" part.
	# If we want to take the first value of each period, then we
	# we can drop the "when matched".  If we want to have "live"
	# values, we keep the "when matched".

	# Part I
	#
	# Update statistics on currently pending transfer queue.  This
	# is a heart-beat routine, we want to execute this regularly
	# so the histogram has no gaps -- if we miss a beat, there will
	# be no data for this time bin.
	&dbexec($dbh, qq{
	    merge into t_link_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority,
	              count(s.fileid) pend_files,
		      sum(f.filesize) pend_bytes,
		      sum(case when s.to_state < 2 then 1 else 0 end) wait_files,
		      sum(case when s.to_state < 2 then f.filesize else 0 end) wait_bytes,
		      sum(case when s.to_state >= 100 then 1 else 0 end) cool_files,
		      sum(case when s.to_state >= 100 then f.filesize else 0 end) cool_bytes,
		      sum(case when s.from_state = 1 then 1 else 0 end) ready_files,
		      sum(case when s.from_state = 1 then f.filesize else 0 end) ready_bytes,
		      sum(case when s.to_state = 2 then 1 else 0 end) xfer_files,
		      sum(case when s.to_state = 2 then f.filesize else 0 end) xfer_bytes
		from t_xfer_state s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.pend_files = v.pend_files, h.pend_bytes = v.pend_bytes,
	        h.wait_files = v.wait_files, h.wait_bytes = v.wait_bytes,
	        h.cool_files = v.cool_files, h.cool_bytes = v.cool_bytes,
	        h.ready_files = v.ready_files, h.ready_bytes = v.ready_bytes,
	        h.xfer_files = v.xfer_files, h.xfer_bytes = v.xfer_bytes
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority,
	              pend_files, pend_bytes, wait_files, wait_bytes,
		      cool_files, cool_bytes, ready_files, ready_bytes,
		      xfer_files, xfer_bytes)
	      values (v.timebin, v.timewidth, v.from_node, v.to_node, v.priority,
	              v.pend_files, v.pend_bytes, v.wait_files, v.wait_bytes,
		      v.cool_files, v.cool_bytes, v.ready_files, v.ready_bytes,
		      v.xfer_files, v.xfer_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part II: Routing statistics.  This is a heartbeat again.
	&dbexec($dbh, qq{
	    merge into t_link_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority,
	              count(s.fileid) confirm_files,
		      sum(s.filesize) confirm_bytes,
		      sum(s.weight) confirm_weight
		from
		  (select
		       xp.from_node, xp.to_node,
		       2 * xp.priority + 1-xp.local_boost priority,
		       xp.fileid, f.filesize, count(xp.fileid) weight
		   from t_xfer_path xp join t_xfer_file f on f.id = xp.fileid
		   group by xp.from_node, xp.to_node,
			    2 * xp.priority + 1-xp.local_boost,
			    xp.fileid, f.filesize) s
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.confirm_files = v.confirm_files,
		h.confirm_bytes = v.confirm_bytes,
		h.confirm_weight = v.confirm_weight
	    when not matched then
	      insert (h.timebin, h.timewidth,
	      	      h.from_node, h.to_node, h.priority,
	              h.confirm_files, h.confirm_bytes, h.confirm_weight)
	      values (v.timebin, v.timewidth,
	      	      v.from_node, v.to_node, v.priority,
	              v.confirm_files, v.confirm_bytes, v.confirm_weight)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part III: Update statistics on recent transfer activity.
	&dbexec ($dbh, qq{lock table t_xfer_tracking in exclusive mode});
	&dbexec($dbh, qq{
	    merge into t_link_histogram h
	    using
	      (select trunc(xt.timestamp/:timewidth)*:timewidth timebin,
		      :timewidth timewidth,
	              xt.from_node, xt.to_node, xt.priority,
		      sum(xt.is_avail) avail_files,
		      sum(xt.is_avail * f.filesize) avail_bytes,
		      sum(xt.is_try) try_files,
		      sum(xt.is_try * f.filesize) try_bytes,
		      sum(xt.is_done) done_files,
		      sum(xt.is_done * f.filesize) done_bytes,
		      sum(xt.is_fail) fail_files,
		      sum(xt.is_fail * f.filesize) fail_bytes,
		      sum(xt.is_expire) expire_files,
		      sum(xt.is_expire * f.filesize) expire_bytes
		from t_xfer_tracking xt join t_xfer_file f on f.id = xt.fileid
		group by trunc(xt.timestamp/:timewidth)*:timewidth, :timewidth,
 			 xt.from_node, xt.to_node, xt.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.avail_files  = nvl(h.avail_files,0)  + v.avail_files,
		h.avail_bytes  = nvl(h.avail_bytes,0)  + v.avail_bytes,
	        h.try_files    = nvl(h.try_files,0)    + v.try_files,
		h.try_bytes    = nvl(h.try_bytes,0)    + v.try_bytes,
	        h.done_files   = nvl(h.done_files,0)   + v.done_files,
		h.done_bytes   = nvl(h.done_bytes,0)   + v.done_bytes,
	        h.fail_files   = nvl(h.fail_files,0)   + v.fail_files,
 		h.fail_bytes   = nvl(h.fail_bytes,0)   + v.fail_bytes,
	        h.expire_files = nvl(h.expire_files,0) + v.expire_files,
 		h.expire_bytes = nvl(h.expire_bytes,0) + v.expire_bytes
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority,
	              avail_files, avail_bytes, try_files, try_bytes,
		      done_files, done_bytes, fail_files, fail_bytes,
		      expire_files, expire_bytes)
	      values (v.timebin, v.timewidth, v.from_node, v.to_node, v.priority,
	              v.avail_files, v.avail_bytes, v.try_files, v.try_bytes,
		      v.done_files, v.done_bytes, v.fail_files, v.fail_bytes,
		      v.expire_files, v.expire_bytes)},
	    ":timewidth" => $timewidth);
	&dbexec ($dbh, qq{delete from t_xfer_tracking});
	$dbh->commit();

	# Part IV: Node statistics.  This is a heartbeat again.
	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              sum(b.files) dest_files, sum(b.bytes) dest_bytes
		from t_dps_block_dest s join t_dps_block b on b.id = s.block
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.dest_files = v.dest_files, h.dest_bytes = v.dest_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.dest_files, h.dest_bytes)
	      values (v.timebin, v.timewidth, v.node, v.dest_files, v.dest_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.node,
	              count(s.fileid) node_files, sum(f.filesize) node_bytes
		from t_xfer_replica s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.node) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.node_files = v.node_files, h.node_bytes = v.node_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.node_files, h.node_bytes)
	      values (v.timebin, v.timewidth, v.node, v.node_files, v.node_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              count(s.fileid) request_files, sum(f.filesize) request_bytes,
		      sum(s.state) idle_files, sum(s.state * f.filesize) idle_bytes
		from t_xfer_request s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.request_files = v.request_files, h.request_bytes = v.request_bytes,
	        h.idle_files = v.idle_files, h.idle_bytes = v.idle_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node,
	              h.request_files, h.request_bytes,
	              h.idle_files, h.idle_bytes)
	      values (v.timebin, v.timewidth, v.node,
	      	      v.request_files, v.request_bytes,
	      	      v.idle_files, v.idle_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	$dbh->commit();
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Check children are still running and then wait
    $self->nap ($$self{WAITTIME});
}
