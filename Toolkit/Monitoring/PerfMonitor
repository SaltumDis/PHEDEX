#!/usr/bin/env perl

##H Update node-to-node transfer performance statistics.  Aggregates
##H completed and pending transfers into historical histogram for
##H performance monitoring and latency estimation.
##H
##H Usage:
##H   PerfMonitor -state DIRECTORY -db FILE[:SECTION] -node NODE [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -node      the node where this agent runs
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
             "node=s"      => \$args{MYNODE},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG} || !$args{MYNODE})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new PerfMonitor (%args))->process();

######################################################################
# Routines for this agent.
package PerfMonitor; use strict; use warnings; use base 'UtilsAgent';
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
	          WAITTIME => 120);		# Agent activity cycle
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Update database.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    eval
    {
	$dbh = &connectToDatabase ($self, 0) or die "failed to connect";

	# Use 5-minute binning.
	my $timewidth = 300;
	my $timebin = int(&mytimeofday() / $timewidth)*$timewidth;

	# FIXME: Decide which periods should have "when matched" part.
	# If we want to take the first value of each period, then we
	# we can drop the "when matched".  If we want to have "live"
	# values, we keep the "when matched".

	# Part I
	#
	# Update statistics on currently pending transfer queue.  This
	# is a heart-beat routine, we want to execute this regularly
	# so the histogram has no gaps -- if we miss a beat, there will
	# be no data for this time bin.
	&dbexec($dbh, qq{
	    merge into t_xfer_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority,
	              count(s.fileid) pend_files,
		      sum(f.filesize) pend_bytes,
		      sum(case when s.to_state < 2 then 1 else 0 end) wait_files,
		      sum(case when s.to_state < 2 then f.filesize else 0 end) wait_bytes,
		      sum(case when s.to_state >= 100 then 1 else 0 end) cool_files,
		      sum(case when s.to_state >= 100 then f.filesize else 0 end) cool_bytes,
		      sum(case when s.from_state = 1 then 1 else 0 end) ready_files,
		      sum(case when s.from_state = 1 then f.filesize else 0 end) ready_bytes,
		      sum(case when s.to_state = 2 then 1 else 0 end) xfer_files,
		      sum(case when s.to_state = 2 then f.filesize else 0 end) xfer_bytes
		from t_xfer_state s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then 
	      update set
	        h.pend_files = v.pend_files, h.pend_bytes = v.pend_bytes,
	        h.wait_files = v.wait_files, h.wait_bytes = v.wait_bytes,
	        h.cool_files = v.cool_files, h.cool_bytes = v.cool_bytes,
	        h.ready_files = v.ready_files, h.ready_bytes = v.ready_bytes,
	        h.xfer_files = v.xfer_files, h.xfer_bytes = v.xfer_bytes
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority,
	              pend_files, pend_bytes, wait_files, wait_bytes,
		      cool_files, cool_bytes, ready_files, ready_bytes,
		      xfer_files, xfer_bytes)
	      values (v.timebin, v.timewidth, v.from_node, v.to_node, v.priority,
	              v.pend_files, v.pend_bytes, v.wait_files, v.wait_bytes,
		      v.cool_files, v.cool_bytes, v.ready_files, v.ready_bytes,
		      v.xfer_files, v.xfer_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);
	$dbh->commit();

	# Part II: Update statistics on recent transfer activity.
	&dbexec ($dbh, qq{lock table t_xfer_tracking in exclusive mode});
	&dbexec($dbh, qq{
	    merge into t_xfer_histogram h
	    using
	      (select trunc(xt.timestamp/300)*300 timebin, 300 timewidth,
	              xt.from_node, xt.to_node, xt.priority,
		      sum(case when xt.reason = 'avail' then 1 else 0 end) avail_files,
		      sum(case when xt.reason = 'avail' then f.filesize else 0 end) avail_bytes,
		      sum(case when xt.reason = 'try' then 1 else 0 end) try_files,
		      sum(case when xt.reason = 'try' then f.filesize else 0 end) try_bytes,
		      sum(case when xt.reason = 'done' then 1 else 0 end) done_files,
		      sum(case when xt.reason = 'done' then f.filesize else 0 end) done_bytes,
		      sum(case when xt.reason = 'fail' then 1 else 0 end) fail_files,
		      sum(case when xt.reason = 'fail' then f.filesize else 0 end) fail_bytes
		from t_xfer_tracking xt join t_xfer_file f on f.id = xt.fileid
		group by trunc(xt.timestamp/300)*300, 300, xt.from_node, xt.to_node, xt.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then 
	      update set
	        h.avail_files = v.avail_files, h.avail_bytes = v.avail_bytes,
	        h.try_files = v.try_files, h.try_bytes = v.try_bytes,
	        h.done_files = v.done_files, h.done_bytes = v.done_bytes,
	        h.fail_files = v.fail_files, h.fail_bytes = v.fail_bytes
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority,
	              avail_files, avail_bytes, try_files, try_bytes,
		      done_files, done_bytes, fail_files, fail_bytes)
	      values (v.timebin, v.timewidth, v.from_node, v.to_node, v.priority,
	              v.avail_files, v.avail_bytes, v.try_files, v.try_bytes,
		      v.done_files, v.done_bytes, v.fail_files, v.fail_bytes)});
	&dbexec ($dbh, qq{delete from t_xfer_tracking});
	$dbh->commit();

	# Part III: Routing statistics.  This is a heartbeat again.
	# FIXME: Priorities should be 4-level, for me/other.
	&dbexec($dbh, qq{
	    merge into t_routing_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, 0 priority,
	              count(s.fileid) offer_files,
		      sum(f.filesize) offer_bytes
		from t_xfer_offer s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.from_node, s.to_node, 0)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then 
	      update set
	        h.offer_files = v.offer_files,
		h.offer_bytes = v.offer_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth,
	      	      h.from_node, h.to_node, h.priority,
	              h.offer_files, h.offer_bytes)
	      values (v.timebin, v.timewidth,
	      	      v.from_node, v.to_node, v.priority,
	              v.offer_files, v.offer_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_routing_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority * 2 priority,
	              count(s.fileid) confirm_files,
		      sum(f.filesize) confirm_bytes,
		      sum(s.weight) confirm_weight
		from t_xfer_confirmation s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority*2)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then 
	      update set
	        h.confirm_files = v.confirm_files,
		h.confirm_bytes = v.confirm_bytes,
		h.confirm_weight = v.confirm_weight
	    when not matched then
	      insert (h.timebin, h.timewidth,
	      	      h.from_node, h.to_node, h.priority,
	              h.confirm_files, h.confirm_bytes, h.confirm_weight)
	      values (v.timebin, v.timewidth,
	      	      v.from_node, v.to_node, v.priority,
	              v.confirm_files, v.confirm_bytes, v.confirm_weight)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_routing_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority * 2 priority,
	              count(s.fileid) expire_files,
		      sum(f.filesize) expire_bytes,
		      sum(s.weight) expire_weight
		from t_xfer_expired s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority*2)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then 
	      update set
	        h.expire_files = v.expire_files,
		h.expire_bytes = v.expire_bytes,
		h.expire_weight = v.expire_weight
	    when not matched then
	      insert (h.timebin, h.timewidth, h.from_node,
	      	      h.to_node, h.priority,
	              h.expire_files, h.expire_bytes, h.expire_weight)
	      values (v.timebin, v.timewidth,
	      	      v.from_node, v.to_node, v.priority,
	              v.expire_files, v.expire_bytes, v.expire_weight)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);
	$dbh->commit();

	# Part IV: Node statistics.  This is a heartbeat again.
	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              count(f.id) dest_files, sum(f.filesize) dest_bytes
		from t_dps_block_dest s join t_xfer_file f on f.inblock = s.block
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then 
	      update set
	        h.dest_files = v.dest_files, h.dest_bytes = v.dest_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.dest_files, h.dest_bytes)
	      values (v.timebin, v.timewidth, v.node, v.dest_files, v.dest_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.node,
	              count(s.fileid) node_files, sum(f.filesize) node_bytes
		from t_xfer_replica s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.node) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then 
	      update set
	        h.node_files = v.node_files, h.node_bytes = v.node_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.node_files, h.node_bytes)
	      values (v.timebin, v.timewidth, v.node, v.node_files, v.node_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.to_node node,
	              count(s.fileid) xfer_files, sum(f.filesize) xfer_bytes
		from t_xfer_state s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.to_node) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then 
	      update set
	        h.xfer_files = v.xfer_files, h.xfer_bytes = v.xfer_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.xfer_files, h.xfer_bytes)
	      values (v.timebin, v.timewidth, v.node, v.xfer_files, v.xfer_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.from_node node,
	              count(s.fileid) expt_files, sum(f.filesize) expt_bytes
		from t_xfer_state s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.from_node) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then 
	      update set
	        h.expt_files = v.expt_files, h.expt_bytes = v.expt_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.expt_files, h.expt_bytes)
	      values (v.timebin, v.timewidth, v.node, v.expt_files, v.expt_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              count(s.fileid) request_files, sum(f.filesize) request_bytes,
		      sum(case when s.state = 0 then 1 else 0 end) open_files,
		      sum(case when s.state = 0 then f.filesize else 0 end) open_bytes,
		      sum(case when s.state = 1 then 1 else 0 end) active_files,
		      sum(case when s.state = 1 then f.filesize else 0 end) active_bytes,
		      sum(case when s.state = 2 then 1 else 0 end) idle_files,
		      sum(case when s.state = 2 then f.filesize else 0 end) idle_bytes
		from t_xfer_request s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then 
	      update set
	        h.open_files = v.open_files, h.open_bytes = v.open_bytes,
	        h.active_files = v.active_files, h.active_bytes = v.active_bytes,
	        h.idle_files = v.idle_files, h.idle_bytes = v.idle_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node,
	              h.open_files, h.open_bytes,
	              h.active_files, h.active_bytes,
	              h.idle_files, h.idle_bytes)
	      values (v.timebin, v.timewidth, v.node,
	      	      v.open_files, v.open_bytes,
	      	      v.active_files, v.active_bytes,
	      	      v.idle_files, v.idle_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	$dbh->commit();
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Check children are still running and then wait
    $self->nap ($$self{WAITTIME});
}
