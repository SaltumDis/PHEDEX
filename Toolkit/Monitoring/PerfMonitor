#!/usr/bin/env perl

##H Update node-to-node transfer performance statistics.  Aggregates
##H completed and pending transfers into historical histogram for
##H performance monitoring and latency estimation.
##H
##H Usage:
##H   PerfMonitor -state DIRECTORY -db FILE[:SECTION] -node NODE [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -node      the node where this agent runs
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
             "node=s"      => \$args{MYNODE},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG} || !$args{MYNODE})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new PerfMonitor (%args))->process();

######################################################################
# Routines for this agent.
package PerfMonitor; use strict; use warnings; use base 'UtilsAgent';
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My TMDB node name
	          WAITTIME => 120,		# Agent activity cycle
		  LAST_COMPACT => 0);		# Last we compacted old entries
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Update database.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    eval
    {
	$dbh = &connectToDatabase ($self);

	# Use 5-minute binning.
	my $now = &mytimeofday();
	my $timewidth = 300;
	my $timebin = int($now/$timewidth)*$timewidth;

	# FIXME: Decide which periods should have "when matched" part.
	# If we want to take the first value of each period, then we
	# we can drop the "when matched".  If we want to have "live"
	# values, we keep the "when matched".

	# Part I
	#
	# Update statistics on currently pending transfer queue.  This
	# is a heart-beat routine, we want to execute this regularly
	# so the histogram has no gaps -- if we miss a beat, there will
	# be no data for this time bin.
	&dbexec($dbh, qq{
	    merge into t_link_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority,
	              count(s.fileid) pend_files,
		      sum(f.filesize) pend_bytes,
		      sum(case when s.to_state < 2 then 1 else 0 end) wait_files,
		      sum(case when s.to_state < 2 then f.filesize else 0 end) wait_bytes,
		      sum(case when s.to_state >= 100 then 1 else 0 end) cool_files,
		      sum(case when s.to_state >= 100 then f.filesize else 0 end) cool_bytes,
		      sum(case when s.from_state = 1 then 1 else 0 end) ready_files,
		      sum(case when s.from_state = 1 then f.filesize else 0 end) ready_bytes,
		      sum(case when s.to_state = 2 then 1 else 0 end) xfer_files,
		      sum(case when s.to_state = 2 then f.filesize else 0 end) xfer_bytes
		from t_xfer_state s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.pend_files = v.pend_files, h.pend_bytes = v.pend_bytes,
	        h.wait_files = v.wait_files, h.wait_bytes = v.wait_bytes,
	        h.cool_files = v.cool_files, h.cool_bytes = v.cool_bytes,
	        h.ready_files = v.ready_files, h.ready_bytes = v.ready_bytes,
	        h.xfer_files = v.xfer_files, h.xfer_bytes = v.xfer_bytes
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority,
	              pend_files, pend_bytes, wait_files, wait_bytes,
		      cool_files, cool_bytes, ready_files, ready_bytes,
		      xfer_files, xfer_bytes)
	      values (v.timebin, v.timewidth, v.from_node, v.to_node, v.priority,
	              v.pend_files, v.pend_bytes, v.wait_files, v.wait_bytes,
		      v.cool_files, v.cool_bytes, v.ready_files, v.ready_bytes,
		      v.xfer_files, v.xfer_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part II: Routing statistics.  This is a heartbeat again.
	&dbexec($dbh, qq{
	    merge into t_link_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority,
	              count(s.fileid) confirm_files,
		      sum(s.filesize) confirm_bytes,
		      sum(s.weight) confirm_weight
		from
		  (select
		       xp.from_node, xp.to_node,
		       2 * xp.priority + 1-xp.local_boost priority,
		       xp.fileid, f.filesize, count(xp.fileid) weight
		   from t_xfer_path xp join t_xfer_file f on f.id = xp.fileid
		   group by xp.from_node, xp.to_node,
			    2 * xp.priority + 1-xp.local_boost,
			    xp.fileid, f.filesize) s
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.confirm_files = v.confirm_files,
		h.confirm_bytes = v.confirm_bytes,
		h.confirm_weight = v.confirm_weight
	    when not matched then
	      insert (h.timebin, h.timewidth,
	      	      h.from_node, h.to_node, h.priority,
	              h.confirm_files, h.confirm_bytes, h.confirm_weight)
	      values (v.timebin, v.timewidth,
	      	      v.from_node, v.to_node, v.priority,
	              v.confirm_files, v.confirm_bytes, v.confirm_weight)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part III: Update statistics on recent transfer activity.
	&dbexec ($dbh, qq{lock table t_xfer_tracking in exclusive mode});
	&dbexec($dbh, qq{
	    merge into t_link_histogram h
	    using
	      (select trunc(xt.timestamp/:timewidth)*:timewidth timebin,
		      :timewidth timewidth,
	              xt.from_node, xt.to_node, xt.priority,
		      sum(xt.is_avail) avail_files,
		      sum(xt.is_avail * f.filesize) avail_bytes,
		      sum(xt.is_try) try_files,
		      sum(xt.is_try * f.filesize) try_bytes,
		      sum(xt.is_done) done_files,
		      sum(xt.is_done * f.filesize) done_bytes,
		      sum(xt.is_fail) fail_files,
		      sum(xt.is_fail * f.filesize) fail_bytes,
		      sum(xt.is_expire) expire_files,
		      sum(xt.is_expire * f.filesize) expire_bytes
		from t_xfer_tracking xt join t_xfer_file f on f.id = xt.fileid
		group by trunc(xt.timestamp/:timewidth)*:timewidth, :timewidth,
 			 xt.from_node, xt.to_node, xt.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.avail_files  = nvl(h.avail_files,0)  + v.avail_files,
		h.avail_bytes  = nvl(h.avail_bytes,0)  + v.avail_bytes,
	        h.try_files    = nvl(h.try_files,0)    + v.try_files,
		h.try_bytes    = nvl(h.try_bytes,0)    + v.try_bytes,
	        h.done_files   = nvl(h.done_files,0)   + v.done_files,
		h.done_bytes   = nvl(h.done_bytes,0)   + v.done_bytes,
	        h.fail_files   = nvl(h.fail_files,0)   + v.fail_files,
 		h.fail_bytes   = nvl(h.fail_bytes,0)   + v.fail_bytes,
	        h.expire_files = nvl(h.expire_files,0) + v.expire_files,
 		h.expire_bytes = nvl(h.expire_bytes,0) + v.expire_bytes
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority,
	              avail_files, avail_bytes, try_files, try_bytes,
		      done_files, done_bytes, fail_files, fail_bytes,
		      expire_files, expire_bytes)
	      values (v.timebin, v.timewidth, v.from_node, v.to_node, v.priority,
	              v.avail_files, v.avail_bytes, v.try_files, v.try_bytes,
		      v.done_files, v.done_bytes, v.fail_files, v.fail_bytes,
		      v.expire_files, v.expire_bytes)},
	    ":timewidth" => $timewidth);
	&dbexec ($dbh, qq{delete from t_xfer_tracking});
	$dbh->commit();

	# Part IV: Node statistics.  This is a heartbeat again.
	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              sum(b.files) dest_files, sum(b.bytes) dest_bytes
		from t_dps_block_dest s join t_dps_block b on b.id = s.block
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.dest_files = v.dest_files, h.dest_bytes = v.dest_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.dest_files, h.dest_bytes)
	      values (v.timebin, v.timewidth, v.node, v.dest_files, v.dest_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.node,
	              count(s.fileid) node_files, sum(f.filesize) node_bytes
		from t_xfer_replica s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.node) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.node_files = v.node_files, h.node_bytes = v.node_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.node_files, h.node_bytes)
	      values (v.timebin, v.timewidth, v.node, v.node_files, v.node_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_dest_histogram h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              count(s.fileid) request_files, sum(f.filesize) request_bytes,
		      sum(s.state) idle_files, sum(s.state * f.filesize) idle_bytes
		from t_xfer_request s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.request_files = v.request_files, h.request_bytes = v.request_bytes,
	        h.idle_files = v.idle_files, h.idle_bytes = v.idle_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node,
	              h.request_files, h.request_bytes,
	              h.idle_files, h.idle_bytes)
	      values (v.timebin, v.timewidth, v.node,
	      	      v.request_files, v.request_bytes,
	      	      v.idle_files, v.idle_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part V: Update link parameters.
	&dbexec($dbh, qq{
	    merge into t_link_param p
	    using (select id from t_link) v
	    on (p.link = v.id)
	    when matched then
	      update set
		p.pend_bytes = null,
		p.done_bytes = null,
		p.try_bytes = null,
		p.xfer_rate = null,
	        p.xfer_latency = null,
		p.time_span = null,
		p.time_update = :now
	    when not matched then
	      insert (p.link, p.pend_bytes, p.done_bytes, p.try_bytes,
	      	      p.xfer_rate, p.xfer_latency, p.time_span, p.time_update)
	      values (v.id, null, null, null, null, null, null, :now)},
	    ":now" => $timebin);
        foreach my $span (3600, 8*3600, 86400, 30*86400)
	{
	    &dbexec($dbh, qq{
	        update t_link_param p
	        set (pend_bytes, done_bytes, try_bytes, time_span) =
		    (select
		       nvl(sum(pend_bytes) keep (dense_rank last order by timebin asc),0),
		       sum(done_bytes),
		       sum(try_bytes),
		       sum(case when pend_bytes > 0 or done_bytes > 0 or try_bytes > 0 then timewidth else 0 end)
		     from t_link_histogram h
		       join t_link l
		         on l.from_node = h.from_node
			 and l.to_node = h.to_node
		     where h.timebin >= :now - :span
		       and p.link = l.id)
	        where time_span is null
		  or (done_bytes is null and try_bytes is null)},
	        ":now" => $timebin, ":span" => $span);
	}

	&dbexec($dbh, qq{
	    update (select
	    	      pend_bytes, xfer_rate, xfer_latency,
		      nvl(done_bytes,0)/time_span rate,
		      case
		        when nvl(try_bytes,0) > 0 then
			  nvl(done_bytes,0)/try_bytes
		        when nvl(done_bytes,0) > 0 then
			  1
			else
			  0
		      end eff
	            from t_link_param
		    where time_span is not null)
	    set xfer_rate = rate,
	        xfer_latency =
		   case
		     when pend_bytes = 0
		       then 0
		     when rate > 0 and eff > 0
		       then least(pend_bytes/rate/eff,7*86400)
		     else 7*86400
		   end});

        &dbexec($dbh, qq{
	    update t_link_histogram h
	    set (param_rate, param_latency) =
	      (select xfer_rate, xfer_latency
	       from t_link_param p
	         join t_link l on l.id = p.link
	       where l.from_node = h.from_node
	         and l.to_node = h.to_node)
	    where timebin = :timebin},
 	    ":timebin" => $timebin);

	$dbh->commit();

	# Part VI: Compact old time series data to be in per-hour instead
	# of per-5-minute bins.  We do this only rarely to avoid loading
	# the database servers excessively.  We read the old data stats
	# in memory, merge, and write back.  This is mainly because some
	# of the data is accumulated, some not, and sql makes it awkward
	# to handle both.
	return if ($$self{LAST_COMPACT} > $timebin - 2*86400);
        my $limit = int($timebin/86400)*86400 - 86400;
	$self->compactLinkData($dbh, $limit);
	$self->compactDestData($dbh, $limit);
	$dbh->commit();
	$$self{LAST_COMPACT} = $timebin;
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Check children are still running and then wait
    $self->nap ($$self{WAITTIME});
}

sub compactUpdate
{
    my ($self, $dbh, $limit, $table, $stats) = @_;
    &dbexec($dbh, qq{
	delete from $table
	where timebin < :old and timewidth = 300},
	":old" => $limit);

    my $i = undef;
    foreach my $data (values %$stats)
    {
	if (! defined $i)
	{
	    my @keys = keys %$data;
	    my $sql = "insert into $table ("
		      . join(",", @keys) . ") values ("
		      . join(",", map { ":$_" } @keys) . ")";
	    $i = &dbprep($dbh, $sql);
	}
	&dbbindexec($i, map { (":$_" => $$data{$_}) } keys %$data);
    }
}

sub compactLinkData
{
    my ($self, $dbh, $limit) = @_;
    my %stats;
    my $q = &dbexec($dbh, qq{
	select * from t_link_histogram
	where timebin < :old and timewidth = 300
	order by timebin asc, from_node, to_node, priority},
    	":old" => $limit);
    while (my $row = $q->fetchrow_hashref())
    {
	my $bin = int($$row{TIMEBIN}/3600)*3600;
	my $key = "$bin $$row{FROM_NODE} $$row{TO_NODE} $$row{PRIORITY}";
	$$row{TIMEBIN} = $bin;
	$$row{TIMEWIDTH} = 3600;
	if (! exists $stats{$key})
	{
	    $stats{$key} = $row;
	}
	else
	{
	    my $s = $stats{$key};
	    $$s{$_} = ($$row{$_} || 0)
		for grep(/^(PEND|WAIT|COOL|READY|XFER|CONFIRM)_/, keys %$row);
	    $$s{$_} = ($$s{$_} || 0) + ($$row{$_} || 0)
		for grep(/^(AVAIL|DONE|TRY|FAIL|EXPIRE)_/, keys %$row);
	}
    }

    $self->compactUpdate ($dbh, $limit, "t_link_histogram", \%stats);
}

sub compactDestData
{
    my ($self, $dbh, $limit) = @_;
    my %stats;
    my $q = &dbexec($dbh, qq{
	select * from t_dest_histogram
	where timebin < :old and timewidth = 300
	order by timebin asc, node},
    	":old" => $limit);
    while (my $row = $q->fetchrow_hashref())
    {
	my $bin = int($$row{TIMEBIN}/3600)*3600;
	my $key = "$bin $$row{NODE}";
	$$row{TIMEBIN} = $bin;
	$$row{TIMEWIDTH} = 3600;
	if (! exists $stats{$key})
	{
	    $stats{$key} = $row;
	}
	else
	{
	    my $s = $stats{$key};
	    $$s{$_} = ($$row{$_} || 0)
		for grep(/^(DEST|NODE|REQUEST|IDLE)_/, keys %$row);
	}
    }

    $self->compactUpdate ($dbh, $limit, "t_dest_histogram", \%stats);
}
