#!/usr/bin/env perl

##H Update node-to-node transfer performance statistics.  Aggregates
##H completed and pending transfers into historical histogram for
##H performance monitoring and latency estimation.
##H
##H Usage:
##H   PerfMonitor -state DIRECTORY -db FILE[:SECTION] -node NODE [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -node      the node where this agent runs
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
             "node=s"      => \$args{MYNODE},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG} || !$args{MYNODE})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new PerfMonitor (%args))->process();

######################################################################
# Routines for this agent.
package PerfMonitor; use strict; use warnings; use base 'UtilsAgent';
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My TMDB node name
	          WAITTIME => 120,		# Agent activity cycle
		  LAST_COMPACT => 0);		# Last we compacted old entries
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Update database.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    eval
    {
	$dbh = &connectToDatabase ($self);

	# Use 5-minute binning.
	my $now = &mytimeofday();
	my $timewidth = 300;
	my $timebin = int($now/$timewidth)*$timewidth;

	# FIXME: Decide which periods should have "when matched" part.
	# If we want to take the first value of each period, then we
	# we can drop the "when matched".  If we want to have "live"
	# values, we keep the "when matched".

	# Part I
	#
	# Update statistics on currently pending transfer queue.  This
	# is a heart-beat routine, we want to execute this regularly
	# so the histogram has no gaps -- if we miss a beat, there will
	# be no data for this time bin.  This conflicts with updates
	# from transfer pump agent.
	&dbexec($dbh, qq{
	    merge into t_history_link h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      xt.from_node, xt.to_node, xt.priority,
	              count(xt.fileid) pend_files,
		      sum(f.filesize) pend_bytes,
		      sum(case when xti.time_update > 0 then 0 else 1 end) wait_files,
		      sum(case when xti.time_update > 0 then 0 else f.filesize end) wait_bytes,
		      sum(case when xte.time_update > 0 then 1 else 0 end) ready_files,
		      sum(case when xte.time_update > 0 then f.filesize else 0 end) ready_bytes,
		      sum(case when xti.time_update > 0 then 1 else 0 end) xfer_files,
		      sum(case when xti.time_update > 0 then f.filesize else 0 end) xfer_bytes
		from t_xfer_task xt
		  join t_xfer_file f on f.id = xt.fileid
		  left join t_xfer_task_export xte on xte.task = xt.id
		  left join t_xfer_task_inxfer xti on xti.task = xt.id
		group by :timebin, :timewidth, xt.from_node, xt.to_node, xt.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.pend_files = v.pend_files, h.pend_bytes = v.pend_bytes,
	        h.wait_files = v.wait_files, h.wait_bytes = v.wait_bytes,
	        h.ready_files = v.ready_files, h.ready_bytes = v.ready_bytes,
	        h.xfer_files = v.xfer_files, h.xfer_bytes = v.xfer_bytes
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority,
	              pend_files, pend_bytes, wait_files, wait_bytes,
		      ready_files, ready_bytes, xfer_files, xfer_bytes)
	      values (v.timebin, v.timewidth, v.from_node, v.to_node, v.priority,
	              v.pend_files, v.pend_bytes, v.wait_files, v.wait_bytes,
		      v.ready_files, v.ready_bytes, v.xfer_files, v.xfer_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part II: Routing statistics.  This is a heartbeat again.
	&dbexec($dbh, qq{
	    merge into t_history_link h
	    using
	      (select :timebin timebin, :timewidth timewidth,
	      	      s.from_node, s.to_node, s.priority,
	              count(s.fileid) confirm_files,
		      sum(s.filesize) confirm_bytes,
		      sum(s.weight) confirm_weight
		from
		  (select
		       xp.from_node, xp.to_node,
		       2 * xp.priority + 1-xp.is_local priority,
		       xp.fileid, f.filesize, count(xp.fileid) weight
		   from t_xfer_path xp join t_xfer_file f on f.id = xp.fileid
		   group by xp.from_node, xp.to_node,
			    2 * xp.priority + 1-xp.is_local,
			    xp.fileid, f.filesize) s
		group by :timebin, :timewidth, s.from_node, s.to_node, s.priority)
		v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
		h.to_node = v.to_node and
		h.priority = v.priority)
	    when matched then
	      update set
	        h.confirm_files = v.confirm_files,
		h.confirm_bytes = v.confirm_bytes,
		h.confirm_weight = v.confirm_weight
	    when not matched then
	      insert (h.timebin, h.timewidth,
	      	      h.from_node, h.to_node, h.priority,
	              h.confirm_files, h.confirm_bytes, h.confirm_weight)
	      values (v.timebin, v.timewidth,
	      	      v.from_node, v.to_node, v.priority,
	              v.confirm_files, v.confirm_bytes, v.confirm_weight)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part III: Update statistics on recent transfer activity.
	# Now "close" the link statistics.  This ensures at least one
	# row of nulls for links which have previously had stats, and
	# makes the link parameter calculation below pick up correct
	# "empty" final state.
	&dbexec($dbh, qq{
	    insert into t_history_link
	    (timebin, timewidth, from_node, to_node, priority)
	    select :timebin, :timewidth, from_node, to_node, priority
	    from (select from_node, to_node, priority, max(timebin) prevbin
	          from t_history_link where timebin < :timebin
		  group by from_node, to_node, priority) h
	    where exists
		(select 1 from t_history_link hh
		 where hh.from_node = h.from_node
		   and hh.to_node = h.to_node
		   and hh.priority = h.priority
		   and hh.timebin = h.prevbin
		   and hh.pend_bytes > 0)
	      and not exists
		(select 1 from t_history_link hh
		 where hh.from_node = h.from_node
		   and hh.to_node = h.to_node
		   and hh.priority = h.priority
		   and hh.timebin > h.prevbin)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);
	$dbh->commit();

	# Part IV: Node statistics.  This is a heartbeat again.
	# FIXME: block destination -> subscriptions!?
	# FIXME: node file source statistics!?
	&dbexec($dbh, qq{
	    merge into t_history_dest h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              sum(b.files) dest_files, sum(b.bytes) dest_bytes
		from t_dps_block_dest s join t_dps_block b on b.id = s.block
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.dest_files = v.dest_files, h.dest_bytes = v.dest_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.dest_files, h.dest_bytes)
	      values (v.timebin, v.timewidth, v.node, v.dest_files, v.dest_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_history_dest h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.node,
	              count(s.fileid) node_files, sum(f.filesize) node_bytes
		from t_xfer_replica s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.node) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.node_files = v.node_files, h.node_bytes = v.node_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node, h.node_files, h.node_bytes)
	      values (v.timebin, v.timewidth, v.node, v.node_files, v.node_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	&dbexec($dbh, qq{
	    merge into t_history_dest h
	    using
	      (select :timebin timebin, :timewidth timewidth, s.destination node,
	              count(s.fileid) request_files, sum(f.filesize) request_bytes,
		      sum(s.state) idle_files, sum(s.state * f.filesize) idle_bytes
		from t_xfer_request s join t_xfer_file f on f.id = s.fileid
		group by :timebin, :timewidth, s.destination) v
	    on (h.timebin = v.timebin and h.node = v.node)
	    when matched then
	      update set
	        h.request_files = v.request_files, h.request_bytes = v.request_bytes,
	        h.idle_files = v.idle_files, h.idle_bytes = v.idle_bytes
	    when not matched then
	      insert (h.timebin, h.timewidth, h.node,
	              h.request_files, h.request_bytes,
	              h.idle_files, h.idle_bytes)
	      values (v.timebin, v.timewidth, v.node,
	      	      v.request_files, v.request_bytes,
	      	      v.idle_files, v.idle_bytes)},
	    ":timebin" => $timebin, ":timewidth" => $timewidth);

	# Part V: Update link parameters.
	&dbexec($dbh, qq{
	    merge into t_adm_link_param p
	    using (select id from t_adm_link) v
	    on (p.link = v.id)
	    when matched then
	      update set
		p.pend_bytes = null,
		p.done_bytes = null,
		p.try_bytes = null,
		p.xfer_rate = null,
	        p.xfer_latency = null,
		p.time_span = null,
		p.time_update = :now
	    when not matched then
	      insert (p.link, p.pend_bytes, p.done_bytes, p.try_bytes,
	      	      p.xfer_rate, p.xfer_latency, p.time_span, p.time_update)
	      values (v.id, null, null, null, null, null, null, :now)},
	    ":now" => $timebin);
        foreach my $span (3600, 12*3600, 2*86400)
	{
	    &dbexec($dbh, qq{
	        update t_adm_link_param p
	        set (pend_bytes, done_bytes, try_bytes, time_span) =
		    (select
		       nvl(sum(pend_bytes) keep (dense_rank last order by timebin asc),0),
		       sum(done_bytes),
		       sum(try_bytes),
		       count(distinct case
		         when timewidth=300
			    and (pend_bytes > 0 or done_bytes > 0 or try_bytes > 0)
		  	  then timebin end)*300
		       + count(distinct case
		         when timewidth=3600
			    and (pend_bytes > 0 or done_bytes > 0 or try_bytes > 0)
		          then timebin end)*3600
		     from t_history_link h
		       join t_adm_link l
		         on l.from_node = h.from_node
			 and l.to_node = h.to_node
		     where h.timebin > :now - :span
		       and p.link = l.id)
	        where (time_span is null or time_span = 0)
		  or (done_bytes is null and try_bytes is null)},
	        ":now" => $timebin, ":span" => $span);
	}

	&dbexec($dbh, qq{
	    update (select
	    	      pend_bytes, xfer_rate, xfer_latency,
		      case
		        when nvl(pend_bytes,0) = 0 then null
		        when time_span > 0 then
		          nvl(done_bytes,0)/time_span
			else 0
		      end rate
	            from t_adm_link_param
		    where time_span is not null and time_span != 0)
	    set xfer_rate = rate,
	        xfer_latency =
		   case
		     when pend_bytes = 0
		       then 0
		     when rate > 0
		       then least(pend_bytes/rate,7*86400)
		     else 7*86400
		   end});

        &dbexec($dbh, qq{
	    update t_history_link h
	    set (param_rate, param_latency) =
	      (select xfer_rate, xfer_latency
	       from t_adm_link_param p
	         join t_adm_link l on l.id = p.link
	       where l.from_node = h.from_node
	         and l.to_node = h.to_node)
	    where timebin = :timebin},
 	    ":timebin" => $timebin);

	$dbh->commit();

	# Part VI: Compact old time series data to be in per-hour instead
	# of per-5-minute bins.  We do this only rarely to avoid loading
	# the database servers excessively.  We read the old data stats
	# in memory, merge, and write back.  This is mainly because some
	# of the data is accumulated, some not, and sql makes it awkward
	# to handle both.
	return if ($$self{LAST_COMPACT} > $timebin - 2*86400);
        my $limit = int($timebin/86400)*86400 - 86400;
	$self->compactLinkData($dbh, $limit);
	$self->compactDestData($dbh, $limit);
	$dbh->commit();
	$$self{LAST_COMPACT} = $timebin;
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Check children are still running and then wait
    $self->nap ($$self{WAITTIME});
}

sub compactUpdate
{
    my ($self, $dbh, $limit, $table, $stats) = @_;
    &dbexec($dbh, qq{
	delete from $table
	where timebin < :old and timewidth = 300},
	":old" => $limit);

    my $i = undef;
    foreach my $data (values %$stats)
    {
	if (! defined $i)
	{
	    my @keys = keys %$data;
	    my $sql = "insert into $table ("
		      . join(",", @keys) . ") values ("
		      . join(",", map { ":$_" } @keys) . ")";
	    $i = &dbprep($dbh, $sql);
	}
	&dbbindexec($i, map { (":$_" => $$data{$_}) } keys %$data);
    }
}

sub compactLinkData
{
    my ($self, $dbh, $limit) = @_;
    my %stats;
    my $q = &dbexec($dbh, qq{
	select * from t_history_link
	where timebin < :old and timewidth = 300
	order by timebin asc, from_node, to_node, priority},
    	":old" => $limit);
    while (my $row = $q->fetchrow_hashref())
    {
	my $bin = int($$row{TIMEBIN}/3600)*3600;
	my $key = "$bin $$row{FROM_NODE} $$row{TO_NODE} $$row{PRIORITY}";
	if (! exists $stats{$key})
	{
	    $stats{$key} = $row;
	    $$row{TIMETOT} = $$row{TIMEWIDTH};
	}
	else
	{
	    my $s = $stats{$key};
	    $$s{$_} = ($$row{$_} || 0)
		for grep(/^(PEND|WAIT|COOL|READY|XFER|CONFIRM)_/, keys %$row);
	    $$s{$_} = ($$s{$_} || 0) + ($$row{$_} || 0)
		for grep(/^(AVAIL|DONE|TRY|FAIL|EXPIRE)_/, keys %$row);
	    $$s{$_} = ($$s{$_} || 0) + ($$row{$_} || 0)*$$row{TIMEWIDTH}
		for grep(/^(PARAM)_/, keys %$row);
	    $$s{TIMETOT} += $$row{TIMEWIDTH};
	}
	$$row{TIMEBIN} = $bin;
	$$row{TIMEWIDTH} = 3600;
    }

    foreach my $s (values %stats)
    {
	if ($$s{TIMETOT})
	{
	    $$s{PARAM_RATE} /= $$s{TIMETOT};
	    $$s{PARAM_LATENCY} /= $$s{TIMETOT};
	}
	else
	{
	    $$s{PARAM_RATE} = 0;
	    $$s{PARAM_LATENCY} = 0;
        }
	delete $$s{TIMETOT};
    }

    $self->compactUpdate ($dbh, $limit, "t_history_link", \%stats);
}

sub compactDestData
{
    my ($self, $dbh, $limit) = @_;
    my %stats;
    my $q = &dbexec($dbh, qq{
	select * from t_history_dest
	where timebin < :old and timewidth = 300
	order by timebin asc, node},
    	":old" => $limit);
    while (my $row = $q->fetchrow_hashref())
    {
	my $bin = int($$row{TIMEBIN}/3600)*3600;
	my $key = "$bin $$row{NODE}";
	$$row{TIMEBIN} = $bin;
	$$row{TIMEWIDTH} = 3600;
	if (! exists $stats{$key})
	{
	    $stats{$key} = $row;
	}
	else
	{
	    my $s = $stats{$key};
	    $$s{$_} = ($$row{$_} || 0)
		for grep(/^(DEST|NODE|REQUEST|IDLE)_/, keys %$row);
	}
    }

    $self->compactUpdate ($dbh, $limit, "t_history_dest", \%stats);
}
