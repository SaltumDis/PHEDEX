#!/usr/bin/env perl

##H Route files toward destination nodes.
##H
##H Usage:
##H   FileRouter
##H      -state DIRECTORY -nodes PATTERN[,PATTERN...]
##H      -db FILE[:SECTION] [-log OUT]
##H
##H -state     agent state directory
##H -nodes     comma-separated list of node name patters to route for
##H              '%' is wildcard for any string, '_' for any character
##H -db        database connection configuration parameter file
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
             "nodes=s"     => sub { push(@{$args{NODES}}, split(/,/, $_[1])) },
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{NODES} || !$args{DBCONFIG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FileRouter (%args))->process();

######################################################################
# Routines for this agent.
package FileRouter; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(max);
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

use constant TERABYTE => 1024**4;
use constant GIGABYTE => 1024**3;
use constant MEGABYTE => 1024**2;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My node name
		  WAITTIME => 60 + rand(10),	# Agent activity cycle
		  NODES => []);			# Nodes to route for
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and pass them to slaves.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my @nodes;

    eval
    {
	# All of this must run in a single agent to avoid database
	# connection proliferation.  The execution order for the
	# phases is important in that it balances progress for this
	# node (destination) and requests by other nodes (relaying).
	# Some of the steps feed to the next one.
	#
	# New requests begin in open state.  Offers will then begin to
	# build.  Once first offer reaches the destination, we mark
	# the request to go active in twice the time elapsed from the
	# opening.  Once a request is active, we confirm the path that
	# Has least total cost.
	#
	# Consider running only one instance of this agent for all
	# nodes.  If the agent passes one loop quickly enough, this
	# will reduce number of connections and database load
	# significantly.

	($dbh, @nodes) = &expandNodesAndConnect ($self,
	    { "FileDownload" => 900, "FileExport" => 900 });

	# Read links and their parameters.  Only consider links which
	# are "alive", i.e. have a live download agent at destination
	# and an export agent at the source.
	my $links = {};
	my $q = &dbexec($dbh, qq{
	    select l.from_node, l.to_node, l.distance, l.local_boost,
	           p.xfer_rate, p.xfer_latency
	    from t_link l
	      join t_node ns on ns.id = l.from_node
	      join t_node nd on nd.id = l.to_node
	      left join t_link_param p on p.link = l.id
	    where (ns.name like '%MSS'
	           and exists (select 1
	    		       from t_agent_status ast
	    		         join t_agent a on a.id = ast.agent
			       where ast.node = l.to_node
			         and a.name = 'FileMSSPublish'
				 and ast.time_update >= :recent))
	       or (nd.name like '%MSS'
	           and exists (select 1
	    		       from t_agent_status ast
	    		         join t_agent a on a.id = ast.agent
			       where ast.node = l.to_node
			         and a.name = 'FileDownload'
				 and ast.time_update >= :recent))
	       or (exists (select 1
	    		   from t_agent_status ast
	    		     join t_agent a on a.id = ast.agent
			   where ast.node = l.from_node
			     and a.name = 'FileExport'
			     and ast.time_update >= :recent)
	           and exists (select 1
      			       from t_agent_status ast
			         join t_agent a on a.id = ast.agent
			       where ast.node = l.to_node
			         and a.name = 'FileDownload'
				 and ast.time_update >= :recent))},
	    ":recent" => &mytimeofday() - 900);
	while (my ($from, $to, $hops, $boost, $rate, $latency) = $q->fetchrow())
	{
	    $$links{$from}{$to} = { HOPS => $hops,
				    LOCAL_BOOST => $boost,
				    XFER_RATE => $rate,
			    	    XFER_LATENCY => $latency };
	}

	# Now route for the nodes
	my $costs = {};
	foreach my $node (@nodes)
	{
	    my $nodeid = $$self{NODES_ID}{$node};
	    $self->route ($dbh, $nodeid, $links, $costs);
	    $self->maybeStop();
	}

	&logmsg ("routed for @nodes");
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Have a little rest
    $self->nap ($$self{WAITTIME});
}

# Run the routing algorithm for one node.
sub route
{
    my ($self, $dbh, $node, $links, $costs) = @_;

    ######################################################################
    # Phase 1: Issue file requests for blocks pending some of their files.
    #
    # In this phase, we request files for blocks in the order of the block
    # priority.  We first check how many files are a) currently in transfer
    # to this node, and b) (un)confirmed for transfer to this node.  If
    # these do not exceed defined limits (a: 1 TB, b: 10 TB), we issue new
    # requests for files with no outstanding request.
    #
    # Note that files which cannot be routed in reasonable amount of time
    # leave requests in "inactive" state for some time.  This deliberately
    # prevents this phase from issuing a request for that file again, thus
    # preventing database churn when routing requests fail to complete in
    # reasonable amount of time.  This behaviour blocks routing for files
    # and sites that do not function properly, but avoids loading database
    # servers and allows other sites to make timely progress.
    # (Do this every five minutes or so.)

    # Update priority on existing requests
    &dbexec ($dbh, qq{
        update (select xq.priority req_priority, bd.priority cur_priority
                from t_xfer_request xq
                  join t_dps_block_dest bd
                    on bd.block = xq.inblock
                    and bd.destination = xq.destination
                where xq.destination = :node)
        set req_priority = cur_priority},
	":node" => $node);

    # Clear completed requests.
    &dbexec($dbh, qq{
        delete from t_xfer_request xq
        where destination = :node and exists
          (select 1 from t_xfer_replica xr
	   where xr.fileid = xq.fileid and xr.node = xq.destination)},
	":node" => $node);

    # Clear requests for files no longer wanted
    &dbexec($dbh, qq{
        delete from t_xfer_request xq
        where destination = :node and not exists
          (select 1 from t_dps_block_dest bd
	   where bd.destination = xq.destination
	     and bd.block = xq.inblock
	     and bd.state = 1)},
	":node" => $node);
    $dbh->commit();

    # Get current transfer load parameters.
    my (%pend_bytes, %confirm_bytes);
    my $now = &mytimeofday();
    my $q = &dbexec($dbh, qq{
	select xs.priority, nvl(sum(f.filesize),0)
        from t_xfer_state xs join t_xfer_file f on f.id = xs.fileid
        where xs.to_node = :node group by xs.priority},
	":node" => $node);
    while (my ($priority, $bytes) = $q->fetchrow())
    {
	$pend_bytes{$priority} = $bytes;
    }

    $q = &dbexec($dbh, qq{
	select xp.priority, nvl(sum(f.filesize),0)
        from t_xfer_path xp join t_xfer_file f on f.id = xp.fileid
        where xp.to_node = :node group by xp.priority},
	":node" => $node);
    while (my ($priority, $bytes) = $q->fetchrow())
    {
	$confirm_bytes{$priority} = $bytes;
    }

    # FIXME: What we need is flexible bandwidth management, in proportions
    # 4:3:2:1 for P0:P1:P2:P3 (P0 = me/high, P1 = others/high, P2 = me/low,
    # P3 = others/low).  (If less actual priority levels needed, just
    # chop proportions from left, e.g. 2:1 for P1:P3 actual need.)
    #
    # We should limit the number of requests, confirmations and transfers
    # roughly in these proportions.
    my ($pend_me, $pend_all, $confirm_me, $confirm_all) = (0) x 4;
    foreach (keys %pend_bytes)
    {
	my $x = $pend_bytes{$_} || 0;
	$pend_me += $x if $_ % 2 == 0;
	$pend_all += $x;
    }
    foreach (keys %confirm_bytes)
    {
	my $x = $confirm_bytes{$_} || 0;
	$confirm_me += $x if $_ % 2 == 0;
	$confirm_all += $x;
    }

    
    if (($pend_all < 5 * TERABYTE
	 && $confirm_all < 5 * TERABYTE
	 && $confirm_me < 5 * TERABYTE)
	|| ($confirm_me < 0.25 * TERABYTE))
    {
	# Find block destinations we can activate.  The activation
	# takes a lock on the block itself so files cannot be added
	# while we are creating file requests; any files added after
	# that will automatically add file requests as well.  Keep
	# adding blocks until we've added at least 500 files; select
	# the blocks in order of priority and age (highest priority
	# = smallest priority value).
	my @blocks;
	my $nbytes = 0;
	my $i = &dbprep($dbh, qq{
	    insert into t_xfer_request
	      (fileid, inblock, destination, priority, state,
	       attempt, time_create, time_expire)
	      select
	        id, :block inblock, :node destination, :priority priority,
		0 state, 1 attempt, :now, :now + 8*3600
	      from t_xfer_file
	      where inblock = :block});
	my $u = &dbprep($dbh, qq{
	    update t_dps_block_dest
	    set state = 1, time_active = :now
	    where block = :block and destination = :node});
	my $q = &dbexec($dbh, qq{
	    select bd.block, bd.priority, b.bytes
	    from t_dps_block_dest bd
	      join t_dps_block b on b.id = bd.block
	    where bd.destination = :node and bd.state = 0
	    order by bd.priority asc, bd.time_create asc},
	    ":node" => $node);
        while (my ($block, $priority, $bytes) = $q->fetchrow())
	{
	    &dbbindexec($u,
		        ":block" => $block,
		    	":node" => $node,
			":now" => $now);
	    push(@blocks, [ $block, $priority ]);
	    last if ($nbytes += $bytes) >= TERABYTE;
	}

	# Commit first phase so any concurrent modification of t_xfer_file
	# and t_xfer_replica is handled by our triggers.
	$dbh->commit();

	foreach my $item (@blocks)
	{
	    &dbbindexec($i,
		        ":block" => $$item[0],
		    	":node" => $node,
			":priority" => $$item[1],
			":now" => $now);
	}

	# Now commit second phase.
	$dbh->commit();
    }

    ######################################################################
    # Phase 2: Expand file requests into transfer paths through the
    # network.  For each request we build a minimum cost path from
    # available replicas using a routing table of network links and
    # current traffic conditions.  The transfer paths are refreshed
    # regularly to account for changes in network conditions.
    #
    # In other words, each destination node decides the entire path
    # for each file, using network configuration information it
    # obtains from other nodes.  For correctness it is important that
    # the entire route is built by one node using a consistent network
    # snapshot, building routes piecewise at each node using only
    # local information does not produce correct results.
    #
    # We begin with file replicas for each active file request and
    # current network conditions.  We calculate a least-cost transfer
    # path for each file.  We then update the database.

    # Clear old paths and those missing a request
    &dbexec($dbh, qq{
        delete from t_xfer_path xp
        where destination = :node
         and (xp.time_confirm < :now - 20*60
	      or not exists (select 1 from t_xfer_request xq
	                     where xq.destination = xp.destination
			       and xq.fileid = xp.fileid))},
	":node" => $node, ":now" => $now);
    
    # Read requests and replicas for requests without paths
    my $requests = {};
    $q = &dbexec($dbh, qq{
	select
	    xq.fileid, f.filesize,
	    xq.priority, xq.time_create, xq.time_expire,
	    xr.node, xr.state
	from t_xfer_request xq
	  join t_xfer_file f
	    on f.id = xq.fileid
	  join t_xfer_replica xr
	    on xr.fileid = xq.fileid
	where xq.state = 0
	  and xq.destination = :node
	  and xq.time_expire > :now
	  and not exists (select 1 from t_xfer_path xp
			  where xp.destination = xq.destination
			    and xp.fileid = xq.fileid)},
	":node" => $node, ":now" => $now);
    while (my ($file, $size, $priority, $create, $expire,
	       $rnode, $state) = $q->fetchrow())
    {
	my $sizebin = (int($size / (500*MEGABYTE))+1)*(500*MEGABYTE);
	$$requests{$file} ||= { FILEID => $file,
				FILESIZE => $size,
				SIZEBIN => $sizebin,
				PRIORITY => $priority,
				TIME_CREATE => $create,
				TIME_EXPIRE => $expire };
	$$requests{$file}{REPLICAS}{$rnode} = $state;
	$self->routeCost($links, $costs, $rnode, $state, $sizebin);
    }

    # Build optimal file paths
    $self->routeFile($node, $links, $costs, $_) for values %$requests;

    # Update database
    my %iargs;
    my $istmt = &dbprep($dbh, qq{
	insert into t_xfer_path
	(fileid, destination, hop, src_node, from_node, to_node,
	 priority, local_boost, cost, total_cost, penalty,
	 time_request, time_confirm, time_expire)
	values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)});

    foreach my $req (values %$requests)
    {
	foreach my $hop (@{$$req{PATH}})
	{
	    my $n = 1;
	    push(@{$iargs{$n++}}, $$req{FILEID});
	    push(@{$iargs{$n++}}, $node);
	    push(@{$iargs{$n++}}, $$hop{INDEX});
	    push(@{$iargs{$n++}}, $$hop{SRC_NODE});
	    push(@{$iargs{$n++}}, $$hop{FROM_NODE});
	    push(@{$iargs{$n++}}, $$hop{TO_NODE});
	    push(@{$iargs{$n++}}, $$req{PRIORITY});
	    push(@{$iargs{$n++}}, $$hop{LOCAL_BOOST});
	    push(@{$iargs{$n++}}, ($$hop{LINK_LATENCY} || 0) + ($$hop{XFER_LATENCY} || 0));
	    push(@{$iargs{$n++}}, ($$hop{TOTAL_LATENCY} || 0));
	    push(@{$iargs{$n++}}, ($$hop{LINK_RATE} || 0));
	    push(@{$iargs{$n++}}, $$req{TIME_CREATE});
	    push(@{$iargs{$n++}}, $now);
	    push(@{$iargs{$n++}}, $$req{TIME_EXPIRE});
	}
    }

    &dbbindexec($istmt, %iargs) if %iargs;
    $dbh->commit();
    undef $requests;

    ######################################################################
    # Phase 3: Confirm requests for which there are valid offers.

    # Discard duplicate confirmations for me for the same file.  Keep
    # the best alternative: local one, or least cost one, or pick one.
    &dbexec($dbh, qq{
	delete from t_xfer_path this
	where to_node = :node and exists
	  (select 1 from t_xfer_path better
	   where this.fileid = better.fileid
	     and this.to_node = better.to_node
	     and ((this.local_boost = 0 and better.local_boost > 0)
		  or this.total_cost > better.total_cost
		  or (this.total_cost = better.total_cost
		      and this.rowid > better.rowid)))},
	":node" => $node);

    # If transfers path are about to expire on links which have
    # reasonable recent transfer rate, give a bit more grace time.  We
    # do _not_ renew transfers themselves, but allow them to expire,
    # then get recreated.  This allows files with many errors to be
    # flushed out and put back into transfer at original plan.
    &dbexec($dbh, qq{
	update t_xfer_path xp
	set xp.time_expire = :now + 2*3600
	where xp.to_node = :me
	  and xp.time_expire >= :now
	  and xp.time_expire < :now + 3600
	  and exists
	    (select 1 from t_link_param lp
	     join t_link l on l.id = lp.link
	     where l.from_node = xp.from_node
	       and l.to_node = xp.to_node
       	       and lp.xfer_rate >= 1048576)},
	":me" => $node, ":now" => $now);

    &dbexec($dbh, qq{
	update t_xfer_request xq
	set xq.time_expire =
	  (select max(xp.time_expire)
	   from t_xfer_path xp
	   where xp.fileid = xq.fileid
	     and xp.destination = xq.destination)
        where xq.destination = :me
	  and xq.state = 0
          and exists
	    (select 1 from t_xfer_path xp
	     where xp.fileid = xq.fileid
	       and xp.destination = xq.destination
	       and xp.time_expire > xq.time_expire)},
	":me" => $node);

    # Confirm transfers where I am the path hop destination, the
    # source replica exists and there is no transfer yet.  However
    # remove inactive transfers no longer requested, but in a way
    # that avoids database contention with PFN export.
    &dbexec($dbh, qq{
        delete from t_xfer_state xs
	where to_node = :me
          and to_state != 2
	  and (from_state = 1 or time_assign < :now - 1200)
	  and time_expire >= :now
          and not exists
	    (select 1 from t_xfer_path xp
	     where xp.to_node = xs.to_node
	       and xp.from_node = xs.from_node
	       and xp.fileid = xs.fileid)},
	":me" => $node, ":now" => $now);

    &dbexec($dbh, qq{
        insert into t_xfer_state
          (fileid, errors, priority, weight, age,
           from_replica, from_node, from_state,
           to_node, to_state, time_expire, time_assign)

	  with files_for_me as
	    (select
		 xp.fileid, xp.from_node, xp.to_node, xr.id replica,
		 min(xp.priority) priority,
		 max(xp.local_boost * nvl(l.local_boost,0)) local_boost,
		 count(xp.fileid) weight,
		 min(xp.time_request) time_request,
		 min(xp.time_confirm) time_confirm,
		 max(xp.time_expire) time_expire
	     from t_xfer_path xp
	       left join t_link l
		 on l.to_node = xp.from_node
		 and l.from_node = xp.to_node
               join t_xfer_replica xr
                 on xr.fileid = xp.fileid
                 and xr.node = xp.from_node
	     where xp.to_node = :me and xp.time_expire > :now
	     group by xp.fileid, xp.from_node, xp.to_node, xr.id)
          select
             fileid, 0, 2*priority + 1-local_boost, weight, time_request,
             replica, from_node, 0, to_node, 0, time_expire, :now
           from files_for_me ffm
           where not exists
               (select 1 from t_xfer_state xs
                where xs.fileid = ffm.fileid
                  and xs.to_node = ffm.to_node)
	     and not exists
               (select 1 from t_xfer_replica xr
                where xr.fileid = ffm.fileid
                  and xr.node = ffm.to_node)},
	":me" => $node, ":now" => $now);
    $dbh->commit();

    ######################################################################
    # Phase 4: Deactivate unsuccessful expired requests.

    # (Do this every ten minutes or so.)

    # The request can fail either because the tender never completed with
    # a successful route, or the transfers don't complete in time.  In the
    # former case we might just as well wait for a while.  In latter, the
    # t_xfer_expired handling picks up details and translates them to link
    # penalties.  So all we do here is put the request to sleep.
    
    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 1, time_expire = :now + 6*3600*least(attempt,8)
        where destination = :node
          and state = 0
          and :now >= time_expire},
	":node" => $node, ":now" => $now);
    # $dbh->commit();

    ######################################################################
    # Phase 5: Wake previously failed requests from sleep.

    # (Do this every ten minutes or so.)

    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 0,
            attempt = attempt + 1,
            time_expire = :now + 8*3600*(1+least(attempt-1,6)/2)
        where destination = :node
          and state = 1
          and :now >= time_expire},
	":node" => $node, ":now" => $now);
    # $dbh->commit();
    
    ######################################################################
    $dbh->commit();

    ######################################################################
    # Phase X: Update transfer parameters.

    # (This should be done by other code.)

    ######################################################################
    # Phase X: Update statistics.

    # (This should be done by other code.)

    # update t_dest_histogram (t_xfer_{request,offer,confirmation,expired})
    # update t_xfer_histogram (t_xfer_{state,completed,tracking})
    # clean t_xfer_expired
    # copy t_xfer_tracking to t_xfer_history?
    # clean t_xfer_tracking

}

# Calculate prototype file transfer cost.  The only factor affecting
# the cost of a file in the network is its source node and whether
# the file is staged in; the rest is determined by link parameters.
# So there is no reason to calculate the full minimum-spanning tree
# algorithm for every file -- we just calculate prototype costs for
# "staged file at node n", and propagate those costs to the entire
# network.  The actual file routing then just picks cheapest paths.
sub routeCost
{
    my ($self, $links, $costs, $node, $state, $sizebin) = @_;

    # If we already have a cost for this prototype, return immediately
    return if (exists $$costs{$node}
	       && exists $$costs{$node}{$state}
	       && exists $$costs{$node}{$state}{$sizebin});

    # Initialise the starting point: instant access for staged file,
    # 0h30 for not staged.  We optimise the transfer cost as the
    # estimated time of arrival, i.e. minimise transfer time,
    # accounting for the link latency (existing transfer queue).
    my %todo = ($node => 1);
    my $latency = $state ? 0 : 1800;
    my $paths = $$costs{$node}{$state}{$sizebin} = {};
    $$paths{$node} = {
	SRC_NODE => $node,
	FROM_NODE => $node,
	TO_NODE => $node,
	LINK_LATENCY => 0,
	LINK_RATE => undef,
	XFER_LATENCY => $latency,
	TOTAL_LINK_LATENCY => 0,
	TOTAL_XFER_LATENCY => $latency,
	TOTAL_LATENCY => $latency,
	LOCAL_BOOST => 1,
	HOPS => 0,
	REMOTE_HOPS => 0
    };

    # Now use Dijkstra's algorithm to compute minimum spanning tree.
    while (%todo)
    {
	foreach my $from (keys %todo)
	{
	    # Remove from list of nodes to do.
	    delete $todo{$from};

	    # Compute cost at each neighbour.
	    foreach my $to (keys %{$$links{$from}})
	    {
		my $rate = $$links{$from}{$to}{XFER_RATE};
		my $latency = $$links{$from}{$to}{XFER_LATENCY} || 0;
		my $nominal = 0.1*MEGABYTE / $$links{$from}{$to}{HOPS};
		my $xfer = (defined ($rate)
			    ? ($rate ? $sizebin / $rate : 7*86400)
			    : $sizebin / $nominal);
		my $total = $$paths{$from}{TOTAL_LATENCY} + $latency + $xfer;
	        my $local = ($$paths{$from}{LOCAL_BOOST}
			     * ((! exists $$links{$to}
			         || ! exists $$links{$to}{$from})
		                ? 0 : $$links{$to}{$from}{LOCAL_BOOST}));

		# If we would involve more than one WAN hop, incur penalty.
		if ($$paths{$from}{REMOTE_HOPS} + (1-$local) > 1)
		{
		    &logmsg ("route from $from to $to has more than 1 remote"
			     . " hop, penalising");
		    $xfer += 7*86400;
		    $total += 7*86400;
	  	}

		# If this looks too bad to be used, cut it off.
		if ($total >= 3*86400)
		{
		    &logmsg("route from $from to $to for $sizebin is too"
			    . " expensive ($total), cutting it off");
		    next;
	        }

		# Update the path if there is none yet, if we have local
		# path and existing is not local, or if we now have a
		# better cost without changing local attribute.
		if (! exists $$paths{$to}
		    || ($local && ! $$paths{$to}{LOCAL_BOOST})
		    || ($local == $$paths{$to}{LOCAL_BOOST}
			&& $total < $$paths{$to}{TOTAL_LATENCY}))
		{
		    # No existing path or it's more expensive.
		    $$paths{$to} = { SRC_NODE => $$paths{$from}{SRC_NODE},
				     FROM_NODE => $from,
				     TO_NODE => $to,
				     LINK_LATENCY => $latency,
				     LINK_RATE => $rate,
				     XFER_LATENCY => $xfer,
				     TOTAL_LINK_LATENCY => $$paths{$from}{TOTAL_LINK_LATENCY} + $latency,
				     TOTAL_XFER_LATENCY => $$paths{$from}{TOTAL_XFER_LATENCY} + $xfer,
				     TOTAL_LATENCY => $total,
				     LOCAL_BOOST => $local,
				     HOPS => $$paths{$from}{HOPS} + 1,
			     	     REMOTE_HOPS => $$paths{$from}{REMOTE_HOPS} + (1-$local) };
		    $todo{$to} = 1;
		}
	    }
	}
    }
}

# Computes the optimal route for the file.
sub routeFile
{
    my ($self, $dest, $links, $costs, $request) = @_;

    # Use the precomupted replica path costs to pick the cheapest
    # available file we could transfer.  The costs are scaled by the
    # size of the file (it doesn't affect the result, but goes into
    # the tables on output).
    my $best = undef;
    my $bestcost = undef;
    my $sizebin = $$request{SIZEBIN};
    foreach my $node (keys %{$$request{REPLICAS}})
    {
	my $state = $$request{REPLICAS}{$node};
	next if (! exists $$costs{$node}
		 || ! exists $$costs{$node}{$state}
		 || ! exists $$costs{$node}{$state}{$sizebin}
	 	 || ! exists $$costs{$node}{$state}{$sizebin}{$dest});
	my $this = $$costs{$node}{$state}{$sizebin};

	my $thiscost = $$this{$dest}{TOTAL_LATENCY};
	if (! defined $best
	    || $$this{$dest}{LOCAL_BOOST} > $$best{$dest}{LOCAL_BOOST}
	    || ($$this{$dest}{LOCAL_BOOST} == $$best{$dest}{LOCAL_BOOST}
		&& $thiscost <= $bestcost))
	{
	    $best = $this;
	    $bestcost = $thiscost;
	}
    }

    # Now record path to the cheapest replica found, if we found one.
    delete $$request{PATH};
    if (defined $best)
    {
	my $index = 0;
	my $node = $dest;
	my $local = 1;
	while ($$best{$node}{FROM_NODE} != $$best{$node}{TO_NODE})
	{
	    my $from = $$best{$node}{FROM_NODE};
	    my $item = { %{$$best{$node}} };
	    $$item{INDEX} = $index++;
	    push(@{$$request{PATH}}, $item);
	    $node = $from;
	}
	
	delete $$self{WARNED}{$dest}{$$request{FILEID}};
    }
    elsif (keys %{$$request{REPLICAS}})
    {
	&warn ("failed to route file $$request{FILEID} to destination $dest"
	       . ": none of the source replicas are reachable ("
	       . (scalar keys %{$$request{REPLICAS}}) . " at: "
	       . "@{[ keys %{$$request{REPLICAS}} ]})")
           if ! $$self{WARNED}{$dest}{$$request{FILEID}};
	$$self{WARNED}{$dest}{$$request{FILEID}} = 1;
    }
    else
    {
	&warn ("failed to route file $$request{FILEID} to destination $dest"
	       . ": no source replica available")
           if ! $$self{WARNED}{$dest}{$$request{FILEID}};
	$$self{WARNED}{$dest}{$$request{FILEID}} = 1;
    }
}
