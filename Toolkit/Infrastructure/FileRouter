#!/usr/bin/env perl

##H Route files toward destination nodes.
##H
##H Usage:
##H   FileRouter -state DIRECTORY -db FILE[:SECTION] [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FileRouter (%args))->process();

######################################################################
# Routines for this agent.
package FileRouter; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(max);
use UtilsLogging;
use UtilsCatalogue;
use UtilsTiming;
use UtilsDB;

use constant TERABYTE => 1024**4;
use constant GIGABYTE => 1024**3;
use constant MEGABYTE => 1024**2;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My node name
		  WAITTIME => 60 + rand(10));	# Agent activity cycle
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and pass them to slaves.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my @nodes;

    eval
    {
	my $now = &mytimeofday();
	$$self{NODES} = [ '%' ];
	($dbh, @nodes) = &expandNodesAndConnect ($self,
	    { "FileDownload" => 5400, "FileExport" => 5400 });

	# First pick up returned task status.
	$self->receive($dbh);

        # Update priority on existing requests.
        &dbexec ($dbh, qq{
            update (select xq.priority req_priority, bd.priority cur_priority
                    from t_xfer_request xq
                      join t_dps_block_dest bd
                        on bd.block = xq.inblock
                        and bd.destination = xq.destination)
            set req_priority = cur_priority});

        # Clear requests for files no longer wanted.
        &dbexec($dbh, qq{
            delete from t_xfer_request xq where not exists
              (select 1 from t_dps_block_dest bd
	       where bd.destination = xq.destination
	         and bd.block = xq.inblock
	         and bd.state = 1)});

	# Clear old paths and those missing an active request.
	&dbexec($dbh, qq{
            delete from t_xfer_path xp where not exists
	      (select 1 from t_xfer_request xq
	       where xq.destination = xp.destination
		 and xq.fileid = xp.fileid
		 and xq.state = 0)});
    
        # If transfers path are about to expire on links which have
        # reasonable recent transfer rate, give a bit more grace time.
        &dbexec($dbh, qq{
	    update t_xfer_path xp
	    set xp.time_expire = :now + 8*3600
	    where xp.time_expire >= :now
	      and xp.time_expire < :now + 3600
	      and exists
	        (select 1 from t_adm_link_param lp
	         join t_adm_link l on l.id = lp.link
	         where l.from_node = xp.from_node
	           and l.to_node = xp.to_node
       	           and lp.xfer_rate >= 1048576)},
	    ":now" => $now);

        &dbexec($dbh, qq{
	    update t_xfer_task xt
	    set xt.time_expire =
	      (select max(xp.time_expire)
	       from t_xfer_path xp
	       where xp.from_node = xt.from_node
	         and xp.to_node = xt.to_node
                 and xp.fileid = xt.fileid)
            where exists
	        (select 1 from t_xfer_path xp
	         where xp.fileid = xt.fileid
	           and xp.from_node = xt.from_node
                   and xp.to_node = xt.to_node
	           and xp.time_expire > xt.time_expire)});

        &dbexec($dbh, qq{
	    update t_xfer_request xq
	    set xq.time_expire =
	      (select max(xp.time_expire)
	       from t_xfer_path xp
	       where xp.fileid = xq.fileid
	         and xp.destination = xq.destination)
            where xq.state = 0 and exists
	        (select 1 from t_xfer_path xp
	         where xp.fileid = xq.fileid
	           and xp.destination = xq.destination
	           and xp.time_expire > xq.time_expire)});

        # Mark as expired tasks which didn't complete in time.
	&dbexec($dbh, qq{
	    insert into t_xfer_task_done
	    (task, report_code, xfer_code, time_update)
	    select id, -1, -1, :now from t_xfer_task
	    where time_expire >= :now},
	    ":now" => $now);

        # Deactivate requests which reached their expire time limit.
        &dbexec($dbh, qq{
            update t_xfer_request
            set state = 1, time_expire = :now + 2*3600
            where state = 0 and :now >= time_expire},
	    ":now" => $now);

        # Auto-export for sites running exports without stage-in.
        &dbexec($dbh, qq{
	    insert into t_xfer_task_export (task, time_update)
	    select xt.id, :now from t_xfer_task xt
	      join t_adm_node ns
	        on ns.id = xt.from_node and ns.kind = 'Disk'
	      join t_xfer_source xs
	        on xs.from_node = xt.from_node
		and xs.to_node = xt.to_node
	        and xs.time_update >= :now - 5400},
            ":now" => $now);

	# Auto transfer for MSS -> Buffer transitions.
        &dbexec($dbh, qq{
	    insert into t_xfer_task_export (task, time_update)
	    select xt.id, :now from t_xfer_task xt
	      join t_adm_node ns on ns.id = xt.from_node and ns.kind = 'MSS'
	      join t_adm_node nd on ns.id = xt.to_node and nd.kind = 'Buffer'},
            ":now" => $now);

        &dbexec($dbh, qq{
	    insert into t_xfer_task_inxfer (task, time_update)
	    select xt.id, :now from t_xfer_task xt
	      join t_adm_node ns on ns.id = xt.from_node and ns.kind = 'MSS'
	      join t_adm_node nd on ns.id = xt.to_node and nd.kind = 'Buffer'},
            ":now" => $now);

        &dbexec($dbh, qq{
	    insert into t_xfer_task_done
	    (task, report_code, xfer_code, time_update)
	    select xt.id, 0, 0, :now from t_xfer_task xt
	      join t_adm_node ns on ns.id = xt.from_node and ns.kind = 'MSS'
	      join t_adm_node nd on ns.id = xt.to_node and nd.kind = 'Buffer'},
            ":now" => $now);

        # Commit the lot above.
        $dbh->commit();

	# All of this must run in a single agent to avoid database
	# connection proliferation.  The execution order for the
	# phases is important in that it balances progress for this
	# node (destination) and requests by other nodes (relaying).
	# Some of the steps feed to the next one.
	#
	# New requests begin in open state.  Offers will then begin to
	# build.  Once first offer reaches the destination, we mark
	# the request to go active in twice the time elapsed from the
	# opening.  Once a request is active, we confirm the path that
	# Has least total cost.
	#
	# Consider running only one instance of this agent for all
	# nodes.  If the agent passes one loop quickly enough, this
	# will reduce number of connections and database load
	# significantly.

	# Read links and their parameters.  Only consider links which
	# are "alive", i.e. have a live download agent at destination
	# and an export agent at the source.
	my $links = {};
	my $q = &dbexec($dbh, qq{
	    select l.from_node, l.to_node, l.distance, l.is_local,
	           p.xfer_rate, p.xfer_latency,
		   xso.protocols, xsi.protocols
	    from t_adm_link l
	      join t_adm_node ns on ns.id = l.from_node
	      join t_adm_node nd on nd.id = l.to_node
	      left join t_adm_link_param p on p.link = l.id
	      left join t_xfer_source xso
	        on xso.from_node = ns.id
		and xso.to_node = nd.id
	        and xso.time_update >= :recent
	      left join t_xfer_sink xsi
	        on xsi.from_node = ns.id
		and xsi.to_node = nd.id
	        and xsi.time_update >= :recent
	    where (ns.kind = 'MSS' and nd.kind = 'Buffer')
	       or (ns.kind = 'Buffer' and nd.kind = 'MSS'
       		   and xsi.from_node is not null)
	       or (xso.from_node is not null
       		   and xsi.from_node is not null)},
	    ":recent" => &mytimeofday() - 5400);
	while (my ($from, $to, $hops, $local, $rate, $latency,
		   $src_protos, $dest_protos) = $q->fetchrow())
	{
	    $$links{$from}{$to} = { HOPS => $hops,
				    IS_LOCAL => $local eq 'y' ? 1 : 0,
				    XFER_RATE => $rate,
			    	    XFER_LATENCY => $latency,
			    	    FROM_PROTOS => $src_protos,
			    	    TO_PROTOS => $dest_protos };
	}

	# Now route for the nodes
	my $costs = {};
	my $cats = {};
	foreach my $node (@nodes)
	{
	    my $nodeid = $$self{NODES_ID}{$node};
	    $self->route ($dbh, $nodeid, $links, $costs, $cats);
	    $self->maybeStop();
	}

	if (@nodes)
	{
	    &logmsg ("routed for @nodes");
	}
	else
	{
	    &logmsg ("nothing to route for");
	}
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Have a little rest
    $self->nap ($$self{WAITTIME});
}

# Harvest completed transfers.
sub receive
{
    my ($self, $dbh) = @_;
    my $q = &dbexec($dbh, qq{
	select
	    xtd.task, xt.fileid, xtd.report_code, xtd.xfer_code,
	    xtd.log_xfer, xtd.log_detail, xtd.log_validate,
	    xt.from_node, ns.name from_node_name,
	    xt.to_node, nd.name to_node_name,
	    xt.priority, f.filesize, f.logical_name,
	    xt.time_assign time_assign,
	    xt.time_expire time_expire,
	    xte.time_update time_export,
	    xti.time_update time_xfer,
	    xtd.time_update time_done
	from t_xfer_task_done xtd
	  join t_xfer_task xt on xt.id = xtd.task
	  join t_xfer_file f on f.id = xt.fileid
	  join t_adm_node ns on ns.id = xt.from_node
	  join t_adm_node nd on nd.id = xt.to_node
	  left join t_xfer_task_export xte on xte.task = xtd.task
	  left join t_xfer_task_inxfer xti on xti.task = xtd.task});
    while (my $task = $q->fetchrow_hashref())
    {
	# First update the statistics.  Create a time bin for the
	# period when the transfer ended if one doesn't exist, then
	# update the statistics according to result.
	my %stats = (TIME_EXPORT => "avail", TIME_XFER => "try",
		     TIME_DONE => ($$task{REPORT_CODE} == 0 ? "done"
				  : $$task{REPORT_CODE} == 3002 ? "expire"
				  : "fail"));
	while (my ($t, $stat) = each %stats)
	{
	    eval { &dbexec($dbh, qq{
	        insert into t_history_link
	        (timebin, timewidth, from_node, to_node, priority)
	        values (:bin, :width, :from_node, :to_node, :priority)},
	        ":bin" => int($$task{$t}/300)*300,
	        ":width" => 300,
	        ":from_node" => $$task{FROM_NODE},
	        ":to_node" => $$task{TO_NODE},
	        ":priority" => $$task{PRIORITY}) };
	    die $@ if $@ && $@ !~ /ORA-00001:/;

	    &dbexec($dbh, qq(
	        update t_history_link
	        set ${stat}_files = nvl(${stat}_files,0) + 1,
	            ${stat}_bytes = nvl(${stat}_bytes,0) + :filesize
	        where timebin = :bin
	          and from_node = :from_node
                  and to_node = :to_node
                  and priority = :priority),
                ":bin" => int($$task{$t}/300)*300,
	        ":filesize" => $$task{FILESIZE},
	        ":from_node" => $$task{FROM_NODE},
	        ":to_node" => $$task{TO_NODE},
	        ":priority" => $$task{PRIORITY});
	}

	# If the transfer was successful, create destination replica.
	# Otherwise if this was a transfer to the destination, put the
	# request into cool-off.
	if ($$task{REPORT_CODE} == 0)
	{
	    &dbexec($dbh, qq{
		insert into t_xfer_replica
		(id, node, fileid, state, time_create, time_state)
		values
		(seq_xfer_replica.nextval, :node, :fileid, 0, :t, :t)},
		":node" => $$task{TO_NODE},
		":fileid" => $$task{FILEID},
		":t" => $$task{TIME_DONE});
	}
	else
	{
	    &dbexec($dbh, qq{
		update t_xfer_request
		set state = 1, time_expire = :now + 3600
		where fileid = :fileid and destination = :node},
		":fileid" => $$task{FILEID},
		":node" => $$task{TO_NODE});
	}

	# Remove the task and its paraphernalia.
	&dbexec($dbh, qq{delete from t_xfer_task where id = :task},
	    ":task" => $$task{TASK});

        # Log the outcome.
	$$task{LOG_XFER} =~ s/\s+/ /gs;
	$$task{LOG_DETAIL} =~ s/\s+/ /gs;
	$$task{LOG_VERIFY} =~ s/\s+/ /gs;
	&logmsg("xstats:"
		. " task=$$task{TASK}"
		. " file=$$task{FILEID}"
		. " from=$$task{FROM_NODE_NAME}"
		. " to=$$task{TO_NODE_NAME}"
		. " priority=$$task{PRIORITY}"
		. " report-code=$$task{REPORT_CODE}"
		. " xfer-code=$$task{XFER_CODE}"
		. " size=$$task{FILESIZE}"
		. ($$task{TIME_EXPIRE} ? " t-expire=$$task{TIME_EXPIRE}" : "")
		. ($$task{TIME_ASSIGN} ? " t-assign=$$task{TIME_ASSIGN}" : "")
		. ($$task{TIME_EXPORT} ? " t-export=$$task{TIME_EXPORT}" : "")
		. ($$task{TIME_XFER}   ? " t-xfer=$$task{TIME_XFER}" : "")
		. ($$task{TIME_DONE}   ? " t-done=$$task{TIME_DONE}" : "")
		. " lfn=$$task{LOGICAL_NAME}"
		. " xfer_detail=($$task{LOG_DETAIL})"
		. " xfer_verify=($$task{LOG_VERIFY})"
		. " xfer_log=($$task{LOG_XFER})");

        $dbh->commit();
    }
}

# Run the routing algorithm for one node.
sub route
{
    my ($self, $dbh, $node, $links, $costs, $cats) = @_;

    ######################################################################
    # Phase 1: Issue file requests for blocks pending some of their files.
    #
    # In this phase, we request files for blocks in the order of the block
    # priority.  We first check how many files are a) currently in transfer
    # to this node, and b) (un)confirmed for transfer to this node.  If
    # these do not exceed defined limits (a: 1 TB, b: 10 TB), we issue new
    # requests for files with no outstanding request.
    #
    # Note that files which cannot be routed in reasonable amount of time
    # leave requests in "inactive" state for some time.  This deliberately
    # prevents this phase from issuing a request for that file again, thus
    # preventing database churn when routing requests fail to complete in
    # reasonable amount of time.  This behaviour blocks routing for files
    # and sites that do not function properly, but avoids loading database
    # servers and allows other sites to make timely progress.

    # Get current transfer load parameters.
    my ($pend_me, $pend_all, $confirm_me, $confirm_all) = (0) x 4;
    my (%pend_bytes, %confirm_bytes);
    my $now = &mytimeofday();
    my $q = &dbexec($dbh, qq{
	select xt.priority, nvl(sum(f.filesize),0)
        from t_xfer_task xt join t_xfer_file f on f.id = xt.fileid
        where xt.to_node = :node group by xt.priority},
	":node" => $node);
    while (my ($priority, $bytes) = $q->fetchrow())
    {
	$pend_bytes{$priority} = $bytes;
	$pend_me += $bytes if $priority % 2 == 0;
	$pend_all += $bytes;
    }

    $q = &dbexec($dbh, qq{
	select xp.priority, nvl(sum(f.filesize),0)
        from t_xfer_path xp join t_xfer_file f on f.id = xp.fileid
        where xp.to_node = :node group by xp.priority},
	":node" => $node);
    while (my ($priority, $bytes) = $q->fetchrow())
    {
	$confirm_bytes{$priority} = $bytes;
	$confirm_me += $bytes if $priority % 2 == 0;
	$confirm_all += $bytes;
    }

    # FIXME: What we need is flexible bandwidth management, in proportions
    # 4:3:2:1 for P0:P1:P2:P3 (P0 = me/high, P1 = others/high, P2 = me/low,
    # P3 = others/low).  (If less actual priority levels needed, just
    # chop proportions from left, e.g. 2:1 for P1:P3 actual need.)
    #
    # We should limit the number of requests, confirmations and transfers
    # roughly in these proportions.
    if (($pend_all < 5 * TERABYTE
	 && $confirm_all < 5 * TERABYTE
	 && $confirm_me < 1 * TERABYTE)
	|| ($confirm_me < 0.25 * TERABYTE))
    {
	# Find block destinations we can activate.  The activation
	# takes a lock on the block itself so files cannot be added
	# while we are creating file requests; any files added after
	# that will automatically add file requests as well.  Keep
	# adding blocks until we've added at least 500 files; select
	# the blocks in order of priority and age (highest priority
	# = smallest priority value).
	my @blocks;
	my $nbytes = 0;
	my $i = &dbprep($dbh, qq{
	    insert into t_xfer_request
	      (fileid, inblock, destination, priority, state,
	       attempt, time_create, time_expire)
	      select
	        id, :block inblock, :node destination, :priority priority,
		0 state, 1 attempt, :now, :now + 8*3600
	      from t_xfer_file
	      where inblock = :block});
	my $u = &dbprep($dbh, qq{
	    update t_dps_block_dest
	    set state = 1, time_active = :now
	    where block = :block and destination = :node});
	my $q = &dbexec($dbh, qq{
	    select bd.block, bd.priority, b.bytes
	    from t_dps_block_dest bd
	      join t_dps_block b on b.id = bd.block
	    where bd.destination = :node and bd.state = 0
	    order by bd.priority asc, bd.time_create asc},
	    ":node" => $node);
        while (my ($block, $priority, $bytes) = $q->fetchrow())
	{
	    &dbbindexec($u,
		        ":block" => $block,
		    	":node" => $node,
			":now" => $now);
	    push(@blocks, [ $block, $priority ]);
	    last if ($nbytes += $bytes) >= TERABYTE;
	}

	# Commit first phase so any concurrent modification of t_xfer_file
	# and t_xfer_replica is handled by our triggers.
	$dbh->commit();

	foreach my $item (@blocks)
	{
	    &dbbindexec($i,
		        ":block" => $$item[0],
		    	":node" => $node,
			":priority" => $$item[1],
			":now" => $now);
	}

	# Now commit second phase.
	$dbh->commit();
    }

    ######################################################################
    # Reactivate expired file requests.
    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 0, attempt = attempt+1, time_expire = :now + 8*3600
        where destination = :me and state = 1 and :now >= time_expire},
	":me" => $node, ":now" => $now);
    $dbh->commit();

    ######################################################################
    # Phase 2: Expand file requests into transfer paths through the
    # network.  For each request we build a minimum cost path from
    # available replicas using a routing table of network links and
    # current traffic conditions.  The transfer paths are refreshed
    # regularly to account for changes in network conditions.
    #
    # In other words, each destination node decides the entire path
    # for each file, using network configuration information it
    # obtains from other nodes.  For correctness it is important that
    # the entire route is built by one node using a consistent network
    # snapshot, building routes piecewise at each node using only
    # local information does not produce correct results.
    #
    # We begin with file replicas for each active file request and
    # current network conditions.  We calculate a least-cost transfer
    # path for each file.  We then update the database.

    # Read requests and replicas for requests without paths
    my $requests = {};
    $q = &dbexec($dbh, qq{
	select
	    xq.fileid, f.filesize,
	    xq.priority, xq.time_create, xq.time_expire,
	    xr.node, xr.state
	from t_xfer_request xq
	  join t_xfer_file f
	    on f.id = xq.fileid
	  join t_xfer_replica xr
	    on xr.fileid = xq.fileid
	where xq.state = 0
	  and xq.destination = :node
	  and xq.time_expire > :now
	  and not exists (select 1 from t_xfer_path xp
			  where xp.destination = xq.destination
			    and xp.fileid = xq.fileid)},
	":node" => $node, ":now" => $now);
    while (my ($file, $size, $priority, $create, $expire,
	       $rnode, $state) = $q->fetchrow())
    {
	my $sizebin = (int($size / (500*MEGABYTE))+1)*(500*MEGABYTE);
	$$requests{$file} ||= { FILEID => $file,
				FILESIZE => $size,
				SIZEBIN => $sizebin,
				PRIORITY => $priority,
				TIME_CREATE => $create,
				TIME_EXPIRE => $expire };
	$$requests{$file}{REPLICAS}{$rnode} = $state;
	$self->routeCost($links, $costs, $rnode, $state, $sizebin);
    }

    # Build optimal file paths
    $self->routeFile($node, $links, $costs, $_) for values %$requests;

    # Update database
    my %iargs;
    my $istmt = &dbprep($dbh, qq{
	insert into t_xfer_path
	(fileid, destination, hop, src_node, from_node, to_node,
	 priority, is_local, cost, total_cost, penalty,
	 time_request, time_confirm, time_expire)
	values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)});

    foreach my $req (values %$requests)
    {
	foreach my $hop (@{$$req{PATH}})
	{
	    my $n = 1;
	    push(@{$iargs{$n++}}, $$req{FILEID});
	    push(@{$iargs{$n++}}, $node);
	    push(@{$iargs{$n++}}, $$hop{INDEX});
	    push(@{$iargs{$n++}}, $$hop{SRC_NODE});
	    push(@{$iargs{$n++}}, $$hop{FROM_NODE});
	    push(@{$iargs{$n++}}, $$hop{TO_NODE});
	    push(@{$iargs{$n++}}, $$req{PRIORITY});
	    push(@{$iargs{$n++}}, $$hop{IS_LOCAL});
	    push(@{$iargs{$n++}}, ($$hop{LINK_LATENCY} || 0) + ($$hop{XFER_LATENCY} || 0));
	    push(@{$iargs{$n++}}, ($$hop{TOTAL_LATENCY} || 0));
	    push(@{$iargs{$n++}}, ($$hop{LINK_RATE} || 0));
	    push(@{$iargs{$n++}}, $$req{TIME_CREATE});
	    push(@{$iargs{$n++}}, $now);
	    push(@{$iargs{$n++}}, $$req{TIME_EXPIRE});
	}
    }

    &dbbindexec($istmt, %iargs) if %iargs;
    $dbh->commit();
    undef $requests;

    ######################################################################
    # Phase 3: Confirm requests for which there are valid offers.

    # Discard duplicate confirmations for me for the same file.  Keep
    # the best alternative: local one, or least cost one, or pick one.
    &dbexec($dbh, qq{
	delete from t_xfer_path this
	where to_node = :node and exists
	  (select 1 from t_xfer_path better
	   where this.fileid = better.fileid
	     and this.to_node = better.to_node
	     and ((this.is_local = 0 and better.is_local > 0)
		  or this.total_cost > better.total_cost
		  or (this.total_cost = better.total_cost
		      and this.rowid > better.rowid)))},
	":node" => $node);

    # Confirm transfers where I am the path hop destination, the
    # source replica exists and there is no transfer yet.
    $q = &dbexec($dbh, qq{
	with files_for_me as
	  (select
	       xp.fileid, f.logical_name, xp.from_node, xp.to_node, xr.id replica,
	       min(xp.priority) priority, max(xp.is_local) is_local,
	       max(xp.time_expire) time_expire
	   from t_xfer_path xp
	     join t_xfer_replica xr
	       on xr.fileid = xp.fileid
	       and xr.node = xp.from_node
             join t_xfer_file f
	       on f.id = xp.fileid
           where xp.to_node = :me and xp.time_expire > :now
	   group by xp.fileid, f.logical_name, xp.from_node, xp.to_node, xr.id)
	select
	   fileid, logical_name, replica, from_node, to_node,
	   2*priority + 1-is_local priority, time_expire
	from files_for_me ffm
	where not exists
	   (select 1 from t_xfer_task xt
	    where xt.fileid = ffm.fileid
	      and xt.to_node = ffm.to_node)
          and not exists
	   (select 1 from t_xfer_replica xr
	    where xr.fileid = ffm.fileid
	      and xr.node = ffm.to_node)},
	":me" => $node, ":now" => $now);

    my @tasks;
    while (my $task = $q->fetchrow_hashref())
    {
	$self->makeTransferTask($dbh, $task, $cats, $links);
	push(@tasks, $task);
    }

    my $rank = 0;
    %iargs = ();
    $istmt = &dbprep($dbh, qq{
	insert into t_xfer_task (id, fileid, from_replica, priority, rank,
	  from_node, to_node, from_pfn, to_pfn, time_expire, time_assign)
	values (seq_xfer_task.nextval, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)});
    foreach my $task (sort { $$a{PRIORITY} <=> $$b{PRIORITY}
		             || $$a{LOGICAL_NAME} cmp $$b{LOGICAL_NAME} }
		      @tasks)
    {
	my $n = 1;
	next if ! $$task{FROM_PFN};
	next if ! $$task{TO_PFN};
	push(@{$iargs{$n++}}, $$task{FILEID});
	push(@{$iargs{$n++}}, $$task{REPLICA});
	push(@{$iargs{$n++}}, $$task{PRIORITY});
	push(@{$iargs{$n++}}, $rank++);
	push(@{$iargs{$n++}}, $$task{FROM_NODE});
	push(@{$iargs{$n++}}, $$task{TO_NODE});
	push(@{$iargs{$n++}}, $$task{FROM_PFN});
	push(@{$iargs{$n++}}, $$task{TO_PFN});
	push(@{$iargs{$n++}}, $$task{TIME_EXPIRE});
	push(@{$iargs{$n++}}, $now);
    }

    &dbbindexec($istmt, %iargs) if %iargs;
    $dbh->commit();
}

# Calculate prototype file transfer cost.  The only factor affecting
# the cost of a file in the network is its source node and whether
# the file is staged in; the rest is determined by link parameters.
# So there is no reason to calculate the full minimum-spanning tree
# algorithm for every file -- we just calculate prototype costs for
# "staged file at node n", and propagate those costs to the entire
# network.  The actual file routing then just picks cheapest paths.
sub routeCost
{
    my ($self, $links, $costs, $node, $state, $sizebin) = @_;

    # If we already have a cost for this prototype, return immediately
    return if (exists $$costs{$node}
	       && exists $$costs{$node}{$state}
	       && exists $$costs{$node}{$state}{$sizebin});

    # Initialise the starting point: instant access for staged file,
    # 0h30 for not staged.  We optimise the transfer cost as the
    # estimated time of arrival, i.e. minimise transfer time,
    # accounting for the link latency (existing transfer queue).
    my %todo = ($node => 1);
    my $latency = $state ? 0 : 1800;
    my $paths = $$costs{$node}{$state}{$sizebin} = {};
    $$paths{$node} = {
	SRC_NODE => $node,
	FROM_NODE => $node,
	TO_NODE => $node,
	LINK_LATENCY => 0,
	LINK_RATE => undef,
	XFER_LATENCY => $latency,
	TOTAL_LINK_LATENCY => 0,
	TOTAL_XFER_LATENCY => $latency,
	TOTAL_LATENCY => $latency,
	IS_LOCAL => 1,
	HOPS => 0,
	REMOTE_HOPS => 0
    };

    # Now use Dijkstra's algorithm to compute minimum spanning tree.
    while (%todo)
    {
	foreach my $from (keys %todo)
	{
	    # Remove from list of nodes to do.
	    delete $todo{$from};

	    # Compute cost at each neighbour.
	    foreach my $to (keys %{$$links{$from}})
	    {
		my $rate = $$links{$from}{$to}{XFER_RATE};
		my $latency = $$links{$from}{$to}{XFER_LATENCY} || 0;
		my $nominal = 0.1*MEGABYTE / $$links{$from}{$to}{HOPS};
		my $xfer = (defined ($rate)
			    ? ($rate ? $sizebin / $rate : 7*86400)
			    : $sizebin / $nominal);
		my $total = $$paths{$from}{TOTAL_LATENCY} + $latency + $xfer;
	        my $thislocal = 0;
		$thislocal = 1 if (exists $$links{$from}
			           && exists $$links{$from}{$to}
			           && $$links{$from}{$to}{IS_LOCAL});
		my $local = ($thislocal && $$paths{$from}{IS_LOCAL} ? 1 : 0);

		# If we would involve more than one WAN hop, incur penalty.
		if ($$paths{$from}{REMOTE_HOPS} && ! $thislocal)
		{
		    $xfer += 7*86400;
		    $total += 7*86400;
	  	}

		# If this looks too bad to be used, cut it off.
		next if ($total >= 3*86400);

		# Update the path if there is none yet, if we have local
		# path and existing is not local, or if we now have a
		# better cost without changing local attribute.
		if (! exists $$paths{$to}
		    || ($local && ! $$paths{$to}{IS_LOCAL})
		    || ($local == $$paths{$to}{IS_LOCAL}
			&& $total < $$paths{$to}{TOTAL_LATENCY}))
		{
		    # No existing path or it's more expensive.
		    $$paths{$to} = { SRC_NODE => $$paths{$from}{SRC_NODE},
				     FROM_NODE => $from,
				     TO_NODE => $to,
				     LINK_LATENCY => $latency,
				     LINK_RATE => $rate,
				     XFER_LATENCY => $xfer,
				     TOTAL_LINK_LATENCY => $$paths{$from}{TOTAL_LINK_LATENCY} + $latency,
				     TOTAL_XFER_LATENCY => $$paths{$from}{TOTAL_XFER_LATENCY} + $xfer,
				     TOTAL_LATENCY => $total,
				     IS_LOCAL => $local,
				     HOPS => $$paths{$from}{HOPS} + 1,
			     	     REMOTE_HOPS => $$paths{$from}{REMOTE_HOPS} + (1-$thislocal) };
		    $todo{$to} = 1;
		}
	    }
	}
    }
}

# Computes the optimal route for the file.
sub routeFile
{
    my ($self, $dest, $links, $costs, $request) = @_;

    # Use the precomupted replica path costs to pick the cheapest
    # available file we could transfer.  The costs are scaled by the
    # size of the file (it doesn't affect the result, but goes into
    # the tables on output).
    my $best = undef;
    my $bestcost = undef;
    my $sizebin = $$request{SIZEBIN};
    foreach my $node (keys %{$$request{REPLICAS}})
    {
	my $state = $$request{REPLICAS}{$node};
	next if (! exists $$costs{$node}
		 || ! exists $$costs{$node}{$state}
		 || ! exists $$costs{$node}{$state}{$sizebin}
	 	 || ! exists $$costs{$node}{$state}{$sizebin}{$dest});
	my $this = $$costs{$node}{$state}{$sizebin};

	my $thiscost = $$this{$dest}{TOTAL_LATENCY};
	if (! defined $best
	    || $$this{$dest}{IS_LOCAL} > $$best{$dest}{IS_LOCAL}
	    || ($$this{$dest}{IS_LOCAL} == $$best{$dest}{IS_LOCAL}
		&& $thiscost <= $bestcost))
	{
	    $best = $this;
	    $bestcost = $thiscost;
	}
    }

    # Now record path to the cheapest replica found, if we found one.
    delete $$request{PATH};
    if (defined $best)
    {
	my $index = 0;
	my $node = $dest;
	my $local = 1;
	while ($$best{$node}{FROM_NODE} != $$best{$node}{TO_NODE})
	{
	    my $from = $$best{$node}{FROM_NODE};
	    my $item = { %{$$best{$node}} };
	    $$item{INDEX} = $index++;
	    push(@{$$request{PATH}}, $item);
	    $node = $from;
	}
	
	delete $$self{WARNED}{$dest}{$$request{FILEID}};
    }
    elsif (keys %{$$request{REPLICAS}})
    {
	&warn ("failed to route file $$request{FILEID} to destination $dest"
	       . ": none of the source replicas are reachable ("
	       . (scalar keys %{$$request{REPLICAS}}) . " at: "
	       . "@{[ keys %{$$request{REPLICAS}} ]})")
           if ! $$self{WARNED}{$dest}{$$request{FILEID}};
	$$self{WARNED}{$dest}{$$request{FILEID}} = 1;
    }
    else
    {
	&warn ("failed to route file $$request{FILEID} to destination $dest"
	       . ": no source replica available")
           if ! $$self{WARNED}{$dest}{$$request{FILEID}};
	$$self{WARNED}{$dest}{$$request{FILEID}} = 1;
    }
}

# Expand transfer task information and insert into the database.
sub makeTransferTask
{
    my ($self, $dbh, $task, $cats, $links) = @_;
    my ($from, $to) = @$task{"FROM_NODE", "TO_NODE"};
    my @from_protos = split(/\s+/, $$links{$from}{$to}{FROM_PROTOS} || '');
    my @to_protos   = split(/\s+/, $$links{$from}{$to}{TO_PROTOS} || '');
    my $from_cat    = $self->catalogue($dbh, $cats, $from);
    my $to_cat      = $self->catalogue($dbh, $cats, $to);
    my $protocol    = undef;

    # Find matching protocol.
    foreach my $p (@to_protos)
    {
	next if ! grep($_ eq $p, @from_protos);
	$protocol = $p;
	last;
    }

    # Check that we have prerequisite information to expand the file names.
    return if (! $from_cat
	       || ! $to_cat
	       || ! $$links{$from}
	       || ! $$links{$from}{$to}
	       || ! $protocol
	       || ! $$from_cat{$protocol}
	       || ! $$to_cat{$protocol});

    # Try to expand the file name.
    $$task{FROM_PFN} = &applyStorageRules($from_cat, $protocol, $to, 'pre');
    $$task{TO_PFN}   = &applyStorageRules($to_cat, $protocol, $from, 'pre');
}

# Build and cache storage mapping catalogue for a node.
sub catalogue
{
    my ($self, $dbh, $cats, $node) = @_;

    # If we haven't yet built the catalogue, fetch from the database.
    if (! exists $$cats{$node})
    {
        $$cats{$node} = {};

        my $q = &dbexec($dbh, qq{
	    select protocol, chain, destination_match, path_match, result_expr
	    from t_xfer_catalogue
	    where node = :node and rule_type = 'lfn-to-pfn'
	    order by rule_index asc},
	    ":node" => $node);

        while (my ($proto, $chain, $dest, $path, $result) = $q->fetchrow())
        {
	    # Check the pattern is valid.  If not, abort.
            eval { qr/$path/ };
	    return $$cats{$node} = {} if $@;

	    # Add the rule to our list.
	    push(@{$$cats{$node}{$proto}}, {
		    'chain' => $chain,
		    'destination-match' => $dest,
		    'path-match' => $path,
		    'result-expr' => $result });
        }
    }

    return $$cats{$node};
}

1;
