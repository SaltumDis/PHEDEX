#!/usr/bin/env perl

##H Route files toward destination nodes.
##H
##H Usage:
##H   FileRouter -state DIRECTORY -db FILE[:SECTION] [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FileRouter (%args))->process();

######################################################################
# Routines for this agent.
package FileRouter; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(max);
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

use constant TERABYTE => 1024**4;
use constant GIGABYTE => 1024**3;
use constant MEGABYTE => 1024**2;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My node name
		  WAITTIME => 60 + rand(10),	# Agent activity cycle
		  FLUSH_MARKER => undef,	# Current slow flush marker
		  FLUSH_PERIOD => 1800,		# Frequency of slow flush
		  NEXT_STATS => 0,		# Next time to refresh stats
		  NEXT_SLOW_FLUSH => 0);	# Next time to flush
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and pass them to slaves.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my @nodes;

    eval
    {
	$$self{NODES} = [ '%' ];
	($dbh, @nodes) = &expandNodesAndConnect ($self,
	    { "FileDownload" => 5400, "FileExport" => 5400 });

        # Run general flush.
	$self->flush($dbh);

	# Route files.
	$self->route($dbh, @nodes);

	# Perhaps update statistics.
	$self->stats($dbh);
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database and reset flush marker
    &disconnectFromDatabase ($self, $dbh);
    $$self{FLUSH_MARKER} = undef;

    # Have a little rest
    $self->nap ($$self{WAITTIME});
}

# Run general system flush.
sub flush
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday();

    return if $now < $$self{NEXT_SLOW_FLUSH};
    $$self{NEXT_SLOW_FLUSH} = $now + $$self{FLUSH_PERIOD};
 
    # Get the current value of marker from file pump.
    my $markerval = (defined $$self{FLUSH_MARKER} ? "currval" : "nextval");
    ($$self{FLUSH_MARKER}) = &dbexec($dbh, qq{
	select seq_xfer_done.$markerval from dual})
	->fetchrow();

    # Update priority on existing requests.
    &dbexec ($dbh, qq{
        update (select xq.priority req_priority, bd.priority cur_priority
                from t_xfer_request xq
                  join t_dps_block_dest bd
                    on bd.block = xq.inblock
                    and bd.destination = xq.destination
		where xq.priority != bd.priority)
        set req_priority = cur_priority});

    # Clear requests for files no longer wanted.
    &dbexec($dbh, qq{
        delete from t_xfer_request xq where not exists
          (select 1 from t_dps_block_dest bd
	   where bd.destination = xq.destination
	     and bd.block = xq.inblock
	     and bd.state = 1)});

    my $ndel = 0;
    my @nodes;
    my $qnode = &dbexec($dbh, qq{select id from t_adm_node});
    while (my ($node) = $qnode->fetchrow())
    {
	push(@nodes, $node);

        # Clear requests where replica exists.  This is required
        # because the request activation from block destination
        # creates requests for files for which replica may exist.
        my ($stmt, $rows) = &dbexec($dbh, qq{
            delete from t_xfer_request xq
	    where destination = :node and exists
              (select 1 from t_xfer_replica xr
	       where xr.fileid = xq.fileid
	         and xr.node = xq.destination)},
	    ":node" => $node);
	do { $dbh->commit(); $ndel = 0 } if (($ndel += $rows) >= 10_000);
    }

    foreach my $node (@nodes)
    {
        # Clear old paths and those missing an active request.
        # Clear invalid expired paths.
        # Clear valid paths, that expired more than 8 hours ago.
        my ($stmt, $rows) = &dbexec($dbh, qq{
            delete from t_xfer_path xp
	    where xp.to_node = :node
	      and (not exists (select 1 from t_xfer_request xq
	                       where xq.destination = xp.destination
	                         and xq.fileid = xp.fileid
	                         and xq.state = 0)
	      or (xp.is_valid = 0 and :now >= xp.time_expire)
	      or (xp.is_valid = 1 and :now >= xp.time_expire + 8*3600))},
	    ":node" => $node, ":now" => $now);
	do { $dbh->commit(); $ndel = 0 } if (($ndel += $rows) >= 10_000);

        # Set the path go again for issuer.
        &dbexec($dbh, qq{
            delete from t_xfer_exclude
	    where to_node = :node},
	    ":node" => $node);
    }


    $dbh->commit() if $ndel;

    # If transfers path are about to expire on links which have
    # reasonable recent transfer rate, give a bit more grace time.
    my %extend;
    my $qextend = &dbexec($dbh, qq{
        select xp.fileid, xp.destination, xp.from_node, xp.to_node
        from t_xfer_path xp
	where xp.time_expire >= :now
	  and xp.time_expire < :now + 2*3600
	  and xp.is_valid = 1
	  and exists
	    (select 1 from t_adm_link_param lp
	     where lp.from_node = xp.from_node
	       and lp.to_node = xp.to_node
	       and lp.xfer_rate >= 1048576)},
	":now" => $now);
    while (my ($file, $dest, $from, $to) = $qextend->fetchrow())
    {
	push(@{$extend{1}}, $now + 6*3600 + rand(6*3600));
	push(@{$extend{2}}, $to);
	push(@{$extend{3}}, $file);
    }

    if (%extend)
    {
        &dbexec($dbh, qq{
	    update t_xfer_path set time_expire = ?
	    where to_node = ? and fileid = ?},
	    %extend);
        &dbexec($dbh, qq{
	    update t_xfer_task set time_expire = ?
	    where to_node = ? and fileid = ?},
	    %extend);
        &dbexec($dbh, qq{
	    update t_xfer_request set time_expire = ?
	    where destination = ? and fileid = ?},
	    %extend);
    }

    # Mark as expired tasks which didn't complete in time.
    &dbexec($dbh, qq{
	merge into t_xfer_task_done xtd using
	  (select id from t_xfer_task where :now >= time_expire) xt
	on (xtd.task = xt.id) when not matched then
          insert (task, report_code, xfer_code, time_xfer, time_update)
	  values (xt.id, -1, -1, -1, :now)},
	":now" => $now);

    # Deactivate requests which reached their expire time limit.
    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 1, time_expire = :now + dbms_random.value(1,3)*3600
        where state = 0 and :now >= time_expire},
	":now" => $now);

    # Commit the lot above.
    $dbh->commit();
}

sub route
{
    my ($self, $dbh, @nodes) = @_;

    # All of this must run in a single agent to avoid database
    # connection proliferation.  The execution order for the
    # phases is important in that it balances progress for this
    # node (destination) and requests by other nodes (relaying).
    # Some of the steps feed to the next one.
    #
    # New requests begin in open state.  Offers will then begin to
    # build.  Once first offer reaches the destination, we mark
    # the request to go active in twice the time elapsed from the
    # opening.  Once a request is active, we confirm the path that
    # Has least total cost.
    #
    # Consider running only one instance of this agent for all
    # nodes.  If the agent passes one loop quickly enough, this
    # will reduce number of connections and database load
    # significantly.

    # Read links and their parameters.  Only consider links which
    # are "alive", i.e. have a live download agent at destination
    # and an export agent at the source.
    my $links = {};
    my $q = &dbexec($dbh, qq{
	select l.from_node, l.to_node, l.distance, l.is_local,
	       p.xfer_rate, p.xfer_latency,
	       xso.protocols, xsi.protocols
	from t_adm_link l
	  join t_adm_node ns on ns.id = l.from_node
	  join t_adm_node nd on nd.id = l.to_node
	  left join t_adm_link_param p
	    on p.from_node = l.from_node
	    and p.to_node = l.to_node
	  left join t_xfer_source xso
	    on xso.from_node = ns.id
	    and xso.to_node = nd.id
	    and xso.time_update >= :recent
	  left join t_xfer_sink xsi
	    on xsi.from_node = ns.id
	    and xsi.to_node = nd.id
	    and xsi.time_update >= :recent
	where (ns.kind = 'MSS' and nd.kind = 'Buffer')
	   or (ns.kind = 'Buffer' and nd.kind = 'MSS'
       	       and xsi.from_node is not null)
	   or (xso.from_node is not null
       	       and xsi.from_node is not null)},
	":recent" => &mytimeofday() - 5400);
    while (my ($from, $to, $hops, $local, $rate, $latency,
	       $src_protos, $dest_protos) = $q->fetchrow())
    {
	$$links{$from}{$to} = { HOPS => $hops,
				IS_LOCAL => $local eq 'y' ? 1 : 0,
				XFER_RATE => $rate,
			    	XFER_LATENCY => $latency,
			    	FROM_PROTOS => $src_protos,
			    	TO_PROTOS => $dest_protos };
    }

    # Now route for the nodes
    $self->prepare ($dbh, $$self{NODES_ID}{$_}) for @nodes;
    $self->routeFiles ($dbh, $links, sort @nodes)
        || &logmsg ("no files to route to @{[sort @nodes]}");
}

# Run the routing algorithm for one node.
sub prepare
{
    my ($self, $dbh, $node) = @_;

    ######################################################################
    # Phase 1: Issue file requests for blocks pending some of their files.
    #
    # In this phase, we request files for blocks in the order of the block
    # priority.  We first check how much transfers we have confirmed.  If
    # a certain level has been reached, we won't activate new files.

    # Get current transfer load parameters.
    my $now = &mytimeofday();
    my $confirm_valid_high = 0;
    my $confirm_valid_low = 0;
    my $confirm_invalid = 0;
    my $q = &dbexec($dbh, qq{
	select nvl(xq.priority,-1), xp.is_valid, nvl(sum(f.filesize),0)
	from t_xfer_path xp
	  join t_xfer_file f
	    on f.id = xp.fileid
	  left join t_xfer_request xq
	    on xq.fileid = xp.fileid
	    and xq.destination = xp.to_node
	where xp.to_node = :node
	group by nvl(xq.priority,-1), xp.is_valid},
	":node" => $node);
    while (my ($priority, $valid, $bytes) = $q->fetchrow())
    {
	if ($priority < 0)
	{
	    $confirm_invalid += $bytes;
	}
	elsif ($priority == 0)
	{
	    $confirm_valid_high += $bytes;
	}
	else
	{
	    $confirm_valid_low += $bytes;
	}
    }

    # Now check which priorities we pass.
    my @priorities;
    if ($confirm_invalid <= 12 * TERABYTE)
    {
        push(@priorities, [ 0, 0, 5 * TERABYTE ])
	    if $confirm_valid_high < 5 * TERABYTE;
	push(@priorities, [ 1, 100, 10 * TERABYTE ])
	    if $confirm_valid_low < 10 * TERABYTE;
    }

    foreach my $prio (@priorities)
    {
	# Find block destinations we can activate.  The activation
	# takes a lock on the block itself so files cannot be added
	# while we are creating file requests; any files added after
	# that will automatically add file requests as well.  Keep
	# adding blocks until we've added at least 500 files; select
	# the blocks in order of priority and age (highest priority
	# = smallest priority value).
	my @blocks;
	my $nbytes = 0;
	my $i = &dbprep($dbh, qq{
	    insert into t_xfer_request
	      (fileid, inblock, destination, priority, state,
	       attempt, time_create, time_expire)
	    select
	      id, :block inblock, :node destination, :priority priority,
	      0 state, 1 attempt, :now, :now + dbms_random.value(7,10)*3600
	    from t_xfer_file
	    where inblock = :block});
	my $u = &dbprep($dbh, qq{
	    update t_dps_block_dest
	    set state = 1, time_active = :now
	    where block = :block and destination = :node});
	my $q = &dbexec($dbh, qq{
	    select bd.block, bd.priority, b.bytes
	    from t_dps_block_dest bd
	      join t_dps_block b on b.id = bd.block
	    where bd.destination = :node
	      and bd.state = 0
	      and bd.priority >= :plow
	      and bd.priority <= :phigh
	    order by bd.priority asc, bd.time_create asc},
	    ":node" => $node,
	    ":plow" => $$prio[0],
	    ":phigh" => $$prio[1]);
        while (my ($block, $priority, $bytes) = $q->fetchrow())
	{
	    &dbbindexec($u,
		        ":block" => $block,
		    	":node" => $node,
			":now" => $now);
	    push(@blocks, [ $block, $priority ]);
	    last if ($nbytes += $bytes) >= $$prio[2];
	}

	# Commit first phase so any concurrent modification of t_xfer_file
	# and t_xfer_replica is handled by our triggers.
	$dbh->commit();

	foreach my $item (@blocks)
	{
	    &dbbindexec($i,
		        ":block" => $$item[0],
		    	":node" => $node,
			":priority" => $$item[1],
			":now" => $now);
	}

	# Now commit second phase.
	$dbh->commit();
    }

    ######################################################################
    # Reactivate expired file requests.
    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 0,
            attempt = attempt+1,
            time_expire = :now + dbms_random.value(7,10)*3600
        where destination = :me and state = 1 and :now >= time_expire},
	":me" => $node, ":now" => $now);
    $dbh->commit();
}

sub routeFiles
{
    my ($self, $dbh, $links, @nodes) = @_;

    ######################################################################
    # Phase 2: Expand file requests into transfer paths through the
    # network.  For each request we build a minimum cost path from
    # available replicas using a routing table of network links and
    # current traffic conditions.  The transfer paths are refreshed
    # regularly to account for changes in network conditions.
    #
    # In other words, each destination node decides the entire path
    # for each file, using network configuration information it
    # obtains from other nodes.  For correctness it is important that
    # the entire route is built by one node using a consistent network
    # snapshot, building routes piecewise at each node using only
    # local information does not produce correct results.
    #
    # We begin with file replicas for each active file request and
    # current network conditions.  We calculate a least-cost transfer
    # path for each file.  We then update the database.

    # Read requests and replicas for requests without paths
    my $now = &mytimeofday();
    my %mynodes = map { ($$self{NODES_ID}{$_} => $_) } @nodes;
    my $costs = {};
    my $ndone = 0;
    my $finished = 0;
    my $saved = undef;
    my $q = &dbexec($dbh, qq{
	select
	    xq.destination, xq.fileid, f.filesize,
	    xq.priority, xq.time_create, xq.time_expire,
	    xr.node, xr.state
	from t_xfer_request xq
	  join t_xfer_file f
	    on f.id = xq.fileid
	  join t_xfer_replica xr
	    on xr.fileid = xq.fileid
	where xq.state = 0
	  and xq.time_expire > :now
	  and not exists (select 1 from t_xfer_path xp
			  where xp.to_node = xq.destination
			    and xp.fileid = xq.fileid)
	order by destination, fileid},
	":now" => $now);
    while (! $finished)
    {
	$finished = 1;
        my %requests;
        my $nreqs = 0;
        my ($discarded, $existing) = (0, 0);
        my ($nhops, $nvalid) = (0, 0);
        my ($inserted, $updated) = (0, 0);
        while (my $row = $saved || $q->fetchrow_hashref())
        {
	    $saved = undef;
	    my $dest = $$row{DESTINATION};
	    next if ! exists $mynodes{$dest};

	    my $file = $$row{FILEID};
	    my $size = $$row{FILESIZE};
	    my $sizebin = (int($size / (500*MEGABYTE))+1)*(500*MEGABYTE);

	    if (! exists $requests{$dest}{$file})
	    {
		if ($nreqs >= 50_000)
		{
		    $finished = 0;
		    $saved = $row;
		    last;
		}
		$nreqs++;
	    }

	    $requests{$dest}{$file} ||= { DESTINATION => $dest,
				          FILEID => $file,
				          FILESIZE => $size,
				          SIZEBIN => $sizebin,
				          PRIORITY => $$row{PRIORITY},
				          TIME_CREATE => $$row{TIME_CREATE},
				          TIME_EXPIRE => $$row{TIME_EXPIRE} };
	    $requests{$dest}{$file}{REPLICAS}{$$row{NODE}} = $$row{STATE};
	    $self->routeCost($links, $costs, $$row{NODE}, $$row{STATE}, $sizebin);
        }

        # Build optimal file paths.
        my @allreqs = map { values %$_ } values %requests;
        $self->routeFile($now, $links, $costs, $_) for @allreqs;

        # Build collection of all the hops.
        my %allhops;
        foreach my $req (@allreqs)
        {
	    foreach my $hop (@{$$req{PATH}})
	    {
	        $allhops{$$hop{TO_NODE}}{$$req{FILEID}} ||= $hop;
	    }
        }

        # Compare with what is already in the database.  Keep new and better.
        my $qpath = &dbexec($dbh, qq{
	    select to_node, fileid, is_valid, is_local, total_cost
	    from t_xfer_path});
        while (my ($to, $file, $valid, $local, $cost) = $qpath->fetchrow())
        {
	    $existing++;

	    # If we are not considering replacement, skip this.
	    next if ! exists $allhops{$to}{$file};

	    # If the replacement is not better, skip this.
	    my $p = $allhops{$to}{$file};
	    if (! ($$p{IS_LOCAL} > $local
	           || ($$p{IS_LOCAL} = $local
		       && ($$p{IS_VALID} > $valid
		           || ($$p{IS_VALID} == $valid
			       && ($$p{TOTAL_LATENCY} || 0) < $cost)))))
	    {
	        $$p{UPDATE} = 0;
	        $discarded++;
	        next;
	    }

	    # The replacement is better, replace this one.
	    $$p{UPDATE} = 1;
        }

        # Build arrays for database operation.
        my (%iargs, %uargs, %destnodes);
        foreach my $to (keys %allhops)
        {
	    foreach my $file (keys %{$allhops{$to}})
	    {
	        my $hop = $allhops{$to}{$file};
	        $nhops++;

	        # Skip if we decided this wasn't worth looking at.
	        next if exists $$hop{UPDATE} && ! $$hop{UPDATE};

	        # Fill insert or update structure as appropriate.
	        my $n = 1;
	        my $args = $$hop{UPDATE} ? \%uargs : \%iargs;
	        push(@{$$args{$n++}}, $$hop{DESTINATION});
	        push(@{$$args{$n++}}, $$hop{INDEX});
	        push(@{$$args{$n++}}, $$hop{SRC_NODE});
	        push(@{$$args{$n++}}, $$hop{FROM_NODE});
	        push(@{$$args{$n++}}, $$hop{PRIORITY});
	        push(@{$$args{$n++}}, $$hop{IS_LOCAL});
	        push(@{$$args{$n++}}, $$hop{IS_VALID});
	        push(@{$$args{$n++}}, ($$hop{LINK_LATENCY} || 0) + ($$hop{XFER_LATENCY} || 0));
	        push(@{$$args{$n++}}, ($$hop{TOTAL_LATENCY} || 0));
	        push(@{$$args{$n++}}, ($$hop{LINK_RATE} || 0));
	        push(@{$$args{$n++}}, $$hop{TIME_REQUEST});
	        push(@{$$args{$n++}}, $now);
	        push(@{$$args{$n++}}, $$hop{TIME_EXPIRE});
	        push(@{$$args{$n++}}, $file);
	        push(@{$$args{$n++}}, $to);
	        $destnodes{$mynodes{$$hop{DESTINATION}}} = 1;
	        $nvalid++ if $$hop{IS_VALID};
	    }
        }

        # Insert and update paths as appropriate.
        &dbexec($dbh, qq{
	    insert into t_xfer_path
	    (destination, hop, src_node, from_node, priority, is_local,
	     is_valid, cost, total_cost, penalty, time_request,
	     time_confirm, time_expire, fileid, to_node)
	    values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)},
	    %iargs) if %iargs;

        &dbexec($dbh, qq{
	    update t_xfer_path
	    set destination = ?, hop = ?, src_node = ?, from_node = ?,
	        priority = ?, is_local = ?, is_valid = ?, cost = ?,
                total_cost = ?, penalty = ?, time_request = ?,
                time_confirm = ?, time_expire = ?
	    where fileid = ? and to_node = ?},
	    %uargs) if %uargs;

        $dbh->commit();

        # Report routing statistics.
        $inserted = %iargs ? scalar @{$iargs{1}} : 0;
        $updated = %uargs ? scalar @{$uargs{1}} : 0;
	$ndone += $inserted + $updated;
    
        &logmsg("$existing existed, $updated updated, $inserted new,"
	        . " $nvalid valid, $discarded discarded of $nhops paths"
	        . " computed for $nreqs requests for the destinations"
	        . " @{[sort keys %destnodes]}")
            if $nreqs;

	# Now would be a great time to go for a long holiday :-)
	$self->maybeStop();

	# Perhaps update statistics if we run for a long time.
	$self->stats($dbh);
    }

    # Bring next slow synchornisation forward if we didn't route any
    # files and the file pump agent has given us a hint to restart
    # and next slow flush would be relatively far away.
    if (! $ndone)
    {
	my $markerval = (defined $$self{FLUSH_MARKER}
			 ? "currval" : "nextval");
	my ($marker) = &dbexec($dbh, qq{
	    select seq_xfer_done.$markerval from dual})
	    ->fetchrow();

	$$self{NEXT_SLOW_FLUSH} = $now
	    if ($marker > ($$self{FLUSH_MARKER} || -1)
		&& $$self{NEXT_SLOW_FLUSH} > $now + $$self{FLUSH_PERIOD}/4);

	$dbh->commit();
    }

    # Return how much we did.
    return $ndone;
}

# Calculate prototype file transfer cost.  The only factor affecting
# the cost of a file in the network is its source node and whether
# the file is staged in; the rest is determined by link parameters.
# So there is no reason to calculate the full minimum-spanning tree
# algorithm for every file -- we just calculate prototype costs for
# "staged file at node n", and propagate those costs to the entire
# network.  The actual file routing then just picks cheapest paths.
sub routeCost
{
    my ($self, $links, $costs, $node, $state, $sizebin) = @_;

    # If we already have a cost for this prototype, return immediately
    return if (exists $$costs{$node}
	       && exists $$costs{$node}{$state}
	       && exists $$costs{$node}{$state}{$sizebin});

    # Initialise the starting point: instant access for staged file,
    # 0h30 for not staged.  We optimise the transfer cost as the
    # estimated time of arrival, i.e. minimise transfer time,
    # accounting for the link latency (existing transfer queue).
    my %todo = ($node => 1);
    my $latency = $state ? 0 : 1800;
    my $paths = $$costs{$node}{$state}{$sizebin} = {};
    $$paths{$node} = {
	SRC_NODE => $node,
	FROM_NODE => $node,
	TO_NODE => $node,
	LINK_LATENCY => 0,
	LINK_RATE => undef,
	XFER_LATENCY => $latency,
	TOTAL_LINK_LATENCY => 0,
	TOTAL_XFER_LATENCY => $latency,
	TOTAL_LATENCY => $latency,
	IS_LOCAL => 1,
	HOPS => 0,
	REMOTE_HOPS => 0,
	IS_PROBE => 0
    };

    # Now use Dijkstra's algorithm to compute minimum spanning tree.
    while (%todo)
    {
	foreach my $from (keys %todo)
	{
	    # Remove from list of nodes to do.
	    delete $todo{$from};

	    # Compute cost at each neighbour.
	    foreach my $to (keys %{$$links{$from}})
	    {
		# The rate estimate we use is the link nominal rate if
		# we have no performance data, where the nominal rate
		# is 0.5 MB/s divided by the database link distance.
		# If we have rate performance data and it shows the
		# link to be reasonably healthy, use that information.
		# If the link is unhealthy, "probe" the link using
		# nominal rate at 2% probability, and "infinite" time
		# otherwise.  (In the latter case the later cut-off
		# will mark the routed path as invalid.)
		my $nominal = 0.5*MEGABYTE / $$links{$from}{$to}{HOPS};
		my $latency = $$links{$from}{$to}{XFER_LATENCY} || 0;
		my $probe = ($$paths{$from}{IS_PROBE}
			     || (defined $$links{$from}{$to}{XFER_RATE}
				 && $$links{$from}{$to}{XFER_RATE} < $nominal
				 && rand() <= 0.02)
			     ? 1 : 0);
		my $rate = ($probe ? $nominal
			    : $$links{$from}{$to}{XFER_RATE});
		my $xfer = ($rate ? $sizebin / $rate : 7*86400);
		my $total = $$paths{$from}{TOTAL_LATENCY} + $latency + $xfer;
	        my $thislocal = 0;
		$thislocal = 1 if (exists $$links{$from}
			           && exists $$links{$from}{$to}
			           && $$links{$from}{$to}{IS_LOCAL});
		my $local = ($thislocal && $$paths{$from}{IS_LOCAL} ? 1 : 0);

		# If we would involve more than one WAN hop, incur penalty.
		# This value is larger than cut-off for valid paths later.
		if ($$paths{$from}{REMOTE_HOPS} && ! $thislocal)
		{
		    $xfer += 7*86400;
		    $total += 7*86400;
	  	}

		# Update the path if there is none yet, if we have local
		# path and existing is not local, or if we now have a
		# better cost without changing local attribute.
		if (! exists $$paths{$to}
		    || ($local && ! $$paths{$to}{IS_LOCAL})
		    || ($local == $$paths{$to}{IS_LOCAL}
			&& $total < $$paths{$to}{TOTAL_LATENCY}))
		{
		    # No existing path or it's more expensive.
		    $$paths{$to} = { SRC_NODE => $$paths{$from}{SRC_NODE},
				     FROM_NODE => $from,
				     TO_NODE => $to,
				     LINK_LATENCY => $latency,
				     LINK_RATE => $rate,
				     XFER_LATENCY => $xfer,
				     TOTAL_LINK_LATENCY => $$paths{$from}{TOTAL_LINK_LATENCY} + $latency,
				     TOTAL_XFER_LATENCY => $$paths{$from}{TOTAL_XFER_LATENCY} + $xfer,
				     TOTAL_LATENCY => $total,
				     IS_LOCAL => $local,
				     HOPS => $$paths{$from}{HOPS} + 1,
			     	     REMOTE_HOPS => $$paths{$from}{REMOTE_HOPS} + (1-$thislocal),
				     IS_PROBE => $probe };
		    $todo{$to} = 1;
		}
	    }
	}
    }
}

# Computes the optimal route for the file.
sub routeFile
{
    my ($self, $now, $links, $costs, $request) = @_;
    my $dest = $$request{DESTINATION};

    # Use the precomupted replica path costs to pick the cheapest
    # available file we could transfer.  The costs are scaled by the
    # size of the file (it doesn't affect the result, but goes into
    # the tables on output).
    my $best = undef;
    my $bestcost = undef;
    my $sizebin = $$request{SIZEBIN};
    foreach my $node (keys %{$$request{REPLICAS}})
    {
	my $state = $$request{REPLICAS}{$node};
	next if (! exists $$costs{$node}
		 || ! exists $$costs{$node}{$state}
		 || ! exists $$costs{$node}{$state}{$sizebin}
	 	 || ! exists $$costs{$node}{$state}{$sizebin}{$dest});
	my $this = $$costs{$node}{$state}{$sizebin};

	my $thiscost = $$this{$dest}{TOTAL_LATENCY};
	if (! defined $best
	    || $$this{$dest}{IS_LOCAL} > $$best{$dest}{IS_LOCAL}
	    || ($$this{$dest}{IS_LOCAL} == $$best{$dest}{IS_LOCAL}
		&& $thiscost <= $bestcost))
	{
	    $best = $this;
	    $bestcost = $thiscost;
	}
    }

    # Now record path to the cheapest replica found, if we found one.
    delete $$request{PATH};
    if (defined $best)
    {
	my $index = 0;
	my $node = $dest;
	my $valid = $bestcost < 3*86400 ? 1 : 0;
	while ($$best{$node}{FROM_NODE} != $$best{$node}{TO_NODE})
	{
	    my $from = $$best{$node}{FROM_NODE};
	    my $item = { %{$$best{$node}} };
	    $$item{INDEX} = $index++;
	    $$item{IS_VALID} = $valid;
	    $$item{DESTINATION} = $$request{DESTINATION};
	    $$item{PRIORITY} = $$request{PRIORITY};
	    $$item{TIME_REQUEST} = $$request{TIME_CREATE};
	    $$item{TIME_EXPIRE} = ($valid ? $$request{TIME_EXPIRE}
				   : $now+(0.5+rand(4))*3600);
	    push(@{$$request{PATH}}, $item);
	    $node = $from;
	}
	
	delete $$self{WARNED}{$dest}{$$request{FILEID}};
    }
    elsif (keys %{$$request{REPLICAS}})
    {
	&warn ("failed to route file $$request{FILEID} to destination $dest"
	       . ": none of the source replicas are reachable ("
	       . (scalar keys %{$$request{REPLICAS}}) . " at: "
	       . "@{[ keys %{$$request{REPLICAS}} ]})")
           if ! $$self{WARNED}{$dest}{$$request{FILEID}};
	$$self{WARNED}{$dest}{$$request{FILEID}} = 1;
    }
    else
    {
	&warn ("failed to route file $$request{FILEID} to destination $dest"
	       . ": no source replica available")
           if ! $$self{WARNED}{$dest}{$$request{FILEID}};
	$$self{WARNED}{$dest}{$$request{FILEID}} = 1;
    }
}

# Update transfer request and path statistics.
sub stats
{
    my ($self, $dbh, $pathinfo) = @_;
    my $now = &mytimeofday();

    # Check if we need to update statistics.
    return if $now < $$self{NEXT_STATS};
    $$self{NEXT_STATS} = int($now/300) + 300;

    # Remove previous data and add new information.
    &dbexec($dbh, qq{delete from t_status_path});
    &dbexec($dbh, qq{
	insert into t_status_path
	(time_update, from_node, to_node, priority, is_valid, files, bytes)
	select :now, xp.from_node, xp.to_node, xp.priority, xp.is_valid,
	       count(xp.fileid), nvl(sum(f.filesize),0)
	from t_xfer_path xp join t_xfer_file f on f.id = xp.fileid
	group by :now, xp.from_node, xp.to_node, xp.priority, xp.is_valid},
	":now" => $now);

    &dbexec($dbh, qq{delete from t_status_request});
    &dbexec($dbh, qq{
	insert into t_status_request
	(time_update, destination, state, files, bytes)
	select :now, xq.destination, xq.state,
	       count(xq.fileid), nvl(sum(f.filesize),0)
	from t_xfer_request xq join t_xfer_file f on f.id = xq.fileid
        group by :now, xq.destination, xq.state},
	":now" => $now);

    $dbh->commit();
}

1;
