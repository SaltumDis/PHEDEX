#!/usr/bin/env perl

##H Route files toward destination nodes.
##H
##H Usage:
##H   FileRouter
##H      -state DIRECTORY -node NAME -nodes PATTERN[,PATTERN...]
##H      -db FILE[:SECTION] [-log OUT]
##H
##H -state     agent state directory
##H -node      the node where this agent runs
##H -nodes     comma-separated list of node name patterns to route for;
##H            pattern wildcards are '%' (any string) and '_' (any character)
##H -db        database connection configuration parameter file
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
             "node=s"      => \$args{MYNODE},
             "nodes=s"     => sub { push(@{$args{NODES}}, split(/,/, $_[1])) },
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{MYNODE} || !$args{NODES} || !$args{DBCONFIG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FileRouter (%args))->process();

######################################################################
# Routines for this agent.
package FileRouter; use strict; use warnings; use base 'UtilsAgent';
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My node name
		  WAITTIME => 60 + rand(10),	# Agent activity cycle
		  NODES => []);			# Nodes to route for
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and pass them to slaves.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    eval
    {
	# Connect to the database.
	$dbh = &connectToDatabase ($self) or die "failed to connect";

        # All of this must run in a single agent to avoid database
	# connection proliferation.  The execution order for the
	# phases is important in that it balances progress for this
	# node (destination) and requests by other nodes (relaying).
	# Some of the steps feed to the next one.
        #
        # New requests begin in open state.  Offers will then begin
	# to build.  Once first offer reaches the destination, we mark
	# the request to go active in twice the time elapsed from the
	# opening.  Once a request is active, we confirm the path that
	# has least total cost.
        #
	# Consider running only one instance of this agent for all
	# nodes.  If the agent passes one loop quickly enough, this
	# will reduce number of connections and database load
	# significantly.
	foreach my $nodepat (@{$$self{NODES}})
	{
	    my $stmt = &dbexec ($dbh, qq{
		select id from t_node where name like :pat},
		":pat" => $nodepat);
	    while (my ($node) = $stmt->fetchrow())
	    {
	        $self->routeForNodes ($dbh, $node, $nodepat);
	        $self->maybeStop();
	    }
	}
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Have a little rest
    $self->nap ($$self{WAITTIME});
}

# Run the routing algorithm for one node.
sub routeForNodes
{
    my ($self, $dbh, $node, $nodepat) = @_;

    ######################################################################
    # Phase 1: Issue file requests for blocks pending some of their files.
    #
    # In this phase, we request files for blocks in the order of the block
    # priority.  We first check how many files are a) currently in transfer
    # to this node, and b) (un)confirmed for transfer to this node.  If
    # these do not exceed defined limits (a: 1 TB, b: 5 TB), we issue new
    # requests for files with no outstanding request.
    #
    # Note that files which cannot be routed in reasonable amount of time
    # leave requests in "inactive" state for some time.  This deliberately
    # prevents this phase from issuing a request for that file again, thus
    # preventing database churn when routing requests fail to complete in
    # reasonable amount of time.  This behaviour blocks routing for files
    # and sites that do not function properly, but avoids loading database
    # servers and allows other sites to make timely progress.
    # (Do this every five minutes or so.)

    # Get current transfer load parameters.  If there aren't any, wait.
    my $now = &mytimeofday();
    my ($timebin, %pend_bytes, %confirm_bytes) = undef;
    my $q = &dbexec($dbh, qq{
        select timebin, priority, sum(pend_bytes)
        from t_xfer_histogram
        where to_node = :node and timebin >= :now - 1800
        group by timebin, priority
        order by timebin desc, priority asc},
	":node" => $node, ":now" => $now);
    while (my ($bin, $priority, $bytes) = $q->fetchrow())
    {
	last if (defined $timebin && $bin != $timebin);
	$pend_bytes{$priority} = $bytes;
    }

    $timebin = undef;
    $q = &dbexec ($dbh, qq{
        select timebin, priority, sum(confirm_bytes)
        from t_routing_histogram
        where to_node = :node and timebin >= :now - 1800
        group by timebin, priority
        order by timebin desc, priority asc},
	":node" => $node, ":now" => $now);
    while (my ($bin, $priority, $bytes) = $q->fetchrow())
    {
	last if (defined $timebin && $bin != $timebin);
	$confirm_bytes{$priority} = $bytes;
    }

    if( ! keys %pend_bytes || ! keys %confirm_bytes )
    {
	&alert( "No pending transfers, or no unconfirmed requests." );
	#return;
    }

    # FIXME: What we need is flexible bandwidth management, in proportions
    # 4:3:2:1 for P0:P1:P2:P3 (P0 = me/high, P1 = others/high, P2 = me/low,
    # P3 = others/low).  (If less actual priority levels needed, just
    # chop proportions from left, e.g. 2:1 for P1:P3 actual need.)
    #
    # We should limit the number of requests, confirmations and transfers
    # roughly in these proportions.
    my ($pend_me, $pend_all, $confirm_me, $confirm_all) = (0) x 4;
    foreach (keys %pend_bytes)
    {
	my $x = $pend_bytes{$_} || 0;
	$pend_me += $x if $_ % 2 == 0;
	$pend_all += $x;
    }
    foreach (keys %confirm_bytes)
    {
	my $x = $confirm_bytes{$_} || 0;
	$confirm_me += $x if $_ % 2 == 0;
	$confirm_all += $x;
    }

    if (($pend_all < 5 * 1024**4
	 && $confirm_all < 5 * 1024**4
	 && $confirm_me < 1 * 1024**4)
	|| ($confirm_me < 0.25 * 1024**4))
    {
        # Issue requests for some more files.  Do at most 500 files at a time,
        # in order of priority (highest priority = smallest priority value).
	&dbexec($dbh, qq{
            insert into t_xfer_request
              (fileid, inblock, destination, priority, state,
               attempt, time_create, time_expire)
              (select * from
                (select f.id, bd.block, bd.destination,
                        bd.priority, 0 state, 1 attempt,
                        :now, :now + 3600
                 from t_dps_block_dest bd
                   join t_xfer_file f
                     on f.inblock = bd.block
                   left join t_xfer_request xq
                     on xq.fileid = f.id
                     and xq.destination = bd.destination
                   left join t_xfer_replica xr
                     on xr.fileid = f.id
                     and xr.node = bd.destination
                 where bd.destination = :node
                   and bd.time_complete is null
                   and (bd.time_suspend_until is null
                    or bd.time_suspend_until < :now)
                   and xr.node is null
                   and xq.fileid is null
                 order by bd.priority asc, bd.time_create asc
                ) where rownum < 500)},
	    ":node" => $node, ":now" => $now);
    }

    # Update priority on existing requests (every fifteen minutes or so.)
    &dbexec ($dbh, qq{
        update (select xq.priority req_priority, bd.priority cur_priority
                from t_xfer_request xq
                  join t_dps_block_dest bd
                    on bd.block = xq.inblock
                    and bd.destination = xq.destination
                where xq.destination = :node)
        set req_priority = cur_priority},
	":node" => $node);
    $dbh->commit();

    ######################################################################
    # Phase 2: Create and update offers for files I have a replica for,
    # or can deliver via one of my neighbours.
    #
    # The way this works is that first nodes that have a file replica make
    # an offer.  Then all their neighbours make further offers accounting
    # for present transfer conditions.  This cascade of offers spreads to
    # cover all nodes able to reach the source files, and finally represent
    # the cost of delivering the file to all reachable destinations.
    #
    # The offers remain valid and are updated continuously as long as some
    # destination is actively interested in the file.  As transfers create
    # new file replicas and transfer performance changes, the offers costs
    # are updated to reflect the new conditions.
    #
    # Since the offers automatically cover all reachable destinations,
    # there is only one offer per file and node at any one time.

    # Delete all my existing offers.  This cleans up when requests expire.
    &dbexec($dbh, qq{
	delete from t_xfer_offer where to_node = :node},
	":node" => $node);

    # Offer my replicas at cost=1 for staged-in, cost=100 otherwise.
    #
    # Offer files my neighbours have offered and which I don't have.  The
    # preferred (*_MSS) nodes will always win this step since a) MSS nodes
    # come first in the loop at the top, b) they are nearest to Buffers,
    # and c) the transfers from shared MSS to Buffer is always cheap.
    #
    # We use a scratch table to filter the intermediate results, otherwise
    # we would have to use a "insert into x () (with z as () select ...)",
    # which would get complex with multiple layers of "with .. as" filters.
    # We do do all files for this node in one go and then clean up.
    #
    # The first step is to select neighbour offers for files we don't have,
    # and calculate the cumulative cost based on recent performance stats.
    # We then randomly pick an offer from those that are within 50 of the
    # lowest bid.  This leaves us to create at most one offer per file for
    # files we know we don't have and know our neighbour can access.
    #
    # We add 10*distance to the neighbour cost and multiply with penalty.
    # tab: check to see if we're offering a now-non-existent replica,
    # and if we are then poison our neighbours' offer
    &dbexec($dbh, qq{
        update (
          select xo.time_expire
          from t_xfer_offer xo
            left join t_xfer_replica xr
              on xr.fileid = xo.fileid
              and xr.node = xo.from_node
          where xo.from_node = :node
          and xo.hops = 1
          and xr.fileid is null)
          set time_expire = 0},
	    ":node" => $node);

    &dbexec($dbh, qq{
        insert into t_xfer_offer_step
          (to_node, fileid, total_cost, from_node, cost, penalty, hops)
          (select
             xr.node, xr.fileid, decode(xr.state,1,1,100),
             xr.node, decode(xr.state,1,1,100), 1, 0
           from (select distinct fileid from t_xfer_request
                 where state <= 1 and :now < time_expire) xq
             join t_xfer_replica xr
               on xr.fileid = xq.fileid
               and xr.node = :node)},
       ":node" => $node, ":now" => $now);

    &dbexec($dbh, qq{
        insert into t_xfer_offer_step
          (to_node, fileid, total_cost, from_node, cost, penalty, hops)
          (select
             :node, xo.fileid, (xo.total_cost + nn.distance*10)*xp.penalty, xo.to_node,
             nn.distance*10, xp.penalty, xo.hops + 1 /* nn.distance? */
           from t_xfer_offer xo
             join t_node_neighbour nn
               on nn.from_node = xo.to_node
               and nn.to_node = :node
             join t_xfer_param xp
               on xp.from_node = nn.from_node
               and xp.to_node = nn.to_node
             left join t_xfer_offer_step xos
               on xos.fileid = xo.fileid
               and xos.to_node = :node
           where xo.time_offer >= :now - 15*60
	   and xo.time_expire > :now
             and xos.fileid is null)},
	":node" => $node, ":now" => $now);

    &dbexec($dbh, qq{
        delete from t_xfer_offer_step this
         where to_node = :node and exists
           (select * from t_xfer_offer_step better
            where this.fileid = better.fileid
              and this.to_node = better.to_node
              and (this.total_cost > better.total_cost + 50
                   or this.rowid > better.rowid))},
	":node" => $node);

    &dbexec($dbh, qq{
        insert into t_xfer_offer
          (to_node, fileid, total_cost, from_node, cost, penalty, 
	   hops, time_offer, time_expire)
          (select
             xos.to_node, xos.fileid, xos.total_cost, xos.from_node,
             xos.cost, xos.penalty, xos.hops, :now, :now+600
           from t_xfer_offer_step xos where xos.to_node = :node)},
	":node" => $node, ":now" => $now);

    &dbexec($dbh, qq{
        delete from t_xfer_offer_step where to_node = :node},
	":node" => $node);
    $dbh->commit();

    ######################################################################
    # Phase 3: Confirm requests for which there are valid offers.

    # Mark as active requests at activation time limit. (FIXME: 1 hr/hop?)
    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 1, time_expire = :now + 8 * 3600
        where destination = :node
          and state = 0
          and :now >= time_activate},
  	":node" => $node, ":now" => $now);

    # Schedule activation for open requests with a complete offer.
    &dbexec($dbh, qq{
        update (select xq.time_activate, xq.time_create
                from t_xfer_request xq
                  join t_xfer_offer xo
                    on xo.fileid = xq.fileid
                    and xo.to_node = xq.destination
		    and xo.time_expire > :now
                where xq.destination = :node
                  and xq.state = 0
                  and xq.time_activate is null)
        set time_activate = :now + (:now - time_create)},
	":node" => $node, ":now" => $now);
    $dbh->commit();

    # Confirm transfers for active requests.  This is a pumping action
    # like the offer generation above.  First update my confirmations:
    # transfer offers from neighbours.  Issue further confirmations for
    # files I have offered, were confirmed, and were from my neighbours.
    # Finally, issue transfer assignments for confirmed files which do
    # not yet have a transfer going on.
    &dbexec($dbh, qq{
        insert into t_xfer_expired
          (select * from t_xfer_confirmation
           where to_node = :node and time_expire >= :now)},
	":node" => $node, ":now" => $now);
    
    &dbexec($dbh, qq{
        delete from t_xfer_confirmation where to_node = :node},
	":node" => $node);

    &dbexec($dbh, qq{
        insert into t_xfer_confirmation
          (fileid, to_node, from_node, priority, weight,
           time_activate, time_confirm, time_expire)
          (select
             xo.fileid, xo.to_node, xo.from_node, xq.priority, 1,
             xq.time_activate, :now, xq.time_expire
           from t_xfer_request xq
             join t_xfer_offer xo
               on xo.fileid = xq.fileid
               and xo.to_node = xq.destination
	       and xo.time_expire > :now
           where xq.destination = :node
             and xq.state = 1
             and xo.from_node != xo.to_node)},
	":node" => $node, ":now" => $now);

    &dbexec($dbh, qq{
        insert into t_xfer_confirmation
          (fileid, to_node, from_node, priority, weight,
           time_activate, time_confirm, time_expire)
          (select
             xo.fileid, xo.to_node, xo.from_node, min(xc.priority), sum(xc.weight),
             min(xc.time_activate), min(xc.time_confirm), max(xc.time_expire)
           from t_xfer_offer xo
             left join t_xfer_replica xr
               on xr.fileid = xo.fileid
               and xr.node = xo.to_node
             join t_xfer_confirmation xc
               on xc.fileid = xo.fileid
               and xc.from_node = xo.to_node
               and xc.time_expire > :now
           where xo.to_node = :node and xr.fileid is null
	   and xo.time_expire > :now
           group by xo.fileid, xo.to_node, xo.from_node)},
	":node" => $node, ":now" => $now);
    $dbh->commit();

    &dbexec($dbh, qq{
        insert into t_xfer_state
          (fileid, errors, priority, weight, age,
           from_replica, from_node, from_state,
           to_node, to_state,
           time_expire, time_assign)

           with files_for_me as
             (select distinct fileid, 1 mine
              from t_xfer_request
                join t_node on id = destination
              where name like (:mypattern))
           select
             xc.fileid, 0, 2*xc.priority + nvl(ffm.mine,0),
             xc.weight, xc.time_activate,
             xr.id, xc.from_node, 0, xc.to_node, 0,
             xc.time_expire, :now
           from t_xfer_confirmation xc
             join t_xfer_replica xr
               on xr.fileid = xc.fileid
               and xr.node = xc.from_node
             left join files_for_me ffm
               on ffm.fileid = xc.fileid
             left join t_xfer_state xs
               on xs.fileid = xc.fileid
               and xs.to_node = xc.to_node
           where xc.time_expire > :now
             and xc.to_node = :me
             and xs.fileid is null},
	":me" => $node, ":now" => $now, ":mypattern" => $nodepat);
    $dbh->commit();

    ######################################################################
    # Phase 4: Delete completed transfers.
    &dbexec($dbh, qq{
        delete from t_xfer_request
        where destination = :node and fileid in
          (select fileid from t_xfer_replica where node = :node)},
	":node" => $node);
    $dbh->commit();

    ######################################################################
    # Phase 5: Deactivate unsuccessful expired requests.

    # (Do this every ten minutes or so.)

    # The request can fail either because the tender never completed with
    # a successful route, or the transfers don't complete in time.  In the
    # former case we might just as well wait for a while.  In latter, the
    # t_xfer_expired handling picks up details and translates them to link
    # penalties.  So all we do here is put the request to sleep.
    
    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 2,
            time_activate = null,
            time_expire = :now + 8 * 3600 * attempt
        where destination = :node
          and state = 0
          and :now >= time_expire},
	":node" => $node, ":now" => $now);
    $dbh->commit();

    ######################################################################
    # Phase 7: Wake previously failed requests from sleep.

    # (Do this every ten minutes or so.)

    &dbexec($dbh, qq{
        update t_xfer_request
        set state = 0,
            attempt = attempt + 1,
            time_expire = :now + 3600,
            time_activate = null
        where destination = :node
          and state = 2
          and :now >= time_expire},
	":node" => $node, ":now" => $now);
    $dbh->commit();
    
    ######################################################################
    # Phase 8: Clean up requests for files we no longer want.
    
    # (Do this every hour or so.)
    
    &dbexec($dbh, qq{
        delete from t_xfer_request
        where destination = :node and inblock not in
          (select block from t_dps_block_dest where destination = :node)},
	":node" => $node);
    $dbh->commit();

    ######################################################################
    # Phase X: Update transfer parameters.

    # (This should be done by other code.)

    # update t_xfer_param

    ######################################################################
    # Phase X: Update statistics.

    # (This should be done by other code.)

    # update t_dest_histogram (t_xfer_{request,offer,confirmation,expired})
    # update t_xfer_histogram (t_xfer_{state,completed,tracking})
    # clean t_xfer_expired
    # copy t_xfer_tracking to t_xfer_history?
    # clean t_xfer_tracking
}
