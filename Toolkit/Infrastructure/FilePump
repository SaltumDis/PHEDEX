#!/usr/bin/env perl

##H Router helper agent to push files around.
##H
##H Usage:
##H   FilePump -state DIRECTORY -db FILE[:SECTION] [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FilePump (%args))->process();

######################################################################
# Routines for this agent.
package FilePump; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(max);
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My node name
		  WAITTIME => 15 + rand(5));	# Agent activity cycle
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and pass them to slaves.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my @nodes;

    eval
    {
	$$self{NODES} = [ '%' ];
	($dbh, @nodes) = &expandNodesAndConnect ($self,
	    { "FileDownload" => 5400, "FileExport" => 5400 });

        # Auto-export/-transfer files for nodes.
	$self->transfer($dbh);

	# First pick up returned task status.
	$self->receive($dbh);
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Have a little rest
    $self->nap ($$self{WAITTIME});
}

# Auto-export/-transfer files for nodes.
sub transfer
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday();

    # Auto-export for sites running exports without stage-in.
    &dbexec($dbh, qq{
	insert into t_xfer_task_export (task, time_update)
	select xt.id, :now from t_xfer_task xt
	  join t_adm_node ns
	    on ns.id = xt.from_node
	    and ns.kind = 'Disk'
	  join t_xfer_source xs
	    on xs.from_node = xt.from_node
	    and xs.to_node = xt.to_node
	    and xs.time_update >= :now - 5400
	where not exists
	  (select 1 from t_xfer_task_export xte
	   where xte.task = xt.id)},
        ":now" => $now);

    # Export staged files for sites with stage-in.
    &dbexec($dbh, qq{
	insert into t_xfer_task_export (task, time_update)
	select xt.id, :now from t_xfer_task xt
	  join t_adm_node ns
	    on ns.id = xt.from_node
	    and ns.kind = 'Buffer'
	  join t_xfer_source xs
	    on xs.from_node = xt.from_node
	    and xs.to_node = xt.to_node
	    and xs.time_update >= :now - 5400
	where exists
            (select 1 from t_xfer_replica xr
	     where xr.node = xt.from_node
	       and xr.fileid = xt.fileid
	       and xr.state = 1)
          and not exists
	    (select 1 from t_xfer_task_export xte
	     where xte.task = xt.id)},
        ":now" => $now);

    # Auto transfer for MSS -> Buffer transitions.
    &dbexec($dbh, qq{
	insert all
	  into t_xfer_task_export (task, time_update) values (id, :now)
	  into t_xfer_task_inxfer (task, time_update) values (id, :now)
	  into t_xfer_task_done   (task, report_code, xfer_code,
				   time_update, is_done)
			          values (id, 0, 0, :now, 0)
	select xt.id from t_xfer_task xt
	  join t_adm_node ns on ns.id = xt.from_node and ns.kind = 'MSS'
	  join t_adm_node nd on nd.id = xt.to_node and nd.kind = 'Buffer'
	where not exists
	  (select 1 from t_xfer_task_done xtd where xtd.task = xt.id)},
        ":now" => $now);

    # Commit the lot above.
    $dbh->commit();
}

# Harvest completed transfers.
sub receive
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday();

    # First mark the rows we are going to read back later.  This
    # comes before the stats update below to guarantee that when
    # we below read back the tasks, we will not read more than
    # we inserted into the stats due to phantom reads.
    &dbexec($dbh, qq{
	update t_xfer_task_done
	set is_done = 1
	where is_done = 0});
    $dbh->commit();

    # Now create history bins for time values we care about.  Do
    # be minimal here as this conflicts with monitoring agents.
    # We don't care that phantom reads may introduce more rows
    # here than we will eventually read back below: what we need
    # is that at least the rows we read are covered.
    foreach my $table (qw(export inxfer done))
    {
	my $bin = 300;
        &dbexec($dbh, qq{
	    merge into t_history_link h using
              (select distinct
	         trunc(xtx.time_update/$bin)*$bin timebin,
	         xt.from_node, xt.to_node, xt.priority
	       from t_xfer_task_$table xtx
	         join t_xfer_task xt on xt.id = xtx.task) v
	    on (h.timebin = v.timebin and
	        h.from_node = v.from_node and
	        h.to_node = v.to_node and
	        h.priority = v.priority)
	    when not matched then
	      insert (timebin, timewidth, from_node, to_node, priority)
	      values (v.timebin, $bin, v.from_node, v.to_node, v.priority)});
	$dbh->commit();
    }

    # Now read in rows and create various update bundles.
    my $n = 0;
    my %history = ();
    my %done = ();
    my $q = &dbexec($dbh, qq{
	select
	    xtd.task, xtd.is_done,
	    xt.fileid, xtd.report_code, xtd.xfer_code,
	    xtd.log_xfer, xtd.log_detail, xtd.log_validate,
	    xt.from_node, ns.name from_node_name, xt.from_pfn,
	    xt.to_node, nd.name to_node_name, xt.to_pfn,
	    xt.priority, f.filesize, f.logical_name,
	    xr.id dest_replica,
	    xt.time_assign time_assign,
	    xt.time_expire time_expire,
	    xte.time_update time_export,
	    xti.time_update time_xfer,
	    xtd.time_update time_done
	from t_xfer_task_done xtd
	  join t_xfer_task xt on xt.id = xtd.task
	  join t_xfer_file f on f.id = xt.fileid
	  join t_adm_node ns on ns.id = xt.from_node
	  join t_adm_node nd on nd.id = xt.to_node
	  left join t_xfer_task_export xte on xte.task = xtd.task
	  left join t_xfer_task_inxfer xti on xti.task = xtd.task
	  left join t_xfer_replica xr
	    on xr.node = xt.to_node and xr.fileid = xt.fileid});
    while (my $task = $q->fetchrow_hashref())
    {
	# If we didn't select it before, skip it.  We don't want to
        # use this in the query 'where' clause as it should be rare
	# and seems to send the query plan bonkers.
	next if ! $$task{IS_DONE};

	# First update the statistics.  Create a time bin for the
	# period when the transfer ended if one doesn't exist, then
	# update the statistics according to result.
	my %stats = (TIME_EXPORT => "avail", TIME_XFER => "try",
		     TIME_DONE => ($$task{REPORT_CODE} == 0 ? "done"
				  : $$task{REPORT_CODE} == 3002 ? "expire"
				  : "fail"));
	while (my ($t, $stat) = each %stats)
	{
	    next if ! defined $$task{$t};
	    my $statbin = int($$task{$t}/300)*300;
            my $key = "$statbin $$task{FROM_NODE} $$task{TO_NODE} $$task{PRIORITY}";
	    $history{$key}{"${stat}_files"} ||= 0;
	    $history{$key}{"${stat}_bytes"} ||= 0;
	    $history{$key}{"${stat}_files"}++;
	    $history{$key}{"${stat}_bytes"} += $$task{FILESIZE};
	}

	# If the transfer was successful, create destination replica.
	# Otherwise if this was a transfer to the destination, put the
	# request into cool-off.
	my $msg = "xstats:";
	if ($$task{REPORT_CODE} == 0)
	{
	    if ($$task{DEST_REPLICA})
	    {
	        # Destination replica exists, discard transfer.
		# Keep the stats since the agent thought it did.
		$msg = "warning: destination replica exists, discarding:";
	    }
	}

	# Mark this task processed.
	push(@{$done{1}}, $$task{TASK});

        # Log the outcome.
	($$task{LOG_XFER} ||= '') =~ s/\s+/ /gs;
	($$task{LOG_DETAIL} ||= '') =~ s/\s+/ /gs;
	($$task{LOG_VALIDATE} ||= '') =~ s/\s+/ /gs;
	&logmsg($msg
		. " task=$$task{TASK}"
		. " file=$$task{FILEID}"
		. " from=$$task{FROM_NODE_NAME}"
		. " to=$$task{TO_NODE_NAME}"
		. " priority=$$task{PRIORITY}"
		. " report-code=$$task{REPORT_CODE}"
		. " xfer-code=$$task{XFER_CODE}"
		. " size=$$task{FILESIZE}"
		. ($$task{TIME_EXPIRE} ? " t-expire=$$task{TIME_EXPIRE}" : "")
		. ($$task{TIME_ASSIGN} ? " t-assign=$$task{TIME_ASSIGN}" : "")
		. ($$task{TIME_EXPORT} ? " t-export=$$task{TIME_EXPORT}" : "")
		. ($$task{TIME_XFER}   ? " t-xfer=$$task{TIME_XFER}" : "")
		. ($$task{TIME_DONE}   ? " t-done=$$task{TIME_DONE}" : "")
		. " lfn=$$task{LOGICAL_NAME}"
		. " from_pfn=$$task{FROM_PFN}"
		. " to_pfn=$$task{TO_PFN}"
		. " xfer_detail=($$task{LOG_DETAIL})"
		. " xfer_validate=($$task{LOG_VALIDATE})"
		. " xfer_log=($$task{LOG_XFER})");

	# If we are doing too many, quit.
	last if ++$n >= 50_000;
    }

    # Mark the above tasks processed.  We want to do this first so
    # when we come to updating the stats, we can go quickly to avoid
    # competing with the performance monitoring agents for rows.
    &dbexec($dbh, qq{
	update t_xfer_task_done
	set is_done = 2
	where task = ?}, %done) if %done;

    # Update link history.  See comment just above.
    foreach my $key (keys %history)
    {
	my ($bin, $from, $to, $priority) = split(/\s+/, $key);
	my $join = "";
	my $sql = "update t_history_link set";
	foreach my $stat (qw(avail try done fail expire))
	{
	    next if ! exists $history{$key}{"${stat}_files"};
	    $sql .= $join;
	    $sql .= " ${stat}_files = nvl(${stat}_files,0) + :${stat}_files";
	    $sql .= ", ${stat}_bytes = nvl(${stat}_bytes,0) + :${stat}_bytes";
	    $join = ",";
	}
	$sql .= " where timebin = :timebin";
	$sql .= "   and from_node = :from_node";
	$sql .= "   and to_node = :to_node";
	$sql .= "   and priority = :priority";

	my ($stmt, $rows) = &dbexec($dbh, $sql,
				    ":timebin" => $bin,
				    ":from_node" => $from,
				    ":to_node" => $to,
				    ":priority" => $priority,
				    map { (":$_" => $history{$key}{$_}) }
				    sort keys %{$history{$key}});
	die "link history updated $rows, expected to update 1\n" if $rows != 1;
    }

    # Commit now to minimise region of conflict.
    $dbh->commit();

    # OK, we can proceed now with the slow phase: everything we've
    # processed is now marked in is_done=2 state, so just create
    # destination replicas or mark the request in cool-off.
    &dbexec($dbh, qq{
	insert into t_xfer_replica
	(id, node, fileid, state, time_create, time_state)
	select
          seq_xfer_replica.nextval, xt.to_node, xt.fileid, 1 state,
	  xtd.time_update time_create, xtd.time_update time_state
        from t_xfer_task_done xtd
	  join t_xfer_task xt on xt.id = xtd.task
	where xtd.is_done = 2
	  and xtd.report_code = 0});

    &dbexec($dbh, qq{
	update t_xfer_request xq
	set state = 1, time_expire = :now + 3600
	where exists
	  (select 1 from t_xfer_task_done xtd
	   join t_xfer_task xt on xt.id = xtd.task
	   where xt.fileid = xq.fileid
	     and xt.to_node = xq.destination
	     and xtd.report_code != 0
	     and xtd.is_done = 2)},
	":now" => $now);

    # Finally remove all we've processed.
    &dbexec($dbh, qq{
	delete from t_xfer_task xt where exists
	  (select 1 from t_xfer_task_done xtd
	   where xtd.task = xt.id and xtd.is_done = 2)});

    # Presto, we are done.
    $dbh->commit();
}

1;
