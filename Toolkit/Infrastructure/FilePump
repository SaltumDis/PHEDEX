#!/usr/bin/env perl

##H Router helper agent to push files around.
##H
##H Usage:
##H   FilePump -state DIRECTORY -db FILE[:SECTION] [-log OUT]
##H
##H -state     agent state directory
##H -db        database connection configuration parameter file
##H -log       where to redirect logging information

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
	     "help|h"      => sub { &usage() });

if (@ARGV || !$args{DROPDIR} || !$args{DBCONFIG})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FilePump (%args))->process();

######################################################################
# Routines for this agent.
package FilePump; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(max);
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  MYNODE => undef,		# My node name
		  WAITTIME => 15 + rand(5));	# Agent activity cycle
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

# Called by agent main routine before sleeping.  Pick up work
# assignments from the database here and pass them to slaves.
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my @nodes;

    eval
    {
	$$self{NODES} = [ '%' ];
	($dbh, @nodes) = &expandNodesAndConnect ($self,
	    { "FileDownload" => 5400, "FileExport" => 5400 });

        # Auto-export/-transfer files for nodes.
	$self->transfer($dbh);

	# First pick up returned task status.
	$self->receive($dbh);
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh);

    # Have a little rest
    $self->nap ($$self{WAITTIME});
}

# Auto-export/-transfer files for nodes.
sub transfer
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday();

    # Auto-export for sites running exports without stage-in.
    &dbexec($dbh, qq{
	insert into t_xfer_task_export (task, time_update)
	select xt.id, :now from t_xfer_task xt
	  join t_adm_node ns
	    on ns.id = xt.from_node
	    and ns.kind = 'Disk'
	  join t_xfer_source xs
	    on xs.from_node = xt.from_node
	    and xs.to_node = xt.to_node
	    and xs.time_update >= :now - 5400
	where not exists
	  (select 1 from t_xfer_task_export xte
	   where xte.task = xt.id)},
        ":now" => $now);

    # Auto transfer for MSS -> Buffer transitions.
    &dbexec($dbh, qq{
	insert all
	  into t_xfer_task_export (task, time_update) values (id, :now)
	  into t_xfer_task_inxfer (task, time_update) values (id, :now)
	  into t_xfer_task_done   (task, report_code, xfer_code, time_update)
			          values (id, 0, 0, :now)
	select xt.id from t_xfer_task xt
	  join t_adm_node ns on ns.id = xt.from_node and ns.kind = 'MSS'
	  join t_adm_node nd on nd.id = xt.to_node /* FIXME! and nd.kind = 'Buffer' */
	where not exists
	  (select 1 from t_xfer_task_done xtd where xtd.task = xt.id)},
        ":now" => $now);

    # Commit the lot above.
    $dbh->commit();
}

# Harvest completed transfers.
sub receive
{
    my ($self, $dbh) = @_;
    my $n = 0;
    my $now = &mytimeofday();
    my $q = &dbexec($dbh, qq{
	select
	    xtd.task, xt.fileid, xtd.report_code, xtd.xfer_code,
	    xtd.log_xfer, xtd.log_detail, xtd.log_validate,
	    xt.from_node, ns.name from_node_name, xt.from_pfn,
	    xt.to_node, nd.name to_node_name, xt.to_pfn,
	    xt.priority, f.filesize, f.logical_name,
	    xt.time_assign time_assign,
	    xt.time_expire time_expire,
	    xte.time_update time_export,
	    xti.time_update time_xfer,
	    xtd.time_update time_done
	from t_xfer_task_done xtd
	  join t_xfer_task xt on xt.id = xtd.task
	  join t_xfer_file f on f.id = xt.fileid
	  join t_adm_node ns on ns.id = xt.from_node
	  join t_adm_node nd on nd.id = xt.to_node
	  left join t_xfer_task_export xte on xte.task = xtd.task
	  left join t_xfer_task_inxfer xti on xti.task = xtd.task});
    while (my $task = $q->fetchrow_hashref())
    {
	# First update the statistics.  Create a time bin for the
	# period when the transfer ended if one doesn't exist, then
	# update the statistics according to result.
	my %stats = (TIME_EXPORT => "avail", TIME_XFER => "try",
		     TIME_DONE => ($$task{REPORT_CODE} == 0 ? "done"
				  : $$task{REPORT_CODE} == 3002 ? "expire"
				  : "fail"));
	while (my ($t, $stat) = each %stats)
	{
	    next if ! defined $$task{$t};

	    # Update history.  If there's no time bin, create one then repeat.
	    for (my $i = 0; $i < 2; ++$i)
	    {
	        my ($stmt, $nrows) = &dbexec($dbh, qq(
	            update t_history_link
	            set ${stat}_files = nvl(${stat}_files,0) + 1,
	                ${stat}_bytes = nvl(${stat}_bytes,0) + :filesize
	            where timebin = :bin
	              and from_node = :from_node
                      and to_node = :to_node
                      and priority = :priority),
                    ":bin" => int($$task{$t}/300)*300,
	            ":filesize" => $$task{FILESIZE},
	            ":from_node" => $$task{FROM_NODE},
	            ":to_node" => $$task{TO_NODE},
	            ":priority" => $$task{PRIORITY});

		last if $nrows > 0;
		die "failed to update t_history_link even on second time!"
		    if $i && $nrows == 0;

		&dbexec($dbh, qq{
	            insert into t_history_link
	            (timebin, timewidth, from_node, to_node, priority)
	            values (:bin, :width, :from_node, :to_node, :priority)},
	            ":bin" => int($$task{$t}/300)*300,
	            ":width" => 300,
	            ":from_node" => $$task{FROM_NODE},
	            ":to_node" => $$task{TO_NODE},
	            ":priority" => $$task{PRIORITY});
	    }
	}

	# If the transfer was successful, create destination replica.
	# Otherwise if this was a transfer to the destination, put the
	# request into cool-off.
	if ($$task{REPORT_CODE} == 0)
	{
	    &dbexec($dbh, qq{
		insert into t_xfer_replica
		(id, node, fileid, state, time_create, time_state)
		values
		(seq_xfer_replica.nextval, :node, :fileid, 0, :t, :t)},
		":node" => $$task{TO_NODE},
		":fileid" => $$task{FILEID},
		":t" => $$task{TIME_DONE});
	}
	else
	{
	    &dbexec($dbh, qq{
		update t_xfer_request
		set state = 1, time_expire = :now + 3600
		where fileid = :fileid and destination = :node},
		":fileid" => $$task{FILEID},
		":node" => $$task{TO_NODE},
		":now" => $now);
	}

	# Remove the task and its paraphernalia.
	&dbexec($dbh, qq{delete from t_xfer_task where id = :task},
	    ":task" => $$task{TASK});

        # Log the outcome.
	($$task{LOG_XFER} ||= '') =~ s/\s+/ /gs;
	($$task{LOG_DETAIL} ||= '') =~ s/\s+/ /gs;
	($$task{LOG_VALIDATE} ||= '') =~ s/\s+/ /gs;
	&logmsg("xstats:"
		. " task=$$task{TASK}"
		. " file=$$task{FILEID}"
		. " from=$$task{FROM_NODE_NAME}"
		. " to=$$task{TO_NODE_NAME}"
		. " priority=$$task{PRIORITY}"
		. " report-code=$$task{REPORT_CODE}"
		. " xfer-code=$$task{XFER_CODE}"
		. " size=$$task{FILESIZE}"
		. ($$task{TIME_EXPIRE} ? " t-expire=$$task{TIME_EXPIRE}" : "")
		. ($$task{TIME_ASSIGN} ? " t-assign=$$task{TIME_ASSIGN}" : "")
		. ($$task{TIME_EXPORT} ? " t-export=$$task{TIME_EXPORT}" : "")
		. ($$task{TIME_XFER}   ? " t-xfer=$$task{TIME_XFER}" : "")
		. ($$task{TIME_DONE}   ? " t-done=$$task{TIME_DONE}" : "")
		. " lfn=$$task{LOGICAL_NAME}"
		. " from_pfn=$$task{FROM_PFN}"
		. " to_pfn=$$task{TO_PFN}"
		. " xfer_detail=($$task{LOG_DETAIL})"
		. " xfer_validate=($$task{LOG_VALIDATE})"
		. " xfer_log=($$task{LOG_XFER})");

        $dbh->commit() if ((++$n % 100) == 0);
    }

    $dbh->commit();
}

1;
