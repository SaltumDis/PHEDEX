#!/usr/bin/env perl

##H Parallel reliable file download agent.
##H
##H This agent monitors TMDB for files assigned to the agent, and
##H manages their download.  A separate backend (Globus, SRM, ...)
##H handles the actual transfer steps.  A configurable number of
##H files is always kept in transfer, using subprocesses to manage
##H transfers and other tasks as appropriate.  The again maintains
##H a local pool of transfer work and uploads file transfer report
##H to the database every once in a while.
##H
##H The agent creates batches of transfer "jobs" out of its queue
##H of files to transfer.  As files complete transfer, the agent
##H verifies transfer completeness, and then uploads the full
##H transfer report back to the database where the central file
##H mover manages the rest.
##H
##H Interaction with the site infrastructure is through scripts
##H named in command line arguments.  The required tools are
##H described in detail in the manual.  The general syntax is
##H CMD[,ARG...], where CMD is the actual command to execute,
##H followed by comma-separated list of arguments to pass to it.
##H
##H Usage:
##H   FileDownload
##H      -state DIRECTORY -nodes PATTERN[,PATTERN...]
##H      -db FILE[:SECTION] [-log OUT] [-verbose]
##H      [-ignore NODE[,NODE...]] [-accept NODE[,NODE...]]
##H      -validate CMD[,ARG...] [-delete CMD[,ARG...]]
##H      -backend TYPE [OPTIONS]
##H
##H Options to all backends:
##H      [-protocols NAME[,NAME...]] [-timeout SECS]
##H
##H Options to Null backend:
##H      (none)
##H
##H Options to Globus backend:
##H      [-command CMD[,ARG...]] [-jobs NJOBS]
##H
##H Options to SRM backend:
##H      [-command CMD[,ARG...]] [-batch-files N] [-jobs NJOBS]
##H
##H Options to FTS backend:
##H      -passfile FILE -server CONTACT [-batch-files N]
##H
##H -state       agent state directory
##H -nodes       patterns for the node names for which this agent runs
##H -db          database connection configuration parameter file
##H -log         where to redirect logging information
##H -verbose     include more information about internal algorithms
##H
##H -ignore      comma-separated list of nodes to ignore transfers from
##H -accept      comma-separated list of nodes to accept transfers from
##H -validate    command to verify file transfer success
##H -delete      command to delete file on failure
##H
##H -backend     the transfer backend to use: Globus, SRM, FTS, DCCP; all
##H               options that follow are passed to the backend as such
##H
##H -protocols   comma-separated list of storage protocols to accept
##H -batch-files number of files per transfer batch (SRM, FTS; default: 30)
##H -jobs        maximum number of concurrent transfer jobs (default: 5)
##H -timeout     execution time limit for commands (default: 3600)
##H -command     override the transfer command, for instance
##H               globus-url-copy,-p,3,-tcp-bs,2097152
##H -passfile    file containing FTS proxy password
##H -server      FTS server to use

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args = (CMDLINE => [ @ARGV ]);
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
	     "verbose"     => \$args{VERBOSE},
             "nodes=s"     => sub { push(@{$args{NODES}},
					 split(/,/, $_[1])) },

             "ignore=s"    => sub { push(@{$args{IGNORE_NODES}},
					 split(/,/, $_[1])) },
             "accept=s"    => sub { push(@{$args{ACCEPT_NODES}},
					 split(/,/, $_[1])) },

             "validate=s"  => sub { push(@{$args{VALIDATE_COMMAND}},
					 split(/,/, $_[1])) },
             "delete=s"    => sub { push(@{$args{DELETE_COMMAND}},
					 split(/,/, $_[1])) },

             "backend=s"   => sub { $args{BACKEND_TYPE} = $_[1]; die "!FINISH"; },

	     "help|h"      => sub { &usage() });

die "Insufficient parameters, use -h for help.\n"
    if (!$args{DROPDIR} || !$args{NODES} || !$args{DBCONFIG}
	|| !$args{VALIDATE_COMMAND} || !$args{BACKEND_TYPE});

(new FileDownload (%args, BACKEND_ARGS => [ @ARGV ]))->process();

######################################################################
# Routines specific to this agent.
package FileDownload; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(min max sum);
use File::Path qw(mkpath rmtree);
use Data::Dumper;
use UtilsCommand;
use UtilsLogging;
use UtilsTiming;
use UtilsDB;
use POSIX;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  NODES => undef,		# Nodes to operate for
	  	  IGNORE_NODES => [],		# TMDB nodes to ignore
	  	  ACCEPT_NODES => [],		# TMDB nodes to accept

		  VALIDATE_COMMAND => undef,	# Post-download test command
		  DELETE_COMMAND => undef,	# Delete bad files command
		  TIMEOUT => 600,		# Maximum execution time
		  NJOBS => 10,			# Max number of utility processes
		  WAITTIME => 15,		# Nap length between cycles
		  VERBOSE => 0,			# Verbosity

		  BACKEND_TYPE => undef,	# Backend type
		  BACKEND_ARGS => undef,	# Options to the backend

		  NODE => {},			# Node bandwidth parameters
		  LINKS => {},			# Per-link transfer parameters
		  FILES => {},			# Files to transfer

		  FIRST_ACTION => undef,	# Time of first action
		  ACTIONS => [],		# Future actions

		  TASKDIR => "$$self{DROPDIR}/tasks", # Tasks to do
		  STATS => [],			# Historical stats.

		  NEXT_SYNC => 0,		# Next time to synchronise
		  NEXT_PURGE => 0,		# Next time to purge bad data
		  NEXT_DISCONNECT => 0,		# Next time to disconnect
		  DBH_LAST_USE => 0,		# Last use of the database

		  BATCH_ID => 0,		# Number of batches created
		  BOOTTIME => time(),		# Time this agent started
	  	  );
    my %args = (@_);
    $$self{$_} = $args{$_} || $params{$_} for keys %params;
    $$self{BACKEND} = eval ("use UtilsDownload$args{BACKEND_TYPE};"
	    		    . " new UtilsDownload$args{BACKEND_TYPE}(\$self)");
    do { chomp ($@); die "Failed to create backend: $@\n" } if $@;
    -d $$self{TASKDIR} || mkdir($$self{TASKDIR}) || -d $$self{TASKDIR}
        || die "$$self{TASKDIR}: cannot create: $!\n";
    bless $self, $class;
    return $self;
}

# If stopped, tell backend to stop, then wait for all the pending
# utility jobs to complete.  All backends just abandon the jobs, and
# we try to pick up on the transfer again if the agent is restarted.
# Utility jobs usually run quickly so we let them run to completion.
sub stop
{
    my ($self) = @_;

    # Wait for utility processes to finish.
    if (@{$$self{JOBS}})
    {
        &logmsg ("waiting pending jobs to finish...");
        while (@{$$self{JOBS}})
        {
            $self->pumpJobs();
	    select(undef, undef, undef, .1);
        }
        &logmsg ("all pending jobs finished, ready to exit");
    }
    else
    {
        &logmsg ("no pending jobs, ready to exit");
    }

    # Clear to exit.
}

sub evalinfo
{
    my ($file) = @_;
    no strict 'vars';
    return eval (&input($file) || '');
}

# Reconnect the agent to the database.  If the database connection
# has been shut, create a new connection.  Update agent status.  Set
# $$self{DBH} to database handle and $$self{NODES_ID} to hash of the
# (node name, id) pairs.
sub reconnect
{
    my ($self) = @_;
    my $now = &mytimeofday();
    
    # Now connect.
    my ($dbh, @nodes) = &expandNodesAndConnect($self);

    # Indicate to file router which links are "live."
    my ($dest, %dest_args) = &myNodeFilter ($self, "l.to_node");
    my ($src, %src_args) = &otherNodeFilter ($self, "l.from_node");
    my @protos = $$self{BACKEND}->protocols();

    &dbexec($dbh, qq{
	delete from t_xfer_sink l where $dest $src},
	%dest_args, %src_args);
    &dbexec($dbh, qq{
	insert into t_xfer_sink (from_node, to_node, protocols, time_update)
	select l.from_node, l.to_node, :protos, :now from t_adm_link l
	where $dest $src},
	":protos" => "@protos", ":now" => $now, %dest_args, %src_args);

    $$self{DBH_LAST_USE} = $now;
    $$self{NEXT_DISCONNECT} = $now + 300;
}

######################################################################
# Mark a task completed.  Brings the next synchronisation into next
# fifteen minutes, and updates statistics for the current period.
sub taskDone
{
    my ($self, $task) = @_;

    # Save it.
    return 0 if ! $self->saveTask($task);

    # If next synchronisation is too far away, pull it forward.
    my $sync = &mytimeofday() + 900;
    $$self{NEXT_SYNC} = $sync if $sync < $$self{NEXT_SYNC};

    # Update statistics for the current period.
    my ($from, $to, $code) = @$task{"FROM_NODE", "TO_NODE", "REPORT_CODE"};
    my $s = $$self{STATS_CURRENT}{LINKS}{$to}{$from}
        ||= { DONE => 0, USED => 0, ERRORS => 0 };
    $$s{$code ? "ERRORS" : "DONE"}++;

    # Indicate success.
    return 1;
}

# Save a task after change of status.
sub saveTask
{
    my ($self, $task) = @_;
    return &output("$$self{TASKDIR}/$$task{TASKID}", Dumper($task));
}

######################################################################
# Start a new statistics period.  If we have more than the desired
# amount of statistics periods, remove old ones.
sub statsNewPeriod
{
    my ($self, $jobs, $tasks) = @_;
    my $now = &mytimeofday();

    # Prune recent history.
    $$self{STATS} = [ grep($now - $$_{TIME} <= 3600, @{$$self{STATS}}) ];

    # Add new period.
    my $current = $$self{STATS_CURRENT} = { TIME => $now, LINKS => {} };
    push(@{$$self{STATS}}, $current);

    # Add statistics on transfer slots used.
    foreach my $t (values %$tasks)
    {
	# Skip if the transfer task was completed or hasn't started.
	next if defined $$t{REPORT_CODE};
	next if ! grep(exists $$_{TASKS}{$$t{TASKID}}, values %$jobs);

	# It's using up a transfer slot, add to time slot link stats.
	my ($from, $to) = @$t{"FROM_NODE", "TO_NODE"};
	$$current{LINKS}{$to}{$from} ||= { DONE => 0, USED => 0, ERRORS => 0 };
	$$current{LINKS}{$to}{$from}{USED}++;
    }
}

######################################################################
# Compare local transfer pool with database and reset everything one
# or the other doesn't know about.  We assume this is the only agent
# managing transfers for the links it's given.  If database has a
# locally unknown transfer, we mark the database one lost.  If we
# have a local transfer unknown to the database, we trash the local.
sub purgeLostTransfers
{
    my ($self, $jobs, $tasks) = @_;
    my (%inlocal, %indb) = ();

    # Get the local transfer pool.  All we need is the task ids.
    $inlocal{$_} = 1 for keys %$tasks;

    # Get the database transfer pool.  Again, just task ids.
    my ($dest, %dest_args) = &myNodeFilter ($self, "xt.to_node");
    my ($src, %src_args) = &otherNodeFilter ($self, "xt.from_node");
    my $qpending = &dbexec($$self{DBH}, qq{
      select xt.id from t_xfer_task xt where $dest $src
        and exists (select 1 from t_xfer_task_inxfer where task = xt.id)
        and not exists (select 1 from t_xfer_task_done where task = xt.id)},
      %dest_args, %src_args);
    $indb{$_} = 1 while (($_) = $qpending->fetchrow());

    # Calculate differences.
    my @lostlocal = grep(! $indb{$_}, keys %inlocal);
    my @lostdb    = grep(! $inlocal{$_}, keys %indb);

    &alert("resetting database tasks lost locally: @{[sort @lostdb]}"
	   . " (locally known: @{[sort keys %inlocal]})")
	if @lostdb;
    &alert("resetting local tasks lost in database: @{[sort @lostlocal]}"
	   . " (database known: @{[sort keys %indb]})")
	if @lostlocal;

    # Mark locally unknown tasks as lost in database.
    my $now = &mytimeofday();
    my $qlost = &dbprep($$self{DBH}, qq{
      insert into t_xfer_task_done
        (task, report_code, xfer_code, time_update)
	values (:task, -2, -2, :now)});
    &dbbindexec($qlost, ":now" => $now, ":task" => $_) for @lostdb;

    # Remove locally known tasks forgotten by database.
    foreach (@lostlocal)
    {
	delete $$tasks{$_};
	unlink("$$self{TASKDIR}/$_");
    }
}

# Fetch new tasks from the database.
sub fetchNewTasks
{
    my ($self, $jobs, $tasks) = @_;
    my ($dest, %dest_args) = &myNodeFilter ($self, "xt.to_node");
    my ($src, %src_args) = &otherNodeFilter ($self, "xt.from_node");
    my $now = &mytimeofday();
    my (%pending, %busy);

    # Find out how many we have pending per link so we can throttle.
    ($pending{"$$_{FROM_NODE} -> $$_{TO_NODE}"} ||= 0)++
	for grep(! exists $$_{REPORT_CODE}, values %$tasks);

    # Fetch new tasks.
    my $i = &dbprep($$self{DBH}, qq{
	insert into t_xfer_task_inxfer (task, time_update)
	values (:task, :now)});

    my $q = &dbexec($$self{DBH}, qq{
	select
	    xt.id taskid, xt.fileid, xt.rank,
	    f.logical_name, f.filesize, f.checksum,
	    xt.from_node from_node_id, ns.name from_node,
	    xt.to_node to_node_id, nd.name to_node,
	    xt.from_pfn, xt.to_pfn,
	    xt.time_assign
	 from t_xfer_task xt
	   join t_adm_node ns on ns.id = xt.from_node
	   join t_adm_node nd on nd.id = xt.to_node
	   join t_xfer_file f on f.id = xt.fileid
	 where $dest $src
   	   and exists
	     (select 1 from t_xfer_task_export xte
	      where xte.task = xt.id)
   	   and not exists
	     (select 1 from t_xfer_task_inxfer xti
	      where xti.task = xt.id)
   	   and not exists
	     (select 1 from t_xfer_task_done xtd
	      where xtd.task = xt.id)
	   and xt.time_expire > :limit},
        ":limit" => $now + 3600, %dest_args, %src_args);
    while (my $row = $q->fetchrow_hashref())
    {
	# Make sure it doesn't exist locally, just in case.
	next if exists $$tasks{$$row{TASKID}};

	# If we have too many on this link, skip.
	my $linkkey = "$$row{FROM_NODE} -> $$row{TO_NODE}";
	if ($pending{$linkkey}++ >= 1000)
	{
            &logmsg("link $linkkey already has $pending{$linkkey} pending"
		    . " tasks, not fetching more from the database")
		if ! $busy{$linkkey} && $$self{VERBOSE};
	    $busy{$linkkey} = 1;
	    next;
	}

	# Mark used in database.
	&dbbindexec($i, ":task" => $$row{TASKID}, ":now" => $now);

	# Generate a local task descriptor.  It doesn't really matter
	# if things go badly wrong here, we'll clean it up in purge.
	return if ! &output("$$self{TASKDIR}/$$row{TASKID}", Dumper($row));
	$$tasks{$$row{TASKID}} = $row;
    }
}

# Upload final task status to the database.
sub updateTaskStatus
{
    my ($self, $tasks, $task) = @_;
    &logmsg("uploading status of task $task") if $$self{VERBOSE};
    &dbexec($$self{DBH}, qq{
	insert into t_xfer_task_done
	(task, report_code, xfer_code, time_update,
	 log_xfer, log_detail, log_validate)
	values
	(:task, :report_code, :xfer_code, :time_update,
	 :log_xfer, :log_detail, :log_validate)},
	":task"         => $$tasks{$task}{TASKID},
	":report_code"  => $$tasks{$task}{REPORT_CODE},
	":xfer_code"    => $$tasks{$task}{XFER_CODE},
	":time_update"  => $$tasks{$task}{TIME_UPDATE},
	":log_xfer"     => $$tasks{$task}{LOG_XFER},
	":log_detail"   => $$tasks{$task}{LOG_DETAIL},
	":log_validate" => $$tasks{$task}{LOG_VALIDATE});
    unlink("$$self{TASKDIR}/$task");
    delete $$tasks{$task};
}

# Push status of completed to the database and pull more tasks.
sub doSync
{
    my ($self, $jobs, $tasks) = @_;

    # Upload status for completed tasks.
    $self->updateTaskStatus($tasks, $_)
        for grep(exists $$tasks{$_}{REPORT_CODE}, keys %$tasks);

    # Fetch new tasks where necessary.
    $self->fetchNewTasks($jobs, $tasks);
    $$self{DBH}->commit();
}

# Check how a copy job is doing.
sub check
{
    my ($self, $jobname, $jobs, $tasks) = @_;

    # Perhaps stop.
    $self->maybeStop();

    # First ask backend to have a look.
    $$self{BACKEND}->check($jobname, $jobs, $tasks);

    # Prepare some useful shortcuts.  "$live" indicates whether the
    # job is still live.  If not, below we will force tasks complete.
    my $now = &mytimeofday();
    my $jobpath = "$$self{WORKDIR}/$jobname";
    my $jobinfo = $$jobs{$jobname};

    my $live = ($now - ((stat("$jobpath/live"))[9] || 0) > 600 ? 0 : 1);
    my $done = 1;

    # Check all the tasks in the job.
    foreach my $task (keys %{$$jobinfo{TASKS}})
    {
	# Prepare useful shortcuts.
	my $fxstatus = "$jobpath/T${task}X";
	my $fvstatus = "$jobpath/T${task}V";
	my $fvlog    = "$jobpath/T${task}L";
	my $taskinfo = $$tasks{$task};

	# If we lost this task, ignore the entry.
	next if ! $taskinfo;

	# Find task details.  Ignore lost tasks.
	my ($xstatus, $vstatus);
	if ((! -f $fxstatus && ! $live)
	    || (-f _ && (! ($xstatus = &evalinfo($fxstatus)) || $@)))
	{
	    $xstatus = { START => $now, END => $now, STATUS => -3,
		         DETAIL => "agent lost the transfer", LOG => "" };
	    return if ! &output($fxstatus, Dumper($xstatus));
	}

	if (-s $fvstatus && (! ($vstatus = &evalinfo($fvstatus)) || $@))
	{
	    $vstatus = { START => $now, END => $now, STATUS => -3,
		         LOG => "agent lost the file validation result" };
	    return if ! &output($fvstatus, Dumper($vstatus));
	}

	# Start verifying if transfer completed.
	if ($xstatus && ! $vstatus)
	{
	    return if ! &output($fvstatus, "");
	    $self->addJob(sub {
		    &output($fvstatus, Dumper ({
		        START => $now, END => &mytimeofday(),
		        STATUS => $_[0]{STATUS}, LOG => &input($fvlog) }));
		    $$jobinfo{RECHECK} = 1; },
	        { TIMEOUT => $$self{TIMEOUT}, LOGFILE => $fvlog },
	        @{$$self{VALIDATE_COMMAND}}, $$xstatus{STATUS},
	        @$taskinfo{qw(TO_PFN FILESIZE CHECKSUM)});
	    $done = 0;
	}

        # Update task status for fully verified tasks.
	elsif ($vstatus && ! exists $$taskinfo{REPORT_CODE})
	{
	    # If the transfer failed, issue clean-up action without
	    # waiting it to return -- just go ahead with harvesting.
	    # The caller will in any case wait before proceeding.
	    $self->addJob(sub {}, { TIMEOUT => $$self{TIMEOUT} },
		@{$$self{DELETE_COMMAND}}, "post", $$taskinfo{TO_PFN})
		if $$vstatus{STATUS} && $$self{DELETE_COMMAND};

	    # FIXME: More elaborate transfer code reporting?
	    #  - validation: successful/terminated/timed out/error + detail + log
	    #      where detail specifies specific error (size mismatch, etc.)
	    #  - transfer: successful/terminated/timed out/error + detail + log
	    $$taskinfo{REPORT_CODE} =
		($$vstatus{STATUS} =~ /^-?\d+$/ ? $$vstatus{STATUS}
		 : 128 + ($$vstatus{STATUS} =~ /(\d+)/)[0]);
	    $$taskinfo{XFER_CODE} =
		($$xstatus{STATUS} =~ /^-?\d+$/ ? $$xstatus{STATUS}
		 : 128 + ($$xstatus{STATUS} =~ /(\d+)/)[0]);
	    $$taskinfo{LOG_DETAIL} = $$xstatus{DETAIL};
	    $$taskinfo{LOG_XFER} = $$xstatus{LOG};
	    $$taskinfo{LOG_VALIDATE} = $$vstatus{LOG};
	    $$taskinfo{TIME_UPDATE} = $$vstatus{END};
	    return if ! $self->taskDone($taskinfo);
	    &logmsg("xstats:"
		    . " to=$$taskinfo{TO_NODE}"
		    . " from=$$taskinfo{FROM_NODE}"
		    . " fileid=$$taskinfo{FILEID}"
		    . " report_code=$$taskinfo{REPORT_CODE}"
		    . " xfer_code=$$taskinfo{XFER_CODE}"
		    . " size=$$taskinfo{FILESIZE}"
		    # . join("", map { sprintf(" time_%s=%.2f", $$_[0], $$_[2]-$$_[1]) }
		    #	   [ 'assigned', $$file{TIME_ASSIGN}, $now ],
		    #	   [ 'all', $$file{TIME_START}, $now ],
		    #	   @{$$file{TIMING}})
		    . " lfn=$$taskinfo{LOGICAL_NAME}"
		    . " from_pfn=$$taskinfo{FROM_PFN}"
		    . " to_pfn=$$taskinfo{TO_PFN}"
		    . " detail=($$taskinfo{LOG_DETAIL})");
	}

	# Otherwise we are still not done with this job.
	else
	{
	    $done = 0;
	}
    }

    # If we are done with the copy job, nuke it.
    if ($done)
    {
        # (FIXME: archive into a directory, failed and successful?)
	&logmsg("copy job $jobname completed") if $$self{VERBOSE};
	&rmtree($jobpath);
	delete $$jobs{$jobname};
    }
}

# Fill the backend with as many transfers as it can take.
# 
# The files are assigned to the link on a fair share basis.  Here the
# fairness means that we try to keep every link with transfers to its
# maximum capacity while spreading the number of available "transfer
# slots" as fairly as possible over the links which have transfers.
#
# The algorithm works by assigning each link weights as a ratio of
# successfully completed transfers vs. number of transfer slots used
# by the link, over the observation period (some hours).  The final
# probability assigned to each link is the fraction of its weight
# from the total weight over all links.  Every link always gets a
# probability greater than a small "epsilon" value, which guarantees
# every link eventually gets a chance to be tried.
#
# This algorithm tends to give more files to links which get through
# more files, but sharing the avaiable backend job slots fairly.  The
# weighting by consumsed transfer slots is a key factor as it permits
# the agent to detect which links benefit from being given more files.
sub fill
{
    my ($self, $jobs, $tasks) = @_;
    my (%stats, %todo, %sorted);
    my $nlinks = 0;

    # If the backend is busy, avoid doing heavy lifting.
    return if $$self{BACKEND}->isBusy($jobs, $tasks);

    # Determine links with pending transfers.
    foreach my $t (values %$tasks)
    {
	my $to = $$t{TO_NODE};
	my $from = $$t{FROM_NODE};
	next if exists $$t{REPORT_CODE};
	next if grep (exists $$_{TASKS}{$$t{TASKID}}, values %$jobs);
	$nlinks++ if ! exists $todo{$to}{$from};
	push(@{$todo{$to}{$from}}, $t);
    }

    # Another quick exit if we have nothing to do.
    return if ! $nlinks;
    &logmsg("balancing transfers on $nlinks links")
        if $$self{VERBOSE};

    # Determine link probability from recent usage.
    foreach my $slot (@{$$self{STATS}})
    {
	foreach my $to (keys %{$$slot{LINKS}})
	{
	    foreach my $from (keys %{$$slot{LINKS}{$to}})
	    {
		my $s = $$slot{LINKS}{$to}{$from};
		# Add statistics based on link usage.
		$stats{$to}{$from} ||= { USED => 0, DONE => 0, ERRORS => 0 };
		$stats{$to}{$from}{DONE} += ($$s{DONE} || 0);
		$stats{$to}{$from}{USED} += ($$s{USED} || 0);
		$stats{$to}{$from}{ERRORS} += ($$s{ERRORS} || 0);
	    }
	}
    }

    my ($W, $wmin) = (0, 0.02 * $nlinks);
    foreach my $to (keys %todo)
    {
	foreach my $from (keys %{$todo{$to}})
	{
	    my $entry = $stats{$to}{$from} ||= {};

	    # Pass links with too many errors.
	    if (($$entry{ERRORS} || 0) > 100)
	    {
		&logmsg("too many ($$entry{ERRORS}) recent errors on"
			. " link $from -> to, not allocating transfers")
		    if $$self{VERBOSE};
		delete $todo{$to}{$from};
		next;
	    }

	    # Give links the weight of one if they have not been used.
	    if (! $$entry{USED})
	    {
		$$entry{W} = 1.0;
	    }

	    # Otherwise the weight is DONE/USED.
	    else
	    {
	        $$entry{W} = (1.0 * $$entry{DONE} / $$entry{USED});
	    }

	    # But if the weight is smaller than ~5 files/hour, clamp
	    # to that limit to guarantee minimum probability value.
	    $$entry{W} = $wmin if $$entry{W} < $wmin;

	    # Update total weight.
	    $W += $$entry{W};
	}
    }

    my @P;
    foreach my $to (sort keys %todo)
    {
	foreach my $from (sort keys %{$todo{$to}})
	{
	    # Compute final link probablity function.
	    my $low = (@P ? $P[$#P]{HIGH} : 0);
	    my $high = $low + $stats{$to}{$from}{W}/$W;
	    push(@P, { LOW => $low, HIGH => $high, TO => $to, FROM => $from });

            &logmsg("link parameters for $from -> $to:"
		    . sprintf(' P=[%0.3f, %0.3f),', $P[$#P]{LOW}, $P[$#P]{HIGH})
		    . sprintf(' W=%0.3f,', $stats{$to}{$from}{W})
		    . " USED=@{[$stats{$to}{$from}{USED} || 0]},"
		    . " DONE=@{[$stats{$to}{$from}{DONE} || 0]},"
		    . " ERRORS=@{[$stats{$to}{$from}{ERRORS} || 0]}")
                if $$self{VERBOSE};
	}
    }

    # For each available job slot, determine which link should have
    # the transfers based on the probability function calculated from
    # the link statistics.  Then fill the job slot from the transfers
    # tasks on that link, in the order of task priority.
    my $exhausted = 0;
    while (! $$self{BACKEND}->isBusy($jobs, $tasks) && @P)
    {
	$self->maybeStop();

	# Select a link that merits to get the files.
	my ($i, $p) = (0, rand());
	$i++ while ($i < $#P && $p >= $P[$i]{HIGH});
	my $to = $P[$i]{TO};
	my $from = $P[$i]{FROM};

	# Get a sorted list of files for this link.
	if (! $sorted{$to}{$from})
	{
	    $todo{$to}{$from} =
	        [ sort { $$a{TIME_ASSIGN} <=> $$b{TIME_ASSIGN}
		         || $$a{RANK} <=> $$b{RANK} }
	          @{$todo{$to}{$from}} ];
	    $sorted{$to}{$from} = 1;
	}

	# Send files to transfer.
	my $id = $$self{BATCH_ID}++;
	my $jobname = "job.$$self{BOOTTIME}.$id";
	my $dir = "$$self{WORKDIR}/$jobname";
	&mkpath($dir);

	&logmsg("copy job $jobname assigned to link $from -> $to with "
		. sprintf('p=%0.3f and W=%0.3f and ', $p, $stats{$to}{$from}{W})
		. scalar(@{$todo{$to}{$from}})
		. " transfer tasks in queue")
	    if $$self{VERBOSE};
	$$self{BACKEND}->startBatch ($jobs, $tasks, $dir, $jobname, $todo{$to}{$from});

	# If we exhausted the files on this link, remove the link's
	# share from P.  We have to recalculate P then also.
	if (! @{$todo{$to}{$from}} && ! $$self{BACKEND}->isBusy($jobs, $tasks))
	{
            &logmsg("transfers on link $from -> $to exhausted,"
		    . " recalculating link probabilities")
	        if $$self{VERBOSE};

	    splice(@P, $i, 1);
	    $W -= $stats{$to}{$from}{W};
	    for ($i = 0; $i <= $#P; ++$i)
	    {
                my $to = $P[$i]{TO};
                my $from = $P[$i]{FROM};
		$P[$i]{LOW} = ($i ? $P[$i-1]{HIGH} : 0);
		$P[$i]{HIGH} = $P[$i]{LOW} + $stats{$to}{$from}{W}/$W;

                &logmsg("new link parameters for $from -> $to:"
		        . sprintf(' P=[%0.3f, %0.3f),',$P[$i]{LOW},$P[$i]{HIGH})
		        . sprintf(' W=%0.3f,', $stats{$to}{$from}{W})
			. " USED=@{[$stats{$to}{$from}{USED} || 0]},"
			. " DONE=@{[$stats{$to}{$from}{DONE} || 0]},"
			. " ERRORS=@{[$stats{$to}{$from}{ERRORS} || 0]}")
                    if $$self{VERBOSE};
	    }

	    --$nlinks;
	    ++$exhausted;
        }
    }

    # If we exhausted all transfer tasks on a link, make sure the next
    # synchronisation will occur relatively soon.  If we exhausted
    # tasks on all links, synchronise immediately.  This applies only
    # on transition from having tasks to not having them (only), so we
    # are not forcing continuous unnecessary reconnects.
    my $now = &mytimeofday();
    if (! $nlinks && $$self{NEXT_SYNC} > $now)
    {
	$$self{NEXT_SYNC} = $now-1;
	&logmsg("ran out of tasks, scheduling immediate synchronisation")
	    if $$self{VERBOSE};
    }
    elsif ($exhausted && $$self{NEXT_SYNC} - $now > 300)
    {
	$$self{NEXT_SYNC} = $now + 300;
	&logmsg("ran out of tasks on $exhausted links, scheduling"
		. " next synchronisation in five minutes")
	    if $$self{VERBOSE};
    }
}

# Initialise agent.
sub init
{
    my ($self) = @_;
    $self->statsNewPeriod({}, {});
}

# Run agent main loop.
sub idle
{
    my ($self, @pending) = @_;

    eval
    {
	my (%tasks, %jobs);
	my $now = &mytimeofday();

	# Read in and verify all transfer tasks.
	my @tasknames;
	return if ! &getdir($$self{TASKDIR}, \@tasknames);

	foreach (@tasknames)
	{
	    my $info = &evalinfo("$$self{TASKDIR}/$_");
	    if (! $info || $@)
	    {
		&alert("garbage collecting corrupted transfer task $_ ($info, $@)");
		unlink("$$self{TASKDIR}/$_");
		$$self{NEXT_PURGE} = 0;
		next;
	    }
	    $tasks{$_} = $info;
	}

	# Read in and verify all copy jobs.
	foreach (@pending)
	{
	    my $info = &evalinfo("$$self{WORKDIR}/$_/info");
	    if (! $info || $@)
	    {
		&alert("garbage collecting corrupted copy job $_");
		&rmtree("$$self{WORKDIR}/$_");
		$$self{NEXT_PURGE} = 0;
		next;
	    }
	    $jobs{$_} = $info;
	}

	# Rescan jobs for completed tasks and fill the backend a few
        # times each.  In between each round flush validation and
	# file removal processes to finalise job completion.
	my @check = grep (exists $jobs{$_}, @pending);
	for (my $i = 0; @check && $i < 5; ++$i)
	{
	    $self->check($_, \%jobs, \%tasks) for @check;
            $self->fill(\%jobs, \%tasks);

	    while (@{$$self{JOBS}})
	    {
	        $self->pumpJobs();
	        select(undef, undef, undef, .1);
	    }

	    @check = grep (exists $jobs{$_} && $jobs{$_}{RECHECK}, @pending);
    	    delete $$_{RECHECK} for values %jobs;
        }

	# Create new time period with current statistics.
        $self->statsNewPeriod(\%jobs, \%tasks);

	# Reconnect if we are due a database sync, but use a database
	# connection opportunistically if we happen to have one.
	my $need_sync = ($$self{NEXT_SYNC} <= $now ? 1 : 0);
	if ($need_sync || $$self{DBH})
	{
	    $self->reconnect() if ! $$self{DBH};
	    if ($$self{NEXT_PURGE} <= $now)
	    {
		# Kill ghost transfers in the database and locally.
		$self->purgeLostTransfers(\%jobs, \%tasks);
		$$self{NEXT_PURGE} = $now + 3600;
		$$self{DBH_LAST_USE} = $now;
	    }
	    $self->doSync(\%jobs, \%tasks);
	    $$self{NEXT_SYNC} = $now + 1800;
	    $$self{DBH_LAST_USE} = $now if $need_sync;
	}

	# Detach from the database if the connection wasn't used
	# recently (at least one minute) and if it looks the agent
	# has enough work for some time.
	if (defined $$self{DBH}
	    && $$self{NEXT_DISCONNECT} <= $now
	    && $now - $$self{DBH_LAST_USE} > 60
	    && $$self{BACKEND}->isBusy(\%jobs, \%tasks))
	{
	    &logmsg("disconnecting from database");
	    &disconnectFromDatabase($self, $$self{DBH}, 1);
	}
    };
    do { chomp ($@); &alert ($@); $$self{NEXT_PURGE} = 0;
	 eval { $$self{DBH}->rollback() } if $$self{DBH}; } if $@;

    # Clear zombie detached processes in the backend.
    $$self{BACKEND}->pumpJobs();

    # Finally have a nap.
    $self->nap($$self{WAITTIME});
}
