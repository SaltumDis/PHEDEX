#!/usr/bin/env perl

##H Parallel reliable file download agent.
##H
##H This agent monitors TMDB for files assigned to the agent, and
##H downloads them.  A separate backend (Globus, SRM, ...) is used
##H to make the actual transfer steps.  A configurable number of
##H parallel subprocesses, both for transfers and other tasks, is
##H automatically maintained.  If an error occurs in any part of
##H the transfer, the file is put into an error state and retried
##H later on.
##H
##H The agent maintains a sliding window of "wanted" files, and
##H creates batches of transferrable files.  The backend assigns
##H the files in a batch to actual transfers, and issues commands
##H to make the copies.  Once the files have been transferred a
##H check is made to verify the completeness of the transfer, and
##H if successful, the file is given to the site for publishing.
##H
##H The agent has no site-specific knowledge, it invokes several
##H commands to invoke operations that require knowledge about the
##H local configuration.  These commands are described in detail
##H in the manual.  The general syntax is CMD[,ARG...], where CMD
##H is the actual command to execute, followed by comma-separated
##H list of arguments to pass to it.
##H
##H Usage:
##H   FileDownload
##H      -state DIRECTORY -node NAME -db FILE[:SECTION] [-wait SECS]
##H      -wanted SIZE[kGMT] -backend TYPE -command CMD[,ARG...]
##H      -pfndest CMD[,ARG...] -publish CMD[,ARG...]
##H      [-validate CMD[,ARG...]] [-delete CMD[,ARG...]]
##H      [-batch-files N] [-batch-size SIZE[kMGT]]
##H      [-ignore NODE[,NODE...]] [-accept NODE[,NODE...]]
##H      [-timeout SECS] [-jobs NJOBS]
##H
##H -state       agent state directory
##H -node        the node where this agent runs
##H -db          database connection configuration parameter file
##H -wait        time to wait in seconds between work scans
##H -wanted      number of bytes to mark wanted before transfer;
##H               suffix k = kilo, M = mega, G = giga, T = tera
##H -backend     the transfer backend to use: Globus, SRM, DCCP
##H -command     override the transfer command, for instance
##H               globus-url-copy,-p,3,-tcp-bs,2097152
##H -pfndest     command to produce destination transfer name
##H -publish     command to publish file after transfer
##H -validate    command to verify file transfer success; if not specified
##H               uses -command exit code, which is normally a bad idea
##H -delete      command to delete file on failure
##H -batch-files enable batching, set maximum number of files per batch
##H -batch-size  enable batching, set maximum number of bytes per batch
##H -ignore      comma-separated list of nodes to ignore transfers from
##H -accept      comma-separated list of nodes to accept transfers from
##H -timeout     set time limit on commands to avoid e.g. hung transfers
##H -jobs        set the number of jobs to run in parallel (default: 1)

BEGIN {
  use strict; use warnings;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
use UtilsHelp;
my %args = (NJOBS => 1);
while (scalar @ARGV)
{
    if ($ARGV[0] eq '-db' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBCONFIG} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-node' && scalar @ARGV > 1)
    { shift (@ARGV); $args{MYNODE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-ignore' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{IGNORE_NODES}}, split(/,/, shift (@ARGV))); }
    elsif ($ARGV[0] eq '-accept' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{ACCEPT_NODES}}, split(/,/, shift (@ARGV))); }

    elsif ($ARGV[0] eq '-wanted' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WANT_LIMIT} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-jobs' && scalar @ARGV > 1)
    { shift (@ARGV); $args{NJOBS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-backend' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BACKEND_TYPE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-state' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DROPDIR} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-wait' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WAITTIME} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-batch-files' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BATCH_FILES} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-batch-size' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BATCH_SIZE} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-pfndest' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{PFN_GEN_COMMAND}}, split(/,/,  shift(@ARGV))); }
    elsif ($ARGV[0] eq '-publish' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{PUBLISH_COMMAND}}, split(/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-validate' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{VALIDATE_COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-delete' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{DELETE_COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-command' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-timeout' && scalar @ARGV > 1)
    { shift (@ARGV); $args{TIMEOUT} = shift (@ARGV); }

    elsif ($ARGV[0] eq '-h')
    { &usage(); }
    else
    { last; }
}

$args{WANT_LIMIT} = &sizeValue ($args{WANT_LIMIT}) if $args{WANT_LIMIT};
$args{BATCH_SIZE} = &sizeValue ($args{BATCH_SIZE}) if $args{BATCH_SIZE};

if (@ARGV || !$args{DROPDIR} || !$args{MYNODE} || !$args{DBCONFIG}
    || !$args{WANT_LIMIT} || !$args{PFN_GEN_COMMAND} || !$args{PUBLISH_COMMAND}
    || !$args{NJOBS} || !$args{BACKEND_TYPE})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FileDownload (%args))->process();

sub sizeValue
{
    my ($value) = @_;
    if ($value =~ /^(\d+)([kMGT])$/)
    {
        my %scale = ('k' => 1024, 'M' => 1024**2, 'G' => 1024**3, 'T' => 1024**4);
        $value = $1 * $scale{$2};
    }
    return $value;
}

######################################################################
# Routines specific to this agent.
package FileDownload; use strict; use warnings; use base 'UtilsAgent';
use File::Path;
use Data::Dumper;
use UtilsCommand;
use UtilsLogging;
use UtilsTiming;
use UtilsCatalogue;
use UtilsDB;
use POSIX;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
	  	  MYNODE => undef,		# My TMDB node name
	  	  IGNORE_NODES => [],		# TMDB nodes to ignore
	  	  ACCEPT_NODES => [],		# TMDB nodes to accept
		  WANT_LIMIT => undef,		# Amount of data to prefetch
		  WANT_UPDATE => 0,		# Last wanted data update
		  BACKEND => undef,		# Backend type
		  NJOBS => 1,			# Max number of parallel processes
		  JOBS => [],			# Pending jobs
		  NEXT_FILES => [],		# Cached files for transfer
		  LAST_RESET => 0,		# Last time we reset bad transfers
	  	  AGENTID => "Download");	# Identity for activity logs
    my %args = (@_);
    map { $self->{$_} = $args{$_} || $params{$_} } keys %params;
    $self->{BACKEND} = eval ("use UtilsDownload$args{BACKEND_TYPE};"
	    		     . " new UtilsDownload$args{BACKEND_TYPE}(\@_)");
    die "Failed to create backend: $@\n" if $@;
    bless $self, $class;
    return $self;
}

######################################################################
# Remove from the database all existing transfers we are not
# aware of.  This may occur if we are stopped and restarted
# forcefully when there were pending transfers.  We assume
# we are the only live transfer agent for this node, so if
# we don't know about it, it's stale and can be removed.
# Same with previously failed transfers, revert them all back.
sub resetPendingTransfers
{
    my ($self, $dbh) = @_;
    &dbexec ($dbh, qq{
	update t_transfer_state
	set to_state = 1, to_timestamp = :now
	where to_node = :node and (to_state = 2 or to_state >= 100)},
	":now" => &mytimeofday(), ":node" => $self->{MYNODE});
}

# Record completion for the transfer.  Mark files either transferred
# or send them to cool-off failure state.
sub completeTransferBatch
{
    my ($self, $batch) = @_;
    my $dbh = eval { &connectToDatabase ($self, 0) };
    if ($@)
    {
	&alert ("failed to update transfer batch status,"
		. " trying again later, error was: $@");
	$self->addJob (new sub { $self->completeTransferBatch($batch) });
	return;
    }

    # Have a connection to the database, now update status for all files
    foreach my $file (@$batch)
    {
        eval
        {
	    my $to_state = 3;
	    my $guid = $file->{GUID};
	    if ($file->{FAILURE})
	    {
	         &alert ("failed to transfer $guid: $file->{FAILURE}");
	         $to_state = 101 + int(rand(10)); # 1..10 min cool-off
	    }

	    my $now = &mytimeofday ();
	    my $dalloc = $now - $file->{TIME_ALLOC};
	    my $dtransfer = $now - $file->{TIME_START};
	    &dbexec ($dbh, qq{
		update t_transfer_state
		set to_state = :to_state, to_timestamp = :now
		where guid = :guid and to_node = :to_node},
		":to_state" => $to_state, ":now" => $now,
		":guid" => $guid, ":to_node" => $self->{MYNODE});

	    &dbexec ($dbh, qq{
                insert into t_replica_state
		(timestamp, guid, node, state, state_timestamp)
		values (:now, :guid, :node, 0, :now)},
		":guid" => $guid, ":node" => $self->{MYNODE}, ":now" => $now)
                if ! $file->{FAILURE};

	    # generate profiling info and display stats
	    my @profile_names = ();
	    my @profile_values = ();
	    if ( defined $file->{TIMING}{GET_DEST_FINISH} ) {
		push( @profile_names, "define_destination");
		push( @profile_values, 
		      $file->{TIMING}{GET_DEST_FINISH} - $file->{TIMING}{GET_DEST_START});
	    }
            if ( defined $file->{TIMING}{JOB_ADD_FINISH} ) {
                push( @profile_names, "add_job_to_queue" );
		push( @profile_values,
		      $file->{TIMING}{JOB_ADD_FINISH} - $file->{TIMING}{JOB_ADD_START});
            }
            if ( defined $file->{TIMING}{PRECLEAN_FINISH} ) {
                push( @profile_names, "preclean" );
                push( @profile_values,
		      $file->{TIMING}{PRECLEAN_FINISH} - $file->{TIMING}{PRECLEAN_START});
            }
            if ( defined $file->{TIMING}{FINISH} ) {
                push( @profile_names, "transfer" );
		push( @profile_values,
		      $file->{TIMING}{FINISH} - $file->{TIMING}{START});
            }
            if ( defined $file->{TIMING}{VALIDATE_FINISH} ) {
                push( @profile_names, "validate" );
		push( @profile_values,
		      $file->{TIMING}{VALIDATE_FINISH} - $file->{TIMING}{VALIDATE_START});
            }
            if ( defined $file->{TIMING}{POSTCLEAN_FINISH} ) {
                push( @profile_names, "postclean" );
		push( @profile_values,
		      $file->{TIMING}{POSTCLEAN_FINISH} - $file->{TIMING}{POSTCLEAN_START});
            }
            if ( defined $file->{TIMING}{UPDATE_CAT_FINISH} ) {
                push( @profile_names, "update_catalogue" );
                push( @profile_values,    
		      $file->{TIMING}{UPDATE_CAT_FINISH} - $file->{TIMING}{UPDATE_CAT_START});
            }
	    my $count = 0;
	    my $profile_string = '';
	    foreach my $name ( @profile_names ) {
		my $value = sprintf( "%.2f", $profile_values[$count] );
		$profile_string .= "$name=$value ";
		$count++;
	    }

	    # Log transfer delay stats
	    &logmsg ("xstats: $guid $self->{MYNODE} $to_state "
		     . sprintf('%.2f %.2f', $dalloc, $dtransfer)
	     	     . " $file->{FILESIZE}"
		     . " [ $profile_string ]");
            $dbh->commit ();
        };

        # If we failed to update status, something's gone terribly bad,
        # just skip this file, we will re-transfer it later again.
        if ($@)
        {
            &alert ("failed to update transfer batch status for $file->{GUID}: $@");
            eval { $dbh->rollback() } if $dbh;
        }
    };

    &disconnectFromDatabase ($self, $dbh);
}

# Begin transferring a file.  Fetches the file information and creates
# a new drop for the file in a worker.  If the drop is successfully
# created, marks the file in transfer in the database and returns
# non-zero.  Otherwise returns non-zero to indicate another attempt
# should be made at a later time to initiate the transfer.
sub startFileTransfer
{
    my ($self, $dbh, $info) = @_;
    my ($guid, $lfn, $pfntype, $size, $cksum, $from, $timestamp, $from_pfn, $attrs) = @$info;

    # Mark the transfer started in the database.
    eval
    {
	&dbexec ($dbh, qq{
	    update t_transfer_state
	    set to_state = 2, to_timestamp = :now
	    where guid = :guid and to_node = :to_node},
	    ":now" => &mytimeofday (), ":guid" => $guid,
	    ":to_node" => $self->{MYNODE});
	$dbh->commit();
    };

    if ($@)
    {
	&alert ("failed to mark $guid in transfer: $@");
	eval { $dbh->rollback() };
	return undef;
    }

    # Return a transfer object.
    return { GUID		=> $guid,
	     LFN		=> $lfn,
	     PFNTYPE		=> $pfntype,
	     FILESIZE		=> $size,
	     CHECKSUM		=> $cksum,
	     TIME_ALLOC		=> $timestamp,
	     TIME_START		=> &mytimeofday(),
	     FROM_NODE		=> $from,
	     FROM_PFN		=> $from_pfn,
	     ATTRS		=> $attrs};
}

# Update wanted status on files in the database.
sub markFilesWanted
{
    my ($self, $dbh) = @_;

    # Avoid executing very often, the updates below do take a fair
    # amount of time, and there's little point running here much
    # more than once a minute.
    my $now = &mytimeofday();
    return if $self->{WANT_UPDATE} > $now - 60;
    $self->{WANT_UPDATE} = $now;
    
    eval
    {
	my $mynode = $self->{MYNODE};

	# Tick back-off timer for previously failed transfers.
	# Each state above 100 counts as one-minute cool-down
	# period (see code above how this is assigned).
	# (FIXME: restore old time stamp?)
    	&dbexec ($dbh, qq{
	    update t_transfer_state
	    set to_state = (case when to_state > 100 then to_state - 1 else 0 end), to_timestamp = :now
	    where to_node = :node and to_state >= 100 and to_timestamp < :old},
    	    ":now" => $now, ":node" => $mynode, ":old" => $now - 60);

	# Keep wanted time stamp at most ten minutes old.
	&dbexec ($dbh, qq{
	    update t_transfer_state set to_timestamp = :now
	    where to_node = :node and to_state = 1 and to_timestamp < :old},
    	    ":now" => $now, ":node" => $mynode, ":old" => $now - 600);

  	# Check how much we already marked wanted.  Only mark new
	# files wanted if we drop below 75% of our limit.
	my ($total) = &dbexec($dbh, qq{
	    select sum(f.filesize)
	    from t_transfer_state ts
	    join t_file f on f.guid = ts.guid
	    where ts.to_node = :node and ts.to_state = 1},
    	    ":node" => $mynode)->fetchrow() || 0;
	return if $total > .75 * $self->{WANT_LIMIT};

	# Mark more files wanted.  Select the best new files, keeping
	# track of accumulated size and stopping at our limit.  The
	# ordering criteria here is decreasing day and the job-id
	# (FIXME: CMS/COBRA specific!).  We pick the newest files first
	# -- if we are moving files in streaming mode and fall behind,
	# it's better to keep up with the stream the best we can rather
	# than disrupt everyone else trying to slug our way through the
	# old files.  (FIXME: Custom file priority rules?)
	my $ustmt = &dbprep($dbh, qq{
	    update t_transfer_state
	    set to_state = 1, to_timestamp = :now
	    where guid = :guid and to_node = :node});
	my $stmt = &dbexec($dbh, qq{
	    select ts.guid, ts.from_node, f.filesize
	    from t_transfer_state ts
	    join t_file f on f.guid = ts.guid
	    where ts.to_node = :node and ts.to_state = 0
	    order by ts.from_state desc,
	             trunc (ts.to_timestamp/86400) desc,
		     f.inblock, f.insubblock},
    	    ":node" => $mynode);
	while (my ($guid, $from, $size) = $stmt->fetchrow())
        {
	    next if grep($_ eq $from, @{$self->{IGNORE_NODES}});
	    next if (@{$self->{ACCEPT_NODES}}
		     && ! grep ($_ eq $from, @{$self->{ACCEPT_NODES}}));

	    $total += $size;
	    last if ($total > $self->{WANT_LIMIT});

	    &dbbindexec($ustmt, ":guid" => $guid, ":node" => $mynode, ":now" => $now);
  	}

	$dbh->commit();
    };

    do { &alert ("failed to mark files wanted: $@");
	 eval { $dbh->rollback() }; } if $@;
}

# Get the list of N guids that should be transferred next.  Returns
# a list of arrays of file information.
sub nextFiles
{
    my ($self, $dbh, $n) = @_;

    # Instead of fetching new files every time we get called, cache
    # lots of them (at least 10*$n) one each access.  Fetch new files
    # only when we don't have enough cached already.  Otherwise just
    # return files from the cache.  This avoids excessive database
    # load and delays from executing the query below, which is not
    # very quick (need an index on state?).
    if (scalar @{$self->{NEXT_FILES}} < $n)
    {
        eval
        {
	    my $attrq = &dbprep ($dbh, qq{
		    select a.attribute, a.value from t_file_attributes a
		    where a.guid = :guid and a.attribute like 'POOL%'});
	    my $stmt = &dbexec ($dbh, qq{
		    select ts.guid,
			   f.lfn,
			   f.filetype,
		           f.filesize,
		           f.checksum,
			   ts.from_node,
		           ts.from_timestamp,
			   ts.from_pfn
		    from t_transfer_state ts
		    join t_file f on f.guid = ts.guid
		    where ts.to_node = :node
		      and ts.to_state < 2
	  	      and ts.from_state = 1
		    order by f.inblock, f.insubblock},
  		    ":node" => $self->{MYNODE});
	    while (my @row = $stmt->fetchrow())
	    {
		next if grep($_->[0] eq $row[0], @{$self->{NEXT_FILES}});
		next if grep($_ eq $row[5], @{$self->{IGNORE_NODES}});
	        next if (@{$self->{ACCEPT_NODES}}
		         && ! grep ($_ eq $row[5], @{$self->{ACCEPT_NODES}}));

	        my $attrs = {};
	        &dbbindexec ($attrq, ":guid" => $row[0]);
	        foreach my $m (@{$attrq->fetchall_arrayref()})
	        {
		    my ($key, $value) = @$m;
		    $key =~ s/^POOL_//;
		    $attrs->{$key} = $value;
	        }

	        push(@{$self->{NEXT_FILES}}, [ @row, $attrs ]);
	        last if (scalar @{$self->{NEXT_FILES}} > 10*$n
			 && scalar @{$self->{NEXT_FILES}} > 50);
	    }
        };
        &alert ("failed to select files for transfer: $@") if $@;
    }

    # Use cached values if we have enough of them
    return splice (@{$self->{NEXT_FILES}}, 0, $n);
}

# Pick up files to download from the database
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my $more = 0;
    eval
    {
    	$dbh = &connectToDatabase ($self) or die "failed to connect";
        # FIXME: Pick up and process messages to me

	# If there are no pending jobs, kill database ghosts
	my $now = &mytimeofday();
	if (scalar @{$self->{JOBS}} == 0 && $self->{LAST_RESET} < $now - 900)
	{
	    my $tmpdir = $self->{DROPDIR};
	    unlink <$tmpdir/*-*-*-*>;
	    $self->resetPendingTransfers($dbh);
	    $self->{LAST_RESET} = $now;
        }

        # Request some more files
        $self->markFilesWanted ($dbh);

	# Initiate as many transfers as we can.
	while (scalar @{$self->{JOBS}} < $self->{NJOBS})
	{
	    last if ! ($more = $self->{BACKEND}->consumeFiles ($self, $dbh));
	    $self->maybeStop ();
	    $self->pumpJobs ();
	}

        # Disconnect from the database
        &disconnectFromDatabase ($self, $dbh);
    };
    do { &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Keep working, resting a bit at a time.  If we have work to do
    # and can start more jobs, break out of this loop and initiate
    # new transfers.  Otherwise if we have exhausted files that can
    # be transferred, just sleep here for a moment.
    my $target = &mytimeofday() + $self->{WAITTIME};
    while (&mytimeofday() < $target)
    {
        $self->maybeStop ();
	$self->pumpJobs ();
	last if $more && scalar @{$self->{JOBS}} < $self->{NJOBS};
	select (undef, undef, undef, .1);
    }
}
