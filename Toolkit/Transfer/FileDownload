#!/usr/bin/env perl

## This is an example transfer agent.
##
## This agent demonstrates how to write a parallel transfer agent.
## It monitors TMDB for files assigned to the agent, creates drop
## box tasks for them internally, and keeps a configurable number
## of transfers going on in parallel.
##
## In implementation, this is a combined drop box and database
## agent.  The TMDB monitoring is done in the &idle() routine,
## which is called whenever there is nothing to be done to the
## drops any more, which is most of the time.  The idle routine
## assigns files to idle workers.  Once the slave worker agent
## has completed the transfer, whether successfully or not, it
## moves the drop to the *inbox* of this agent, where this agent
## collects the status information and updates the database to
## reflect transfer progress.  In other words, the drops cycle
## from the idle routine into the inbox of a slave, to the work
## area of a slave, to inbox of the main agent, to its work area,
## and finally udpated back to the database and destroyed.  Never
## add data from outside this agent into its inbox!
##
## The copies this agent does are pure globus-url-copy operations
## for demonstration purpose.  This makes this agent fully capable
## LCG SE transfer agent.

BEGIN {
  use strict; use warnings;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args = (DBITYPE => "Oracle", NJOBS => 1);
while (scalar @ARGV)
{
    if ($ARGV[0] eq '-db' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBNAME} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbi' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBITYPE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbuser' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBUSER} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbpass' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBPASS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-node' && scalar @ARGV > 1)
    { shift (@ARGV); $args{MYNODE} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-wanted' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WANT_LIMIT} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-jobs' && scalar @ARGV > 1)
    { shift (@ARGV); $args{NJOBS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-backend' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BACKEND_TYPE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-state' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DROPDIR} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-wait' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WAITTIME} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-pfndest' && scalar @ARGV > 1)
    { shift (@ARGV); $args{PFNSCRIPT} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-batch-files' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BATCH_FILES} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-batch-size' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BATCH_SIZE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-validate' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{VALIDATE_COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-delete' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{DELETE_COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-command' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-timeout' && scalar @ARGV > 1)
    { shift (@ARGV); $args{TIMEOUT} = shift (@ARGV); }
    else
    { last; }
}

$args{WANT_LIMIT} = &sizeValue ($args{WANT_LIMIT}) if $args{WANT_LIMIT};
$args{BATCH_SIZE} = &sizeValue ($args{BATCH_SIZE}) if $args{BATCH_SIZE};

if (scalar @ARGV || !$args{DROPDIR} || !$args{DBNAME} || !$args{DBUSER}
    || !$args{DBPASS} || !$args{DBITYPE} || !$args{MYNODE}
    || !$args{WANT_LIMIT} || !$args{PFNSCRIPT} || !$args{NJOBS}
    || !$args{BACKEND_TYPE})
{
    print STDERR
	"usage: $me -state IN-DROP-BOX -backend BACKEND-TYPE\n",
	"    -db NAME -dbuser USER -dbpass PASSWORD [-dbitype TYPE]\n",
	"    -node NODE-NAME -pfndest PROGRAM -wanted SIZE[kMGT]\n",
	"    [-batch-files N-FILES-LIMIT] [-batch-size SIZE-LIMIT[kMGT]]\n",
	"    [-timeout TIMEOUT] [-command COPY-COMMAND[,ARGS]]\n",
	"    [-validate COMMAND[,ARGS]] [-delete COMMAND[,ARGS]]\n",
	"    [-jobs NUM-PARALLEL-PROCESSES] [-wait SECS-TO-WAIT]\n";
    exit (1);
}

my $agent = new FileDownload (%args);
# Recapture interrupt signal, oracle swallows it.
$SIG{INT} = sub { system "touch $agent->{STOPFLAG}"; $agent->maybeStop (); };
$agent->process ();

sub sizeValue
{
    my ($value) = @_;
    if ($value =~ /^(\d+)([kMGT])$/)
    {
        my %scale = ('k' => 1024, 'M' => 1024**2, 'G' => 1024**3, 'T' => 1024**4);
        $value = $1 * $scale{$2};
    }
    return $value;
}

######################################################################
# Routines specific to this agent.
package FileDownload; use strict; use warnings; use base 'UtilsAgent';
use File::Path;
use Data::Dumper;
use UtilsCommand;
use UtilsLogging;
use UtilsTiming;
use UtilsCatalogue;
use UtilsDB;
use POSIX;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBITYPE => undef,		# Database driver binding
    		  DBNAME => undef,		# Database name
	  	  DBUSER => undef,		# Database user name
	  	  DBPASS => undef,		# Database user password
	  	  MYNODE => undef,		# My TMDB node name
		  WANT_LIMIT => undef,		# Amount of data to prefetch
		  WANT_UPDATE => 0,		# Last wanted data update
		  BACKEND => undef,		# Backend type
		  NJOBS => 1,			# Max number of parallel processes
		  JOBS => [],			# Pending jobs
		  NEXT_FILES => [],		# Cached files for transfer
		  LAST_RESET => 0,		# Last time we reset bad transfers
	  	  AGENTID => "Download");	# Identity for activity logs
    my %args = (@_);
    map { $self->{$_} = $args{$_} || $params{$_} } keys %params;
    $self->{BACKEND} = eval ("use UtilsDownload$args{BACKEND_TYPE};"
	    		     . " new UtilsDownload$args{BACKEND_TYPE}(\@_)");
    die "Failed to create backend: $@\n" if $@;
    bless $self, $class;
    return $self;
}

######################################################################
# Remove from the database all existing transfers we are not
# aware of.  This may occur if we are stopped and restarted
# forcefully when there were pending transfers.  We assume
# we are the only live transfer agent for this node, so if
# we don't know about it, it's stale and can be removed.
# Same with previously failed transfers, revert them all back.
sub resetPendingTransfers
{
    my ($self, $dbh) = @_;
    &dbexec ($dbh, qq{
	update t_transfer_state
	set to_state = 1, to_time_stamp = :now
	where to_node = :node and (to_state = 2 or to_state >= 100)},
	":now" => time(), ":node" => $self->{MYNODE});
}

# Record completion for the transfer.  Mark files either transferred
# or send them to cool-off failure state.
sub completeTransferBatch
{
    my ($self, $batch) = @_;
    my $dbh = eval { &connectToDatabase ($self, 0) };
    if ($@)
    {
	&alert ("failed to update transfer batch status,"
		. " trying again later, error was: $@");
	$self->addJob (new sub { $self->completeTransferBatch($batch) });
	return;
    }

    # Have a connection to the database, now update status for all files
    eval
    {
        foreach my $file (@$batch)
        {
	    my $now = time();
	    my $to_state = 3;
	    my $guid = $file->{GUID};
	    my $to_node = $file->{TO_NODE};
	    my $from_node = $file->{FROM_NODE};
	    if ($file->{FAILURE})
	    {
	         &logmsg ("failed to transfer $guid: $file->{FAILURE}");
	         $to_state = 101 + int(rand(10)); # 1..10 min cool-off
	    }

	    $now = &mytimeofday ();
	    my $dalloc = $now - $file->{TIME_ALLOC};
	    my $dtransfer = $now - $file->{TIME_START};
	    &dbexec ($dbh, qq{
		update t_transfer_state
		set to_state = :to_state, to_time_stamp = :now
		where guid = :guid
		  and to_node = :to_node
		  and from_node = :from_node},
		":to_state" => $to_state, ":now" => $now,
		":guid" => $guid, ":to_node" => $to_node,
		":from_node" => $from_node);

	    &dbexec ($dbh, qq{
                insert into t_replica_state
		(guid, node, state, local_state,
		 insert_time_stamp, time_stamp, local_time_stamp)
		values (:guid, :node, 0, 0, :now, :now, :now)},
		":guid" => $guid, ":node" => $to_node, ":now" => $now)
                if ! $file->{FAILURE};

	    # Log transfer delay stats
	    &logmsg ("xstats: $guid $to_node $to_state "
		     . sprintf('%.2f %.2f', $dalloc, $dtransfer)
	     	     . " $file->{FILESIZE}");
        }
        $dbh->commit ();
    };

    # If we failed to update status, something's gone terribly bad,
    # just skip these files, we will re-transfer them later again.
    if ($@)
    {
        &alert ("failed to update transfer batch status: $@");
        $dbh->rollback() if $dbh;
    }

    $dbh->disconnect();
    undef $dbh;
}

# Begin transferring a file.  Fetches the file information and creates
# a new drop for the file in a worker.  If the drop is successfully
# created, marks the file in transfer in the database and returns
# non-zero.  Otherwise returns non-zero to indicate another attempt
# should be made at a later time to initiate the transfer.
sub startFileTransfer
{
    my ($self, $dbh, $info) = @_;
    my ($guid, $size, $cksum, $timestamp,
	$from_node, $from_catalogue, $from_host,
	$to_catalogue, $attrs) = @$info;

    # Mark the transfer started in the database.
    eval
    {
	my $now = &mytimeofday();
	&dbexec ($dbh, qq{
	    update t_transfer_state
	    set to_state = 2, to_time_stamp = :now
	    where guid = :guid and to_node = :to_node and from_node = :from_node},
	    ":now" => $now, ":guid" => $guid,
	    ":to_node" => $self->{MYNODE}, ":from_node" => $from_node);
	$dbh->commit();
    };

    if ($@)
    {
	&alert ("failed to mark $guid in transfer: $@");
	$dbh->rollback();
	return undef;
    }

    # Return a transfer object.
    return { GUID		=> $guid,
	     FILESIZE		=> $size,
	     CHECKSUM		=> $cksum,
	     TIME_ALLOC		=> $timestamp,
	     TIME_START		=> &mytimeofday(),
	     FROM_NODE		=> $from_node,
	     FROM_CATALOGUE	=> $from_catalogue,
	     FROM_HOST		=> $from_host,
	     TO_NODE		=> $self->{MYNODE},
	     TO_CATALOGUE	=> $to_catalogue,
	     ATTRS		=> $attrs };
}

# Update wanted status on files in the database.
sub markFilesWanted
{
    my ($self, $dbh) = @_;

    # Avoid executing very often, the updates below do take a fair
    # amount of time, and there's little point running here much
    # more than once a minute.
    my $now = time();
    return if $self->{WANT_UPDATE} > $now - 60;
    $self->{WANT_UPDATE} = $now;
    
    eval
    {
	my $mynode = $self->{MYNODE};

	# Tick back-off timer for previously failed transfers.
	# Each state above 100 counts as one-minute cool-down
	# period (see code above how this is assigned).
	# (FIXME: restore old time stamp?)
    	&dbexec ($dbh, qq{
	    update t_transfer_state
	    set to_state = (case when to_state > 100 then to_state - 1 else 0 end), to_time_stamp = :now
	    where to_node = :node and to_state >= 100 and to_time_stamp < :old},
    	    ":now" => $now, ":node" => $mynode, ":old" => $now - 60);

	# Keep wanted time stamp at most ten minutes old.
	&dbexec ($dbh, qq{
	    update t_transfer_state
	    set to_time_stamp = :now
	    where to_node = :node and to_state = 1 and to_time_stamp < :old},
    	    ":now" => $now, ":node" => $mynode, ":old" => $now - 600);

  	# Check how much we already marked wanted.  Only mark new
	# files wanted if we drop below 75% of our limit.
	my ($total) = &dbexec($dbh, qq{
	    select sum(m.value)
	    from t_transfer_state ts
	    left join t_replica_metadata m
	      on ts.guid = m.guid and m.attribute = 'filesize'
	    where ts.to_node = :node and ts.to_state = 1},
    	    ":node" => $mynode)->fetchrow() || 0;
	return if $total > .75 * $self->{WANT_LIMIT};

	# Mark more files wanted.  Select the best new files, keeping
	# track of accumulated size and stopping at our limit.  The
	# ordering criteria here is decreasing day and the job-id
	# (FIXME: CMS/COBRA specific!).  We pick the newest files first
	# -- if we are moving files in streaming mode and fall behind,
	# it's better to keep up with the stream the best we can rather
	# than disrupt everyone else trying to slug our way through the
	# old files.  (FIXME: Custom file priority rules?)
	my $ustmt = &dbprep($dbh, qq{
	    update t_transfer_state
	    set to_state = 1, to_time_stamp = :now
	    where guid = :guid and to_node = :node});
	my $stmt = &dbexec($dbh, qq{
	    select ts.guid, ts.from_node, ts.to_time_stamp, m1.value
	    from t_transfer_state ts
	    left join t_replica_metadata m1
	      on ts.guid = m1.guid and m1.attribute = 'filesize'
	    left join t_replica_metadata m2
	      on ts.guid = m2.guid and m2.attribute = 'POOL_jobid'
	    where ts.to_node = :node and ts.to_state = 0
	    order by ts.from_state desc, trunc (ts.to_time_stamp/86400) desc, m2.value},
    	    ":node" => $mynode);
	while (my ($guid, $from_node, $stamp, $size) = $stmt->fetchrow())
        {
	    $total += $size;
	    last if ($total > $self->{WANT_LIMIT});

	    &dbbindexec($ustmt, ":guid" => $guid, ":node" => $mynode, ":now" => $now);
  	}

	$dbh->commit();
    };

    do { &alert ("failed to mark files wanted: $@"); $dbh->rollback() } if $@;
}

# Get the list of N guids that should be transferred next.  Returns
# a list of arrays with members GUID, ALLOC_TIMESTAMP, FROM_NODE,
# FROM_CATALOGUE, TO_CATALOGUE plus a hash of POOL file attributes.
sub nextFiles
{
    my ($self, $dbh, $n) = @_;

    # Instead of fetching new files every time we get called, cache
    # lots of them (at least 10*$n) one each access.  Fetch new files
    # only when we don't have enough cached already.  Otherwise just
    # return files from the cache.  This avoids excessive database
    # load and delays from executing the query below, which is not
    # very quick (need an index on state?).
    if (scalar @{$self->{NEXT_FILES}} < $n)
    {
        # FIXME: sort by filegroup, descending time?
        eval
        {
	    my $attrq = &dbprep ($dbh, qq{
		    select m.attribute, m.value from t_replica_metadata m
		    where m.guid = :guid and m.attribute like 'POOL%'});
	    my $stmt = &dbexec ($dbh, qq{
		    select ts.guid,
		           m1.value,
		           m2.value,
		           ts.from_time_stamp,
		           ts.from_node,
		           n1.catalogue_contact,
		           n1.host_string,
		           n2.catalogue_contact
		    from t_transfer_state ts
		    left join t_replica_metadata m1
		      on m1.guid = ts.guid and m1.attribute = 'filesize'
		    left join t_replica_metadata m2
		      on m2.guid = ts.guid and m2.attribute = 'checksum'
		    left join t_nodes n1
		      on n1.node_name = ts.from_node
		    left join t_nodes n2
		      on n2.node_name = ts.to_node
		    where ts.to_node = :node
		      and ts.to_state < 2
	  	      and ts.from_state = 1},
  		    ":node" => $self->{MYNODE});
	    while (my @row = $stmt->fetchrow())
	    {
	        my $attrs = {};
	        &dbbindexec ($attrq, ":guid" => $row[0]);
	        foreach my $m (@{$attrq->fetchall_arrayref()})
	        {
		    my ($key, $value) = @$m;
		    $key =~ s/^POOL_//;
		    $attrs->{$key} = $value;
	        }

	        push(@{$self->{NEXT_FILES}}, [ @row, $attrs ]);
	        last if (scalar @{$self->{NEXT_FILES}} > 10*$n
			 && scalar @{$self->{NEXT_FILES}} > 50);
	    }
        };
        &alert ("failed to select files for transfer: $@") if $@;
    }

    # Use cached values if we have enough of them
    my $nfiles = scalar @{$self->{NEXT_FILES}};
    $n = $nfiles if $nfiles < $n;
    return splice (@{$self->{NEXT_FILES}}, 0, $n);
}

# Pick up files to download from the database
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my $more = 0;
    eval
    {
    	$dbh = &connectToDatabase ($self) or die "failed to connect";
        # FIXME: Pick up and process messages to me

	# If there are no pending jobs, kill database ghosts
	my $now = time();
	if (scalar @{$self->{JOBS}} == 0 && $self->{LAST_RESET} < $now - 900)
	{
	    my $tmpdir = $self->{DROPDIR};
	    unlink <$tmpdir/*-*-*-*>;
	    $self->resetPendingTransfers($dbh);
	    $self->{LAST_RESET} = $now;
        }

        # Request some more files
        $self->markFilesWanted ($dbh);

	# Initiate as many transfers as we can.
	while (scalar @{$self->{JOBS}} < $self->{NJOBS})
	{
	    last if ! ($more = $self->{BACKEND}->consumeFiles ($self, $dbh));
	    $self->maybeStop ();
	    $self->pumpJobs ();
	}

        # Disconnect from the database
        $dbh->disconnect if $dbh;
        undef $dbh;
    };
    do { &alert ("database error: $@"); $dbh->rollback() if $dbh; } if $@;

    # Keep working, resting a bit at a time.  If we have work to do
    # and can start more jobs, break out of this loop and initiate
    # new transfers.  Otherwise if we have exhausted files that can
    # be transferred, just sleep here for a moment.
    my $target = time() + $self->{WAITTIME};
    while (time() < $target)
    {
        $self->maybeStop ();
	$self->pumpJobs ();
	last if $more && scalar @{$self->{JOBS}} < $self->{NJOBS};
	select (undef, undef, undef, .1);
    }
}
