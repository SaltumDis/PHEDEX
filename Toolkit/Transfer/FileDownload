#!/usr/bin/env perl

##H Parallel reliable file download agent.
##H
##H This agent monitors TMDB for files assigned to the agent, and
##H downloads them.  A separate backend (Globus, SRM, ...) is used
##H to make the actual transfer steps.  A configurable number of
##H parallel subprocesses, both for transfers and other tasks, is
##H automatically maintained.  If an error occurs in any part of
##H the transfer, the file is put into an error state and retried
##H later on.
##H
##H The agent maintains a sliding window of "wanted" files, and
##H creates batches of transferrable files.  The backend assigns
##H the files in a batch to actual transfers, and issues commands
##H to make the copies.  Once the files have been transferred a
##H check is made to verify the completeness of the transfer, and
##H if successful, the file is given to the site for publishing.
##H
##H The agent has no site-specific knowledge, it invokes several
##H commands to invoke operations that require knowledge about the
##H local configuration.  These commands are described in detail
##H in the manual.  The general syntax is CMD[,ARG...], where CMD
##H is the actual command to execute, followed by comma-separated
##H list of arguments to pass to it.
##H
##H Usage:
##H   FileDownload
##H      -state DIRECTORY -nodes PATTERN[,PATTERN...]
##H      -db FILE[:SECTION] [-log OUT] [-verbose] -storagemap PATH
##H      [-ignore NODE[,NODE...]] [-accept NODE[,NODE...]]
##H      [-bypass CMD[,ARG...]] [-validate CMD[,ARG...]] [-delete CMD[,ARG...]]
##H      -backend TYPE [OPTIONS]
##H
##H Options to all backends:
##H      [-protocols NAME[,NAME...]]
##H      [-batch-size SIZE[kMGT]]
##H      [-batch-files N]
##H      [-jobs NJOBS]
##H      [-timeout SECS]
##H
##H Options to Null backend:
##H      (none)
##H
##H Options to Globus backend:
##H      [-command CMD[,ARG...]]
##H
##H Options to SRM backend:
##H      [-command CMD[,ARG...]]
##H
##H Options to FTS backend:
##H      [-glite-args ARG[,ARG...]]
##H
##H -state       agent state directory
##H -nodes       patterns for the node names for which this agent runs
##H -db          database connection configuration parameter file
##H -log         where to redirect logging information
##H -verbose     include more information about internal algorithms
##H
##H -storagemap  storage mapping catalogue for download destination
##H -ignore      comma-separated list of nodes to ignore transfers from
##H -accept      comma-separated list of nodes to accept transfers from
##H -bypass      command to short-circuit transfers
##H -validate    command to verify file transfer success; if not specified
##H               uses transfer command exit code, normally a bad idea
##H -delete      command to delete file on failure
##H
##H -backend     the transfer backend to use: Globus, SRM, DCCP; all
##H              options that follow are passed to the backend as such
##H
##H -protocols   comma-separated list of storage protocols to accept
##H -batch-size  enable batching, set maximum number of bytes per batch
##H -batch-files enable batching, set maximum number of files per batch
##H -jobs        set the number of jobs to run in parallel (default: 5)
##H -timeout     set time limit on commands (default: 3600)
##H -command     override the transfer command, for instance
##H               globus-url-copy,-p,3,-tcp-bs,2097152
##H -glite-args  additional arguments to give to gLite FTS commands

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
	     "verbose"     => \$args{VERBOSE},
             "nodes=s"     => sub { push(@{$args{NODES}},
					 split(/,/, $_[1])) },

             "ignore=s"    => sub { push(@{$args{IGNORE_NODES}},
					 split(/,/, $_[1])) },
             "accept=s"    => sub { push(@{$args{ACCEPT_NODES}},
					 split(/,/, $_[1])) },

             "bypass=s"    => sub { push(@{$args{BYPASS_COMMAND}},
					 split(/,/, $_[1])) },
             "validate=s"  => sub { push(@{$args{VALIDATE_COMMAND}},
					 split(/,/, $_[1])) },
             "delete=s"    => sub { push(@{$args{DELETE_COMMAND}},
					 split(/,/, $_[1])) },

             "storagemap=s"=> \$args{STORAGEMAP},,
             "backend=s"   => sub { $args{BACKEND_TYPE} = $_[1]; die "!FINISH"; },

	     "help|h"      => sub { &usage() });

die "Insufficient parameters, use -h for help.\n"
    if (!$args{DROPDIR} || !$args{NODES} || !$args{DBCONFIG}
	|| !$args{STORAGEMAP} || !$args{BACKEND_TYPE});

(new FileDownload (%args, BACKEND_ARGS => [ @ARGV ]))->process();

######################################################################
# Routines specific to this agent.
package FileDownload; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(min max sum);
use UtilsLogging;
use UtilsTiming;
use UtilsDB;
use POSIX;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
	  	  MYNODE => undef,		# My TMDB node name
		  NODES => undef,		# Nodes to operate for
	  	  IGNORE_NODES => [],		# TMDB nodes to ignore
	  	  ACCEPT_NODES => [],		# TMDB nodes to accept
		  STORAGEMAP => undef,		# Storage path mapping rules
		  BYPASS_COMMAND => undef,	# Bypass-check command
		  VALIDATE_COMMAND => undef,	# Post-download test command
		  DELETE_COMMAND => undef,	# Delete bad files command
		  TIMEOUT => 600,		# Maximum execution time
		  NJOBS => 5,			# Max number of utility processes
		  WAITTIME => 7,		# Nap length between cycles
		  VERBOSE => 0,			# Verbosity

		  BACKEND_TYPE => undef,	# Backend type
		  BACKEND_ARGS => undef,	# Options to the backend
		  # GLITE_ARGS

		  NODE => {},			# Node bandwidth parameters
		  LINKS => {},			# Per-link transfer parameters
		  FILES => {},			# Files to transfer

		  LAST_PRUNE => 0,		# Last prune of history data
		  LAST_WANTED => 0,		# Last refresh of wanted files
		  LAST_PARAMS => 0,		# Last refresh of link params
		  LAST_PREPARE => 0,		# Last file preparation run
		  LAST_FILES => 0,		# Last file fetch
		  LAST_RESET => 0);		# Last reset of bogus transfers
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    $$self{BACKEND} = eval ("use UtilsDownload$args{BACKEND_TYPE};"
	    		    . " new UtilsDownload$args{BACKEND_TYPE}"
			    . " (MASTER => \$self, \@_)");
    do { chomp ($@); die "Failed to create backend: $@\n" } if $@;
    bless $self, $class;
    return $self;
}

# If stopped, also stop backend jobs.
sub stop
{
    my ($self) = @_;
    $$self{BACKEND}->killAllJobs();
}

######################################################################
# Remove from the database all existing transfers we are not aware of.
# This may occur if we are stopped and restarted forcefully when there
# were pending transfers.  We assume we are the only live transfer
# agent for this node, so if we don't know about it, it's stale and
# can be removed.   Idem with previously failed transfers.
sub resetPendingTransfers
{
    my ($self, $dbh) = @_;
    my ($dest, %dest_args) = &myNodeFilter ($self, "xs.to_node");
    my ($filter, %filter_args) = &otherNodeFilter ($self, "xs.from_node");
    &dbexec ($dbh, qq{
	update t_xfer_state xs
	set xs.to_state = 0,
	    xs.to_protocols = null,
	    xs.to_pfn = null,
	    xs.from_state = 0,
	    xs.from_pfn = null,
	    xs.time_request = :now,
	    xs.time_available = null,
	    xs.time_xfer_start = null,
	    xs.time_xfer_end = null,
	    xs.time_error_start = null,
	    xs.time_error_end = null
	where $dest
  	  and time_expire > :now
	  and (to_state = 2 or (to_state = 100 and xs.time_error_end <= :now))
  	  $filter},
	":now" => &mytimeofday(),
	%dest_args, %filter_args);
}

# Mark my files wanted.
sub markFilesWanted
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday ();
    return if $$self{LAST_WANTED} > $now - 60;

    my ($dest, %dest_args) = &myNodeFilter ($self, "to_node");

    # Restore files from cool-off when they've done their time.
    &dbexec ($dbh, qq{
	update t_xfer_state
	set to_state = 0,
	    to_protocols = null,
	    to_pfn = null,
	    from_state = 0,
	    from_pfn = null,
	    time_available = null,
	    time_error_start = null,
	    time_error_end = null
	where $dest
	  and to_state = 100
          and time_error_end <= :now
          and time_expire > :now},
        ":now" => $now, %dest_args);

    # Mark all my files wanted.  We want this updated regularly enough.
    &dbexec ($dbh, qq{
	update t_xfer_state
	set to_state = 1, time_request = :now
	where $dest
	  and time_expire > :now
  	  and (time_request is null or time_request < :now - 300)
          and to_state <= 1},
    	":now" => $now, %dest_args);

    $$self{LAST_WANTED} = $now;
    $dbh->commit();
}

# Update link parameter configuration.  Maintains in-memory tables for
# node and link bandwidth caps, as well as the shares assigned to each
# priority class.  The data is refreshed from database infrequently.
sub updateLinkParameters
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday();

    # If we haven't refreshed parameters recently, update now.
    if ($$self{LAST_PARAMS} < $now - 600)
    {
	my ($dest, %dest_args) = &myNodeFilter ($self, "nd.id");
	my ($filter, %filter_args) = &otherNodeFilter ($self, "ns.id");

	# Read per-node parameters
	my $q = &dbexec ($dbh, qq{
	    select name, bandwidth_cap from t_node nd where $dest},
	    %dest_args);
	while (my ($node, $cap) = $q->fetchrow())
	{
	    $$self{NODE}{$node}{BANDWIDTH_CAP} = $cap;
	}

	# Clear out any old per-link data.
	$$self{LINKS} = {};

	# Read per-link parameters
	$q = &dbexec ($dbh, qq{
	    select
		l.to_node to_node_id,
		nd.name to_node,
		l.from_node from_node_id,
		ns.name from_node,
		l.bandwidth_cap
	    from t_link l
	      join t_node nd on nd.id = l.to_node
	      join t_node ns on ns.id = l.from_node
	    where $dest $filter},
	    %dest_args, %filter_args);
	while (my $row = $q->fetchrow_hashref())
	{
	    $$self{LINKS}{$$row{TO_NODE}}{$$row{FROM_NODE}}{PARAMS} = $row;
	}

	# Read per-link priority shares
	$q = &dbexec ($dbh, qq{
	    select nd.name, ns.name, ls.priority, ls.link_share
	    from t_link_share ls
	      join t_link l on l.id = ls.link
	      join t_node nd on nd.id = l.to_node
	      join t_node ns on ns.id = l.from_node
	    where $dest $filter},
	    %dest_args, %filter_args);
	while (my ($to, $from, $priority, $shares) = $q->fetchrow()) {
	    $$self{LINKS}{$to}{$from}{SHARES}{$priority} = $shares;
	}

	# Remember when we last did this
	$$self{LAST_PARAMS} = $now;
    }

    # Update recent link statistics.  This is used as "dead weight
    # momentum" in the fair share algorithm to maintain desired
    # balance over time, and to track which links have had too many
    # errors recently.
    foreach my $to (keys %{$$self{LINKS}})
    {
	foreach my $from (keys %{$$self{LINKS}{$to}})
	{
	    my $link = $$self{LINKS}{$to}{$from};
	    $$link{XFERS} = 0;
	    $$link{BYTES} = 0;
	    $$link{ERRORS} = 0;

	    foreach my $recent (@{$$self{RECENT}{$to}{$from}})
	    {
		my $prio = $$recent{PRIORITY};
		my $size = $$recent{FILESIZE};
		$$link{$prio} ||= { XFERS => 0, BYTES => 0 };

		if ($$recent{STATE} == 3) {
		    $$link{$prio}{XFERS}++;
		    $$link{XFERS}++;

		    $$link{$prio}{BYTES} += $size;
		    $$link{BYTES} += $size;
		} elsif ($$recent{STATE} == 100) {
		    $$link{ERRORS}++;
		}
	    }
	}

	# Disable and enable links as necessary
	my $bytes = sum(0, map { $$_{BYTES} } values %{$$self{LINKS}{$to}});
	foreach my $from (keys %{$$self{LINKS}{$to}})
	{
	    my $link = $$self{LINKS}{$to}{$from};
	    my $linkcap = $$link{PARAMS}{BANDWIDTH_CAP};
	    my $nodecap = $$self{NODE}{$to}{BANDWIDTH_CAP};

	    if ($$link{ERRORS} > 100)
	    {
		&logmsg ("excessive errors in transfers on link"
			 . " from $from to $to,"
			 . " suspending transfers on this link")
		    if ! $$link{DOWN};
		$$link{DOWN} = 1;
	    }
	    elsif (defined $linkcap && $$link{BYTES} > $linkcap * 3600)
	    {
		&logmsg ("transfer rate on link from $from to $to"
			 . " exceeded link bandwidth cap "
			 . sprintf ("%.1f", $linkcap/(1024**2))
			 . " MB/s, suspending transfers on this link")
		    if ! $$link{DOWN};
		$$link{DOWN} = 2;
	    }
	    elsif (defined $nodecap && $bytes > $nodecap * 3600)
	    {
		&logmsg ("aggregate transfer rate on all links to $to"
			 . " exceeded node bandwidth cap "
			 . sprintf ("%.1f", $nodecap/(1024**2))
			 . " MB/s, suspending transfers on all links")
		    if ! $$link{DOWN};
		$$link{DOWN} = 3;
	    }
	    elsif ($$link{DOWN})
	    {
		&logmsg ("reactivating transfers on suspended link"
			 . " from $from to $to");
		$$link{DOWN} = 0;
	    }
	}
    }
}

# Prepare files for transfer handshake.
sub prepareFiles
{
    my ($self, $dbh) = @_;

    # Avoid repeating too frequently
    my $now = &mytimeofday ();
    return if $$self{LAST_PREPARE} > $now - 60;

    # Find files still lacking a file name, give them a name
    # and tell the other side which protocols we can accept.
    my $n = 0;
    my ($dest, %dest_args) = &myNodeFilter ($self, "xs.to_node");
    my ($filter, %filter_args) = &otherNodeFilter ($self, "xs.from_node");

    my $proto = &dbprep($dbh, qq{
	update t_xfer_state
	set to_pfn = :pfn, to_protocols = :protos
	where fileid = :fileid and to_node = :node});
    my $q = &dbexec ($dbh, qq{
	select
	    xs.fileid,
	    xs.to_node to_node_id,
	    nd.name to_node,
	    f.logical_name
	from t_xfer_state xs
          join t_xfer_file f on f.id = xs.fileid
	  join t_node nd on nd.id = xs.to_node
	where $dest
	  and xs.to_pfn is null
	  and xs.time_expire > :now
	  and xs.errors < 5
	  $filter},
	":now" => $now,
	%dest_args,
	%filter_args);

    while (my $row = $q->fetchrow_hashref())
    {
	# Tell file provider the protocols we are willing to accept
	# and record the download destination.  The latter is mainly
	# to help distributed transfer debugging, there is no
	# intrisinc reason to communicate the paths to "outside."
	$$self{BACKEND}->fillFileRequest ($row);
	&dbbindexec ($proto,
		     ":pfn" => $$row{TO_PFN},
		     ":protos" => $$row{TO_PROTOCOLS},
		     ":fileid" => $$row{FILEID},
		     ":node" => $$row{TO_NODE_ID});
    }

    # Commit and make note when we last did this
    &logmsg ("$n files prepared for download")
	if $n && $$self{VERBOSE};
    $$self{LAST_PREPARE} = $now;
    $dbh->commit();
}

# Update file transfer work list.
#
# The full work list is refreshed from the database only every few
# minutes, or if the agent is running out of work.  The agent will
# therefore take a little time to adjust to behind-the-scenes changes
# to the database.  Files already in transfer by the agent are not
# refreshed from the database.
#
# The work queue contains all files available for transfer on active
# links.  The lists exclude files not yet available for transfer,
# missing either source or destination name, in error states, on
# suspended links, expired or having experienced too many errors.
sub refreshFiles
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday ();

    # Remove transfers on links that are now down
    delete $$self{FILES}{$$_{FILEID}}
	for grep($$_{TO_STATE} != 2
		 && $$self{LINKS}{$$_{TO_NODE}}{$$_{FROM_NODE}}{DOWN},
		 values %{$$self{FILES}});

    # Avoid refresh if we've read recently and have enough to chew on
    return if (($$self{LAST_FILES} > $now - 300 && scalar keys %{$$self{FILES}})
	       || $$self{LAST_FILES} > $now - 15);

    # Reset old state
    delete $$self{FILES}{$$_{FILEID}}
      for grep($$_{TO_STATE} != 2, values %{$$self{FILES}});

    # Fetch transfer backlog
    my ($dest, %dest_args) = &myNodeFilter ($self, "xs.to_node");
    my ($filter, %filter_args) = &otherNodeFilter ($self, "xs.from_node");

    my $q = &dbexec ($dbh, qq{
	select
	    xs.fileid, f.logical_name, f.filesize, f.checksum, b.name inblock,
	    xs.priority, xs.weight, xs.age,
	    xs.to_node to_node_id, nd.name to_node, xs.to_state, xs.to_pfn,
	    xs.from_node from_node_id, ns.name from_node, xs.from_pfn,
	    xs.time_assign, xs.time_available
	from t_xfer_state xs
	  join t_xfer_file f on f.id = xs.fileid
	  join t_node nd on nd.id = xs.to_node
	  join t_node ns on ns.id = xs.from_node
          join t_dps_block b on b.id = f.inblock
	where $dest
	  and xs.from_state = 1
 	  and xs.to_pfn is not null
	  and xs.time_expire > :now
	  and xs.errors < 5
	  $filter},
	":now" => $now,
	%dest_args,
	%filter_args);

    while (my $row = $q->fetchrow_hashref())
    {
	# Remember the file if it's not on a link that's down
	$$self{FILES}{$$row{FILEID}} = $row
	    if (! exists $$self{FILES}{$$row{FILEID}}
		&& ! $$self{LINKS}{$$row{TO_NODE}}{$$row{FROM_NODE}}{DOWN});
    }

    &logmsg ("@{[scalar keys %{$$self{FILES}}]} files in refreshed list")
	if $$self{VERBOSE};
    $$self{LAST_FILES} = $now;
}

# Issue file transfers.
sub issueTransfers
{
    my ($self, $dbh, $to, $nxfers) = @_;

    # Divide up the $nxfers transfer share by link and priority using
    # a "corrected auto-tuning fair share algorithm" as described at
    # http://www.psc.edu/publications/tech_reports/fairshare/fairshare.html.
    #
    # The essence of the algorithm is:
    # 1. P = the total size of the pool, allocated and unallocated
    #    D(i) = the amount of buffer space desired by connection i
    #    B(i) = the amount of buffer space allotted to connection i so
    #           far (zero initially).
    # 2. U = set of all connections i for which B(i) < D(i)
    #    N = the number of connections in U.
    #    current share = (P - sum(B(i)) for all i) / N
    # 3. For all i in U,
    #      if D(i) < B(i) + current share then B(i) = D(i)
    #      otherwise, increase B(i) by the current share
    # 4. Repeat from step 2 until U is empty, or the current share
    # falls to a certain threshold (which may be zero).
    #
    # In our implementation, each link has a share for each priority
    # level, typically four values n:m:o:p.  We sum these up over all
    # links for which there are transfers at that priority level into
    # Sn:Sm:So:Sp.  To balance the distribution of shares fairly over
    # time, both over links and priorities, we use as dead weight
    # momentum the transfer history from the last hour.  In other
    # words, our pool size P is ((# of transfers in last hour) +
    # $nxfers), and initialise B to the transfers of last hour.
    #
    # We run the fair share algorithm to determine the fraction of
    # $nxfers each priority level receives on this round, then
    # distribute each priority share to the links according to
    # their relative fraction n:Sn.
    #
    # The variables below have the following meanings:
    #  @files:      files ready for transfer
    #  @levels:     the priority levels we have transfers for
    #  %queues:     @files divided into per-priority, per-link queues
    #  %shares:     per-priority sum of link shares
    #  %old:        usage of bandwidth share in last hour
    #  %desired:    the transfers desired by priority level "i" (D(i))
    #  %allocated:  %old + transfers allocated to priority level "i" (B(i))
    #  %issue:      transfers to issue by priority level (%allocated - %old)
    #  "P"           $oldbias + $nxfers

    my (@files, @levels, %queues, %shares);
    my (%old, %desired, %allocated, %issue);

    # Divide files into by priority, then by source node (link).
    @files = grep($$_{TO_NODE} eq $to && $$_{TO_STATE} <= 1,
		  values %{$$self{FILES}});
    push(@{$queues{$$_{PRIORITY}}{$$_{FROM_NODE}}}, $_) for @files;
    @levels = sort { $b <=> $a } keys %queues;

    # Compute total share allocation for links with transfers.
    foreach my $prio (@levels)
    {
	$shares{$prio} = $desired{$prio} = $issue{$prio} = 0;

	foreach my $from (keys %{$queues{$prio}})
	{
	    $shares{$prio} += $$self{LINKS}{$to}{$from}{SHARES}{$prio} || 1;
	    $old{$prio} += $$self{LINKS}{$to}{$from}{XFERS};
	    $desired{$prio} += scalar @{$queues{$prio}{$from}};
	}

	$desired{$prio} += $old{$prio};
	$allocated{$prio} = $old{$prio};
    }

    # Run the fair share algorithm.
    my $oldbias = sum(0, values %old);
    while (1)
    {
	&logmsg ("nxfers=$nxfers"
		 . join(" ", map { ("(level=$_"
				    . " desired=$desired{$_}"
				    . " allocated=$allocated{$_})") }
			@levels))
	    if $$self{VERBOSE};

	# Select priorities which haven't reached their quota yet.
	my @unallocated = grep($allocated{$_} < $desired{$_}, @levels);
	last if ! @unallocated;

	# Calculate current weighted share.  If too little left, give up.
	my $n = sum(map { $shares{$_} } @unallocated);
	my $share = ($oldbias + $nxfers - sum(0, values %allocated)) / $n;
	last if $share < 0.01;

	# Allocate the share to each priority level according to
	# the relative weight of the level.
	foreach (@levels)
	{
	    my $pshare = $share * $shares{$_};
	    if ($desired{$_} < $allocated{$_} + $pshare)
	    {
		my $issue = $desired{$_} - $allocated{$_};
		$issue{$_} += $issue;
		$allocated{$_} += $issue;
	    }
	    else
	    {
		$issue{$_} += $pshare;
		$allocated{$_} += $pshare;
	    }
	}
    }

    &logmsg ("nxfers=$nxfers"
	     . join(" ", map { "(level=$_ issue=$issue{$_})" } @levels))
	if $$self{VERBOSE};

    # Now divide the priority slots by participating link, from the
    # lowest priority to the highest.  Each priority level takes its
    # integer share of the available transfers; leftovers are given
    # to the highest remaining priority level.
    my %xfers;
    while (@levels)
    {
	# Share of this priority level from the total
	my $prio = shift(@levels);
	my $pshare = (@levels ? int($issue{$prio}) : $nxfers);
	$nxfers -= $pshare;
	next if ! $pshare;

	&logmsg ("priority $prio: issuing $pshare/$nxfers transfers to $to")
	    if $$self{VERBOSE};

	# Same division as we just did with priority shares, but
	# dividing priority-level share to links in proportion to
	# their relative weights.
	my @links = keys %{$queues{$prio}};
	while (@links)
	{
	    # Share of this link from the priority level
	    my $from = shift(@links);
	    my $factor = (($$self{LINKS}{$to}{$from}{SHARES}{$prio} || 1) / $shares{$prio});
	    my $lshare = (@links ? int($factor*$pshare) : $pshare);
	    $pshare -= $lshare;
	    next if ! $lshare;

	    &logmsg ("priority $prio: issuing $lshare/$pshare/$nxfers"
		     . " transfers to $to from $from")
		if $$self{VERBOSE};

	    # Rank the transfers, then pick $lshare first ones.
	    my @new = sort { (int($$a{AGE}/3600) <=> int($$b{AGE}/3600)
			      || $$a{WEIGHT} <=> $$b{WEIGHT}
			      || $$a{INBLOCK} cmp $$b{INBLOCK}
			      || $$a{LOGICAL_NAME} cmp $$b{LOGICAL_NAME}) }
		@{$queues{$prio}{$from}};
	    push(@{$xfers{$from}}, splice(@new, 0, $lshare));
	}
    }

    # Finally, issue the transfers
    $self->startTransfer ($dbh, map { @$_ } values %xfers);
    $$self{BACKEND}->transfer(@$_) for values %xfers;
}

# Mark the files in transfer in the database.
sub startTransfer
{
    my ($self, $dbh, @files) = @_;
    my $now = &mytimeofday();
    foreach my $file (@files)
    {
	&dbexec ($dbh, qq{
	    update t_xfer_state
	    set to_state = 2, time_xfer_start = :now
	    where fileid = :fileid and to_node = :node},
	    ":now" => $now, ":fileid" => $$file{FILEID},
	    ":node" => $$file{TO_NODE_ID});
	$$file{TIME_START} = $now;
    }
    $dbh->commit();
    $$_{TO_STATE} = 2 for @files;
}

# Record completion for the transfer.  Mark files either transferred
# or sends them to cool-off failure state.
sub completeTransfer
{
    my ($self, @files) = @_;
    my $dbh = &connectToDatabase ($self, 0);

    # Have a connection to the database, update status
    my $now = &mytimeofday ();
    foreach my $file (@files)
    {
	if ($$file{FAILURE})
	{
	    $$file{TO_STATE} = 100;
	    &alert ("transfer failed: $$file{FAILURE};"
		    . " to=$$file{TO_NODE}"
		    . " from=$$file{FROM_NODE}"
		    . " fileid=$$file{FILEID}"
		    . " lfn=$$file{LOGICAL_NAME}"
		    . " from_pfn=$$file{FROM_PFN}"
		    . " to_pfn=$$file{TO_PFN}");
	    &dbexec ($dbh, qq{
	        update t_xfer_state
	        set to_state = :state,
		    errors = errors + 1,
		    time_xfer_start = null,
		    time_request = null,
		    time_error_start = :error_start,
		    time_error_end = :error_end,
		    time_error_total = nvl(time_error_total,0)
				       + :error_end - :error_start
	        where fileid = :fileid and to_node = :node},
	        ":state" => $$file{TO_STATE},
	        ":error_start" => $now,
	        ":error_end" => $now + 60*(30 + rand(30)),
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID});
	}
	else
	{
	    $$file{TO_STATE} = 3;
	    &dbexec ($dbh, qq{
	        update t_xfer_state
	        set to_state = :state, time_xfer_end = :now
	        where fileid = :fileid and to_node = :node},
	        ":now" => $now,
	        ":state" => $$file{TO_STATE},
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID});

	    &dbexec ($dbh, qq{
                insert into t_xfer_replica
	        (id, fileid, node, state, time_create, time_state)
	        values (seq_xfer_replica.nextval, :fileid, :node, 1, :now, :now)},
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID},
	        ":now" => $now);

	    &dbexec ($dbh, qq{
	        delete from t_xfer_state
	        where fileid = :fileid and to_node = :node},
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID});
	}

	# Log transfer stats
	&logmsg ("xstats:"
		 . " to=$$file{TO_NODE}"
		 . " from=$$file{FROM_NODE}"
		 . " fileid=$$file{FILEID}"
		 . " state=$$file{TO_STATE}"
		 . " size=$$file{FILESIZE}"
		 . join("", map { sprintf(" time_%s=%.2f", $$_[0], $$_[2]-$$_[1]) }
			[ 'assigned', $$file{TIME_ASSIGN}, $now ],
			[ 'all', $$file{TIME_START}, $now ],
			@{$$file{TIMING}})
		 . " lfn=$$file{LOGICAL_NAME}"
		 . " from_pfn=$$file{FROM_PFN}"
		 . " to_pfn=$$file{TO_PFN}");

	# Record in my recent history
	push(@{$$self{RECENT}{$$file{TO_NODE}}{$$file{FROM_NODE}}},
	     { TIME => $now,
	       STATE => $$file{TO_STATE},
	       PRIORITY => $$file{PRIORITY},
	       FILESIZE => $$file{FILESIZE} });

	# Remove from work list.  If it errored out, we will reread eventually.
	delete $$self{FILES}{$$file{FILEID}};
    }

    $dbh->commit ();
    &disconnectFromDatabase ($self, $dbh);
}

# Pick up files to download from the database
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my @nodes;

    eval
    {
	($dbh, @nodes) = &expandNodesAndConnect($self);
	
	# If there are no pending jobs, kill database ghosts
	my $now = &mytimeofday();
	if (! @{$$self{JOBS}}
	    && ! @{$$self{BACKEND}{JOBS}}
	    && $$self{LAST_RESET} < $now - 900)
	{
	    my $tmpdir = $$self{DROPDIR};
	    unlink grep(! /\.log$/, <$tmpdir/[0-9]*>);
	    unlink <$tmpdir/*.log> if $$self{LAST_RESET} < $now - 3*86400;
	    $self->resetPendingTransfers($dbh);
	    $$self{LAST_RESET} = $now;
        }

	# Prune old recent transfers
	if ($$self{LAST_PRUNE} < $now - 60)
	{
	    foreach my $to (keys %{$$self{RECENT}})
	    {
	        foreach my $from (keys %{$$self{RECENT}{$to}})
	        {
		    @{$$self{RECENT}{$to}{$from}}
		        = grep($$_{TIME} > $now-3600,
			       @{$$self{RECENT}{$to}{$from}});
	        }
	    }
	    $$self{LAST_PRUNE} = $now;
	}

        # Request some more files
        $self->markFilesWanted ($dbh);

	# Initiate as many transfers as we can.
	my $nxfers = $$self{BACKEND}->transferSlots($$self{FILES});
	if ($nxfers >= 1)
	{
	    $self->updateLinkParameters ($dbh);
	    $self->prepareFiles ($dbh);
	    $self->refreshFiles ($dbh);
	    $self->issueTransfers ($dbh, $_, $nxfers) for @nodes;
	    # FIXME: Distribute $nxfers more sanely with @nodes!
	}

        # Disconnect from the database
        &disconnectFromDatabase ($self, $dbh);
    };

    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Keep working, resting a bit at a time.  If we have work to do
    # and can start more jobs, break out of this loop and initiate
    # new transfers.  Otherwise if we have exhausted files that can
    # be transferred, just sleep here for a moment.
    my $target = &mytimeofday() + $$self{WAITTIME};
    while (&mytimeofday() < $target)
    {
        $self->maybeStop ();
	$self->pumpJobs ();
	$$self{BACKEND}->pumpJobs ();
	last if (keys %{$$self{FILES}}
		 && scalar @{$$self{BACKEND}{JOBS}} < $$self{BACKEND}{NJOBS}
		 && scalar @{$$self{JOBS}} < $$self{NJOBS});
	select (undef, undef, undef, .1);
    }
}
