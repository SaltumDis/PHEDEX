#!/usr/bin/env perl

##H Parallel reliable file download agent.
##H
##H This agent monitors TMDB for files assigned to the agent, and
##H downloads them.  A separate backend (Globus, SRM, ...) is used
##H to make the actual transfer steps.  A configurable number of
##H parallel subprocesses, both for transfers and other tasks, is
##H automatically maintained.  If an error occurs in any part of
##H the transfer, the file is put into an error state and retried
##H later on.
##H
##H The agent maintains a sliding window of "wanted" files, and
##H creates batches of transferrable files.  The backend assigns
##H the files in a batch to actual transfers, and issues commands
##H to make the copies.  Once the files have been transferred a
##H check is made to verify the completeness of the transfer, and
##H if successful, the file is given to the site for publishing.
##H
##H The agent has no site-specific knowledge, it invokes several
##H commands to invoke operations that require knowledge about the
##H local configuration.  These commands are described in detail
##H in the manual.  The general syntax is CMD[,ARG...], where CMD
##H is the actual command to execute, followed by comma-separated
##H list of arguments to pass to it.
##H
##H Usage:
##H   FileDownload
##H      -state DIRECTORY -node NAME -db FILE[:SECTION]
##H      [-ignore NODE[,NODE...]] [-accept NODE[,NODE...]]
##H      -pfndest CMD[,ARG...] [-bypass CMD[,ARG...]]
##H      [-validate CMD[,ARG...]] [-publish CMD[,ARG...]]
##H      [-delete CMD[,ARG...]] [-timeout SECS] [-jobs NJOBS]
##H      -backend TYPE [OPTIONS]
##H
##H Options to Globus backend:
##H      [-command CMD[,ARG...]]
##H      [-protocols NAME[,NAME...]]
##H
##H Options to SRM backend:
##H      [-command CMD[,ARG...]]
##H      [-protocols NAME[,NAME...]]
##H      [-batch-size SIZE[kMGT]]
##H      [-batch-files N]
##H
##H Options to FTS backend:
##H      [-glite-args ARG[,ARG...]]
##H      [-protocols NAME[,NAME...]]
##H      [-batch-size SIZE[kMGT]]
##H      [-batch-files N]
##H
##H -state       agent state directory
##H -node        the node where this agent runs
##H -db          database connection configuration parameter file
##H -ignore      comma-separated list of nodes to ignore transfers from
##H -accept      comma-separated list of nodes to accept transfers from
##H -pfndest     command to produce destination transfer name
##H -bypass      command to short-circuit transfers
##H -validate    command to verify file transfer success; if not specified
##H               uses transfer command exit code, normally a bad idea
##H -publish     command to publish file after transfer
##H -delete      command to delete file on failure
##H -timeout     set time limit on commands to avoid e.g. hung transfers
##H -jobs        set the number of jobs to run in parallel (default: 5)
##H -backend     the transfer backend to use: Globus, SRM, DCCP; all
##H              options that follow are passed to the backend as such
##H
##H -command     override the transfer command, for instance
##H               globus-url-copy,-p,3,-tcp-bs,2097152
##H -glite-args  additional arguments to give to gLite FTS commands
##H -protocols   comma-separated list of storage protocols to accept
##H -batch-size  enable batching, set maximum number of bytes per batch
##H -batch-files enable batching, set maximum number of files per batch

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
use UtilsHelp;
my %args;
while (scalar @ARGV)
{
    if ($ARGV[0] eq '-state' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DROPDIR} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-node' && scalar @ARGV > 1)
    { shift (@ARGV); $args{MYNODE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-db' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBCONFIG} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-ignore' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{IGNORE_NODES}}, split(/,/, shift (@ARGV))); }
    elsif ($ARGV[0] eq '-accept' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{ACCEPT_NODES}}, split(/,/, shift (@ARGV))); }

    elsif ($ARGV[0] eq '-pfndest' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{PFN_GEN_COMMAND}}, split(/,/,  shift(@ARGV))); }
    elsif ($ARGV[0] eq '-bypass' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{BYPASS_COMMAND}}, split(/,/,  shift(@ARGV))); }
    elsif ($ARGV[0] eq '-validate' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{VALIDATE_COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-publish' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{PUBLISH_COMMAND}}, split(/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-delete' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{DELETE_COMMAND}}, split (/,/, shift(@ARGV))); }

    elsif ($ARGV[0] eq '-timeout' && scalar @ARGV > 1)
    { shift (@ARGV); $args{TIMEOUT} = shift (@ARGV); }
    elsif ($ARGV[0] eq '-jobs' && scalar @ARGV > 1)
    { shift (@ARGV); $args{NJOBS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-backend' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BACKEND_TYPE} = shift(@ARGV); last; }

    elsif ($ARGV[0] eq '-h')
    { &usage(); }
    else
    { last; }
}

# elsif ($ARGV[0] eq '-batch-files' && scalar @ARGV > 1)
# { shift (@ARGV); $args{BATCH_FILES} = shift(@ARGV); }
# elsif ($ARGV[0] eq '-batch-size' && scalar @ARGV > 1)
# { shift (@ARGV); $args{BATCH_SIZE} = shift(@ARGV); }
# elsif ($ARGV[0] eq '-command' && scalar @ARGV > 1)
# { shift (@ARGV); push (@{$args{COMMAND}}, split (/,/, shift(@ARGV))); }
# $args{BATCH_SIZE} = &sizeValue ($args{BATCH_SIZE}) if $args{BATCH_SIZE};

if (!$args{DROPDIR} || !$args{MYNODE} || !$args{DBCONFIG}
    || !$args{PFN_GEN_COMMAND} || !$args{BACKEND_TYPE})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FileDownload (%args, BACKEND_ARGS => [ @ARGV ]))->process();

######################################################################
# Routines specific to this agent.
package FileDownload; use strict; use warnings; use base 'UtilsAgent';
use UtilsLogging;
use UtilsTiming;
use UtilsDB;
use POSIX;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
	  	  MYNODE => undef,		# My TMDB node name
	  	  IGNORE_NODES => [],		# TMDB nodes to ignore
	  	  ACCEPT_NODES => [],		# TMDB nodes to accept
		  # PFN_GEN_COMMAND => undef,	# Destination PFN command
		  # BYPASS_COMMAND => undef,	# Bypass-check command
		  # VALIDATE_COMMAND => undef,	# Post-download test command
		  # PUBLISH_COMMAND => undef,	# Catalogue publis command
		  # DELETE_COMMAND => undef,	# Delete bad files command
		  # TIMEOUT => undef,		# Maximum execution time
		  NJOBS => 5,			# Max number of parallel processes
		  BACKOFF_FACTOR => 10,		# Factor to cool off bad files
		  WAITTIME => 7,		# Nap lenght between cycles

		  BACKEND => undef,		# Backend type
		  BACKEND_ARGS => undef,	# Options to the backend
		  # COMMAND
		  # GLITE_ARGS
		  # BATCH_SIZE
		  # BATCH_FILES
		  # PROTOCOLS

		  JOBS => [],			# Pending jobs
		  FILES_FOR_PREPARE => [],	# Cached files for prepare
		  FILES_FOR_TRANSFER => [],	# Cached files for transfer
		  LAST_RESET => 0);
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    $$self{BACKEND} = eval ("use UtilsDownload$args{BACKEND_TYPE};"
	    		    . " new UtilsDownload$args{BACKEND_TYPE}(\@_)");
    do { chomp ($@); die "Failed to create backend: $@\n" } if $@;
    bless $self, $class;
    return $self;
}

######################################################################
# Construct database query parameters for ignore/accept filters.
sub sourceNodeFilters
{
    my ($self, $prefix) = @_;
    my ($ignore_filter, %ignore_args) = "";
    my ($accept_filter, %accept_args) = "";
    for (my $n = 0; $n < scalar @{$$self{IGNORE_NODES}}; ++$n)
    {
	$ignore_args{":ignore$n"} = $$self{IGNORE_NODES}[$n];
	$ignore_filter .= " and ${prefix}name != :ignore$n";
    }
    for (my $n = 0; $n < scalar @{$$self{ACCEPT_NODES}}; ++$n)
    {
	$accept_args{":accept$n"} = $$self{ACCEPT_NODES}[$n];
	$accept_filter .= " and ${prefix}name = :accept$n";
    }

    return ($ignore_filter, \%ignore_args,
	    $accept_filter, \%accept_args);
}

# Remove from the database all existing transfers we are not aware of.
# This may occur if we are stopped and restarted forcefully when there
# were pending transfers.  We assume we are the only live transfer
# agent for this node, so if we don't know about it, it's stale and
# can be removed.   Idem with previously failed transfers.
sub resetPendingTransfers
{
    my ($self, $dbh) = @_;
    my $from_node_filter = "";
    my ($ignore_filter, $ignore_args, $accept_filter, $accept_args)
        = $self->sourceNodeFilters ("");

    if ($ignore_filter || $accept_filter)
    {
        $from_node_filter =
            "and from_node in (select id from t_node"
	    . " where id = id$ignore_filter$accept_filter)";
    }

    &dbexec ($dbh, qq{
	update t_xfer_state
	set to_state = 1,
	    to_protocols = null,
	    to_pfn = null,
	    from_pfn = null,
	    time_request = :now,
	    time_available = null,
	    time_xfer_start = null,
	    time_xfer_end = null,
	    time_error_start = null,
	    time_error_end = null
	where to_node = :node
	  and (to_state = 2 or to_state = 100)
  	  and time_expire > :now
  	  $from_node_filter},
	":now" => &mytimeofday(), ":node" => $$self{ID_MYNODE},
	%$ignore_args, %$accept_args);
}

# Mark my files wanted.
sub markFilesWanted
{
    my ($self, $dbh) = @_;

    eval
    {
	# Restore files that have cooled off long enough.
	my $now = &mytimeofday();
    	&dbexec ($dbh, qq{
	    update t_xfer_state
	    set to_state = 0,
	        to_protocols = null,
	        to_pfn = null,
	        from_pfn = null,
		time_available = null,
	        time_error_start = null,
		time_error_end = null
	    where to_node = :node
	      and to_state = 100
              and time_error_end <= :now
              and time_expire > :now},
            ":now" => $now, ":node" => $$self{ID_MYNODE});

	# Mark all my files wanted.  We want this updated regularly enough.
	&dbexec ($dbh, qq{
	    update t_xfer_state
	    set to_state = 1, time_request = :now
	    where to_node = :node
	      and time_expire > :now
              and to_state <= 1},
    	    ":now" => $now, ":node" => $$self{ID_MYNODE});

	$dbh->commit();
    };

    do { chomp ($@); &alert ("failed to mark files wanted: $@");
	 eval { $dbh->rollback() }; } if $@;
}

# Return files we should prepare for transfer: give them a destination
# PFN and tell the source node which transfer protocols we accept.
sub filesForPrepare
{
    my ($self, $dbh) = @_;
    if (! @{$$self{FILES_FOR_PREPARE}})
    {
        eval
        {
	    # FIXME: Check if this file is for me?  Let the PFN script
	    # know if this file is actually destined for me, or only
	    # for other for pass-through traffic.
	    my ($ignore_filter, $ignore_args, $accept_filter, $accept_args)
	        = $self->sourceNodeFilters("ns.");

	    my $stmt = &dbexec ($dbh, qq{
		select
		    xs.fileid, f.logical_name, f.filetype, f.filesize, f.checksum,
		    xs.priority, xs.to_node to_node_id, nd.name to_node
		from t_xfer_state xs
		  join t_file f on f.id = xs.fileid
		  join t_node ns on ns.id = xs.from_node
		where xs.to_node = :node
	          and xs.to_pfn is null
	          and xs.time_expire > :now
	          $ignore_filter
		  $accept_filter},
  		":node" => $$self{ID_MYNODE}, ":now" => &mytimeofday(),
		%$ignore_args, %$accept_args);
	    while (my $row = $stmt->fetchrow_hashref())
	    {
		next if grep($$_{FILEID} eq $$row{FILEID}, @{$$self{FILES_FOR_PREPARE}});
	        push(@{$$self{FILES_FOR_PREPARE}}, $row);
	        last if (scalar @{$$self{FILES_FOR_PREPARE}} >= 100);
	    }
        };
        do { chomp ($@); &alert ("failed to select files for prepare: $@") } if $@;
    }

    # Use cached values if we have enough of them
    return shift (@{$$self{FILES_FOR_PREPARE}});
}

# Assign files destination PFN and possible transfer protocols.
sub setFilePrepared
{
    my ($self, $dbh, $file) = @_;
    my $stmt = &dbexec($dbh, qq{
	update t_xfer_state
	set to_pfn = :pfn, to_protocols = :protos
	where fileid = :fileid and to_node = :node},
	":pfn" => $$file{TO_PFN},
	":protos" => $$file{TO_PROTOCOLS},
	":fileid" => $$file{FILEID},
	":node" => $$self{ID_MYNODE});
}

# Get the list of N files ready for transfer.  Returns list of file hashes.
# Files are fetched from database in relatively large batches, at least
# 10*$n and minimum 50 at a time, and then returned however many files at
# a time the caller wants.  This avoids excessive roundtrip to the database.
sub filesForTransfer
{
    my ($self, $dbh, $n) = @_;

    if (scalar @{$$self{FILES_FOR_TRANSFER}} < $n)
    {
        eval
        {
	    # FIXME: Partition downloaded files into priority
	    # categories, and make sure we give a fair share
	    # to each priority category.  See notes in routing.

	    my ($ignore_filter, $ignore_args, $accept_filter, $accept_args)
	        = $self->sourceNodeFilters("ns.");

	    my $stmt = &dbexec ($dbh, qq{
		select
		    xs.fileid, f.logical_name, f.filetype, f.filesize, f.checksum,
		    xs.from_node from_node_id, ns.name from_node, xs.from_pfn,
		    xs.to_pfn, xs.time_assign, xs.time_available
		from t_xfer_state xs
		  join t_file f on f.id = xs.fileid
		  join t_node ns on ns.id = xs.from_node
		where xs.to_node = :node
		  and xs.to_state < 2
	  	  and xs.from_state = 1
	          and xs.to_pfn is not null
	          and xs.time_expire > :now
	  	  and xs.errors < 5
	          $ignore_filter
		  $accept_filter
		order by xs.priority asc, xs.weight asc, xs.age asc},
  		":node" => $$self{ID_MYNODE}, ":now" => &mytimeofday(),
		%$ignore_args, %$accept_args);
	    while (my $row = $stmt->fetchrow_hashref())
	    {
		next if grep($$_{FILEID} eq $$row{FILEID}, @{$$self{FILES_FOR_TRANSFER}});
	        push(@{$$self{FILES_FOR_TRANSFER}}, $row);
	        last if (scalar @{$$self{FILES_FOR_TRANSFER}} > 10*$n
			 && scalar @{$$self{FILES_FOR_TRANSFER}} > 50);
	    }
        };
        do { chomp ($@); &alert ("failed to select files for transfer: $@") } if $@;
    }

    # Use cached values if we have enough of them
    return splice (@{$$self{NEXT_FILES}}, 0, $n);
}

# Begin transferring a file.  Fetches the file information and creates
# a new drop for the file in a worker.  If the drop is successfully
# created, marks the file in transfer in the database and returns
# non-zero.  Otherwise returns non-zero to indicate another attempt
# should be made at a later time to initiate the transfer.
sub startFileTransfer
{
    my ($self, $dbh, $file) = @_;

    # Mark the transfer started in the database.
    eval
    {
	&dbexec ($dbh, qq{
	    update t_xfer_state
	    set to_state = 2, time_xfer_start = :now
	    where fileid = :fileid and to_node = :node},
	    ":now" => &mytimeofday (), ":filed" => $$file{FILEID},
	    ":node" => $$self{ID_MYNODE});
	$dbh->commit();
    };

    if ($@)
    {
	chomp ($@);
	&alert ("failed to mark $$file{LOGICAL_NAME} in transfer: $@");
	eval { $dbh->rollback() };
	return undef;
    }

    # Return a transfer object.
    return $file;
}

# Record completion for the transfer.  Mark files either transferred
# or send them to cool-off failure state.
sub completeTransfer
{
    my ($self, $file) = @_;
    my $dbh = eval { &connectToDatabase ($self, 0) };
    if ($@)
    {
	chomp ($@);
	&alert ("failed to update transfer batch status,"
		. " trying again later, error was: $@");
	$self->addJob (new sub { $self->completeTransfer($file) },
		       {}, "sleep", "10");
	return;
    }

    # Have a connection to the database, now update status for all files
    eval
    {
	my $state = 3;
	my $now = &mytimeofday ();
	if ($$file{FAILURE})
	{
	    $state = 100;
	    &alert ("failed to transfer $$file{FILEID} ($$file{LFN}): $$file{FAILURE}");
	    &dbexec ($dbh, qq{
		update t_xfer_state
		set to_state = $state,
		    errors = errors + 1,
		    time_xfer_start = null,
		    time_request = null,
		    time_error_start = :error_start,
		    time_error_end = :error_end,
		    time_error_total = time_error_total + :error_end - :error_start
		where fileid = :fileid and to_node = :node},
		":error_start" => $now,
		":error_end" => $now + 60*(30 + rand(30)*$$self{BACKOFF_FACTOR}),
		":fileid" => $$file{FILEID},
		":node" => $$self{ID_MYNODE});
	}
	else
	{
	    &dbexec ($dbh, qq{
		update t_xfer_state
		set to_state = $state, time_xfer_end = :now
		where fileid = :fileid and to_node = :node},
		":now" => $now,
		":fileid" => $$file{FILEID},
		":node" => $$self{ID_MYNODE});

	    &dbexec ($dbh, qq{
                insert into t_xfer_replica
		(id, fileid, node, state, time_create, time_state)
		values (seq_xfer_replica.nextval, :fileid, :node, 0, :now, :now)},
		":fileid" => $$file{FILEID},
		":node" => $$self{ID_MYNODE},
		":now" => $now);

	    &dbexec ($dbh, qq{
		insert into t_xfer_completed
		(select * from t_xfer_state
		 where fileid = :fileid and to_node = :node)},
		":fileid" => $$file{FILEID},
		":node" => $$self{ID_MYNODE});

	    &dbexec ($dbh, qq{
		delete from t_xfer_state
		where fileid = :fileid and to_node = :node},
		":fileid" => $$file{FILEID},
		":node" => $$self{ID_MYNODE});
	}

	# Generate timing profile data
	my @times = map { my ($op, $start, $end) = @$_;
		          sprintf("%s=%.2f", $op, $end-$start) }
		    @{$$file{TIMING}};

	# Log transfer delay stats
	my $dalloc = $now - $$file{TIME_ALLOC};
	my $dtransfer = $now - $$file{TIME_START};
	&logmsg ("xstats: $$file{FILEID} ($$file{LFN}) "
		 . "$$file{FROM_NODE} $$self{MYNODE} $state "
		 . sprintf('%.2f %.2f', $dalloc, $dtransfer)
	     	 . " $$file{FILESIZE} [ @times ]");
        $dbh->commit ();
    };

    # If we failed to update status, something's gone terribly bad,
    # just skip this file, we will re-transfer it later again.
    if ($@)
    {
	chomp ($@);
        &alert ("failed to update transfer batch status for $$file{FILEID}: $@");
        eval { $dbh->rollback() } if $dbh;
    }

    &disconnectFromDatabase ($self, $dbh);
}

# Pick up files to download from the database
sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    my $more = 0;
    eval
    {
    	$dbh = &connectToDatabase ($self) or die "failed to connect";

	# If there are no pending jobs, kill database ghosts
	my $now = &mytimeofday();
	if (scalar @{$$self{JOBS}} == 0 && $$self{LAST_RESET} < $now - 900)
	{
	    my $tmpdir = $$self{DROPDIR};
	    unlink <$tmpdir/*-*-*-*>;
	    $self->resetPendingTransfers($dbh);
	    $$self{LAST_RESET} = $now;
        }

        # Request some more files
        $self->markFilesWanted ($dbh);

	# FIXME: Assign PFNs for files
	# FIXME: Throttle myself if lots of recent errors

	# Initiate as many transfers as we can.
	while (scalar @{$$self{JOBS}} < $$self{NJOBS})
	{
	    last if ! ($more = $$self{BACKEND}->consumeFiles ($self, $dbh));
	    $self->maybeStop ();
	    $self->pumpJobs ();
	}

        # Disconnect from the database
        &disconnectFromDatabase ($self, $dbh);
    };
    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;

    # Keep working, resting a bit at a time.  If we have work to do
    # and can start more jobs, break out of this loop and initiate
    # new transfers.  Otherwise if we have exhausted files that can
    # be transferred, just sleep here for a moment.
    my $target = &mytimeofday() + $$self{WAITTIME};
    while (&mytimeofday() < $target)
    {
        $self->maybeStop ();
	$self->pumpJobs ();
	last if $more && scalar @{$$self{JOBS}} < $$self{NJOBS};
	select (undef, undef, undef, .1);
    }
}
