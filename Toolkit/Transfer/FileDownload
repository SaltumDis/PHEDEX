#!/usr/bin/perl

## This is an example transfer agent.
##
## This agent demonstrates how to write a parallel transfer agent.
## It monitors TMDB for files assigned to the agent, creates drop
## box tasks for them internally, and keeps a configurable number
## of transfers going on in parallel.
##
## In implementation, this is a combined drop box and database
## agent.  The TMDB monitoring is done in the &idle() routine,
## which is called whenever there is nothing to be done to the
## drops any more, which is most of the time.  The idle routine
## assigns files to idle workers.  Once the slave worker agent
## has completed the transfer, whether successfully or not, it
## moves the drop to the *inbox* of this agent, where this agent
## collects the status information and updates the database to
## reflect transfer progress.  In other words, the drops cycle
## from the idle routine into the inbox of a slave, to the work
## area of a slave, to inbox of the main agent, to its work area,
## and finally udpated back to the database and destroyed.  Never
## add data from outside this agent into its inbox!
##
## The copies this agent does are pure globus-url-copy operations
## for demonstration purpose.  This makes this agent fully capable
## LCG SE transfer agent.

BEGIN {
  use strict; use warnings;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args = (DBITYPE => "Oracle");
while (scalar @ARGV)
{
    if ($ARGV[0] eq '-db' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBNAME} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbi' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBITYPE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbuser' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBUSER} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-dbpass' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DBPASS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-node' && scalar @ARGV > 1)
    { shift (@ARGV); $args{MYNODE} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-wanted' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WANT_LIMIT} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-jobs' && scalar @ARGV > 1)
    { shift (@ARGV); $args{NJOBS} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-backend' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BACKEND_TYPE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-state' && scalar @ARGV > 1)
    { shift (@ARGV); $args{DROPDIR} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-wait' && scalar @ARGV > 1)
    { shift (@ARGV); $args{WAITTIME} = shift(@ARGV); }

    elsif ($ARGV[0] eq '-pfndest' && scalar @ARGV > 1)
    { shift (@ARGV); $args{PFNSCRIPT} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-batch-files' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BATCH_FILES} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-batch-size' && scalar @ARGV > 1)
    { shift (@ARGV); $args{BATCH_SIZE} = shift(@ARGV); }
    elsif ($ARGV[0] eq '-command' && scalar @ARGV > 1)
    { shift (@ARGV); push (@{$args{COMMAND}}, split (/,/, shift(@ARGV))); }
    elsif ($ARGV[0] eq '-timeout' && scalar @ARGV > 1)
    { shift (@ARGV); $args{TIMEOUT} = shift (@ARGV); }
    else
    { last; }
}

$args{WANT_LIMIT} = &sizeValue ($args{WANT_LIMIT}) if $args{WANT_LIMIT};
$args{BATCH_SIZE} = &sizeValue ($args{BATCH_SIZE}) if $args{BATCH_SIZE};

if (scalar @ARGV || !$args{DROPDIR} || !$args{DBNAME} || !$args{DBUSER}
    || !$args{DBPASS} || !$args{DBITYPE} || !$args{MYNODE}
    || !$args{WANT_LIMIT} || !$args{PFNSCRIPT} || !$args{NJOBS}
    || !$args{BACKEND_TYPE})
{
    print STDERR
	"usage: $me -state IN-DROP-BOX -backend BACKEND-TYPE\n",
	"    -db NAME -dbuser USER -dbpass PASSWORD [-dbitype TYPE]\n",
	"    -node NODE-NAME -pfndest PROGRAM -wanted SIZE[kMGT]\n",
	"    [-batch-files N-FILES-LIMIT] [-batch-size SIZE-LIMIT[kMGT]]\n",
	"    [-command COPY-COMMAND[,ARGS]] [-jobs NUM-PARALLEL-PROCESSES]\n",
	"    [-timeout TIMEOUT] [-wait SECS-TO-WAIT]\n";
    exit (1);
}

my $agent = new FileDownload (%args);
# Recapture interrupt signal, oracle swallows it.
$SIG{INT} = sub { system "touch $agent->{STOPFLAG}"; $agent->maybeStop (); };
$agent->process ();

sub sizeValue
{
    my ($value) = @_;
    if ($value =~ /^(\d+)([kMGT])$/)
    {
        my %scale = ('k' => 1024, 'M' => 1024**2, 'G' => 1024**3, 'T' => 1024**4);
        $value = $1 * $scale{$2};
    }
    return $value;
}

######################################################################
# Routines specific to this agent.
package FileDownload; use strict; use warnings; use base 'UtilsAgent';
use File::Path;
use Data::Dumper;
use UtilsCommand;
use UtilsLogging;
use UtilsTiming;
use UtilsCatalogue;
use UtilsDB;
use POSIX;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBITYPE => undef,		# Database driver binding
    		  DBNAME => undef,		# Database name
	  	  DBUSER => undef,		# Database user name
	  	  DBPASS => undef,		# Database user password
	  	  MYNODE => undef,		# My TMDB node name
		  WANT_LIMIT => undef,		# Amount of data to prefetch
		  BACKEND => undef,		# Backend type
		  NJOBS => 1,			# Max number of parallel processes
		  JOBS => [],			# Pending jobs
		  LAST_RESET => 0,		# Last time we reset bad transfers
	  	  AGENTID => "Download");	# Identity for activity logs
    my %args = (@_);
    map { $self->{$_} = $args{$_} || $params{$_} } keys %params;
    $self->{BACKEND} = eval "use UtilsDownload$args{BACKEND_TYPE}; new UtilsDownload$args{BACKEND_TYPE}(\@_)";
    die "Failed to create backend: $@\n" if $@;
    bless $self, $class;
    return $self;
}

######################################################################
# JOB MANAGEMENT TOOLS
#
# Add a new command to the job list.  The command will only be started
# if the current limit of available job slots is not exceeded; otherwise
# the job simply gets added to the list of processes to start later on.
# If the command list is empty, the job represents a delayed action to
# be invoked on the next "pumpJobs".
sub addJob
{
    my ($self, $action, $jobargs, @cmd) = @_;
    my $job = { PID => 0, ACTION => $action, CMD => [ @cmd ], %{$jobargs || {}} };
    my $jobs = $self->{JOBS};
    push (@$jobs, $job);

    $self->startJob($job)
        if (scalar @cmd && scalar (grep ($_->{PID} > 0, @$jobs)) < $self->{NJOBS});
}

# Actually fork and execute a subcommand.  Updates the job object to
# have the process id of the subprocess.  Internal helper routine.
sub startJob
{
    my ($self, $job) = @_;
    my $pid = undef;
    while (1)
    {
        last if defined ($pid = fork ());
        print STDERR "cannot fork: $!; trying again in 5 seconds\n";
        sleep (5);
    }

    if ($pid)
    {
	# Parent, record this child process
	$job->{PID} = $pid;
	$job->{STARTED} = time();
    }
    else
    {
	# Child, execute the requested program
        exec { $job->{CMD}[0] } @{$job->{CMD}};
        die "Cannot start @{$job->{CMD}}: $!\n";
    }
}

# Find out which subprocesses have finished and collect them to a list
# returned to the caller.  Finished jobs are removed from JOBS list.
# Internal helper routine.
sub checkJobs
{
    my ($self) = @_;
    my @pending = ();
    my @finished = ();
    my $now = 0;
    foreach my $job (@{$self->{JOBS}})
    {
	my $status;
	if (! scalar @{$job->{CMD}})
	{
	    # Delayed action callback, no job associated with this one
	    push (@finished, $job);
	}
	elsif ($job->{PID} > 0
	       && $job->{TIMEOUT}
	       && ($now - $job->{STARTED}) > $job->{TIMEOUT})
	{
	    # Command has taken too long to execute.  Nuke it.  First time
	    # around use SIGINT.  Next time around use SIGKILL.
	    kill ($job->{PID}, $job->{FORCE_TERMINATE} ||= 1);
	    $job->{FORCE_TERMINATE} = 9;
	}
	elsif ($job->{PID} > 0 && waitpid ($job->{PID}, WNOHANG) > 0)
	{
	    # Command finished executing, save exit code and mark finished
	    $job->{STATUS} = $? / 256;
	    push (@finished, $job);
	}
	else
	{
	    # Still pending
	    push(@pending, $job);
	}
    }

    $self->{JOBS} = \@pending;
    return @finished;
}

# Invoke actions on completed subprocesses and start new jobs if there
# are free slots.  Invoke this every once in a while to keep processes
# going.
sub pumpJobs
{
    my ($self) = @_;
    
    # Invoke actions on completed jobs
    foreach my $job ($self->checkJobs())
    {
	&{$job->{ACTION}} ($job);
    }

    # Start new jobs if possible
    my $jobs = $self->{JOBS};
    my $running = grep ($_->{PID} > 0, @$jobs);
    foreach my $job (@$jobs)
    {
	next if ! @{$job->{CMD}};
	next if $job->{PID} > 0;
	last if $running >= $self->{NJOBS};
	$self->startJob ($job);
	$running++;
    }
}

######################################################################
# FILE TRANSFER MANAGEMENT
#
# Remove from the database all existing transfers we are not
# aware of.  This may occur if we are stopped and restarted
# forcefully when there were pending transfers.  We assume
# we are the only live transfer agent for this node, so if
# we don't know about it, it's stale and can be removed.
# Same with previously failed transfers, revert them all back.
sub resetPendingTransfers
{
    my ($self, $dbh) = @_;
    my $mynode = $self->{MYNODE};
    my $now = time();
    $dbh->do(qq{
	    update t_transfer_state
	    set to_state = 1, to_time_stamp = $now
	    where to_node = '$mynode'
	      and (to_state = 2 or to_state >= 100)});
}

# Record completion for the transfer.  Mark files either transferred
# or send them to cool-off failure state.
sub completeTransferBatch
{
    my ($self, $batch) = @_;
    my $dbh = eval { &connectToDatabase ($self, 0) };
    if ($@)
    {
	&alert ("failed to update transfer batch status,"
		. " trying again later, error was: $@");
	$self->addJob (new sub { $self->completeTransferBatch($batch) });
	return;
    }

    # Have a connection to the database, now update status for all files
    eval
    {
        foreach my $file (@$batch)
        {
	    my $now = time();
	    my $to_state = 3;
	    my $guid = $file->{GUID};
	    my $to_node = $file->{TO_NODE};
	    my $from_node = $file->{FROM_NODE};
	    if ($file->{FAILURE})
	    {
	         &logmsg ("failed to transfer $guid: $file->{FAILURE}");
	         $to_state = 101 + int(rand(10)); # 1..10 min cool-off
	    }

            $dbh->do (qq{
              update t_transfer_state
              set to_state = $to_state, to_time_stamp = $now
              where guid = '$guid'
                and to_node = '$to_node'
                and from_node = '$from_node'});

            $dbh->do (qq{
              insert into t_replica_state values
              ('$guid', '$to_node', $now, 0, $now, 0, $now)})
                if ! $file->{FAILURE};

	    # Log transfer delay stats
	    $now = &mytimeofday ();
	    my $dalloc = $now - $file->{TIME_ALLOC};
	    my $dtransfer = $now - $file->{TIME_START};
	    &logmsg ("xstats: $guid $to_node $to_state "
		     . sprintf('%.2f %.2f', $dalloc, $dtransfer)
	     	     . " $file->{FILESIZE}");
        }
        $dbh->commit ();
    };

    # If we failed to update status, something's gone terribly bad,
    # just skip these files, we will re-transfer them later again.
    if ($@)
    {
        &alert ("failed to update transfer batch status: $@");
        $dbh->rollback() if $dbh;
    }

    $dbh->disconnect();
    undef $dbh;
}

# Begin transferring a file.  Fetches the file information and creates
# a new drop for the file in a worker.  If the drop is successfully
# created, marks the file in transfer in the database and returns
# non-zero.  Otherwise returns non-zero to indicate another attempt
# should be made at a later time to initiate the transfer.
sub startFileTransfer
{
    my ($self, $dbh, $info) = @_;
    my ($guid, $size, $timestamp,
	$from_node, $from_catalogue, $from_host,
	$to_catalogue, $attrs) = @$info;

    # Mark the transfer started in the database.
    eval
    {
	my $now = time();
	my $mynode = $self->{MYNODE};
	$dbh->do (qq{
	  update t_transfer_state
	  set to_state = 2, to_time_stamp = $now
	  where guid = '$guid'
	    and to_node = '$mynode'
	    and from_node = '$from_node'});
	$dbh->commit();
    };

    if ($@)
    {
	&alert ("failed to mark $guid in transfer: $@");
	$dbh->rollback();
	return undef;
    }

    # Return a transfer object.
    return { GUID		=> $guid,
	     FILESIZE		=> $size,
	     TIME_ALLOC		=> $timestamp,
	     TIME_START		=> &mytimeofday(),
	     FROM_NODE		=> $from_node,
	     FROM_CATALOGUE	=> $from_catalogue,
	     FROM_HOST		=> $from_host,
	     TO_NODE		=> $self->{MYNODE},
	     TO_CATALOGUE	=> $to_catalogue,
	     ATTRS		=> $attrs };
}

# Update wanted status on files in the database.
sub markFilesWanted
{
    my ($self, $dbh) = @_;
    eval
    {
	my $mynode = $self->{MYNODE};
	my $now = time();

	# Tick back-off timer for previously failed transfers.
	# Each state above 100 counts as one-minute cool-down
	# period (see code above how this is assigned).
	# (FIXME: restore old time stamp?)
	my $old = $now - 60;
	$dbh->do(qq{
		update t_transfer_state
		set to_state = to_state - 1,
		    to_time_stamp = $now
		where to_node = '$mynode'
		  and to_state >= 101
	  	  and to_time_stamp < $old});
	$dbh->do(qq{
		update t_transfer_state
		set to_state = 0,
		    to_time_stamp = $now
		where to_node = '$mynode'
		  and to_state = 100
	  	  and to_time_stamp < $old});

	# Keep wanted time stamp at most ten minutes old.
	$old = $now - 600;
	$dbh->do(qq{
		update t_transfer_state
		set to_time_stamp = $now
		where to_node = '$mynode'
		  and to_state = 1
	  	  and to_time_stamp < $old});

  	# Check how much we already marked wanted.  Only mark new
	# files wanted if we drop below 75% of our limit.
	my $row = $dbh->selectrow_arrayref(qq{
		select sum(m.value)
		from t_transfer_state ts
		left join t_replica_metadata m
		  on ts.guid = m.guid and m.attribute = 'filesize'
		where ts.to_node = '$mynode' and ts.to_state = 1});
	return if (! defined $row || ($row->[0] || 0) > .75 * $self->{WANT_LIMIT});

	# Mark more files wanted.  Select the best new files, keeping
	# track of accumulated size and stopping at our limit.  The
	# ordering criteria here is decreasing day and the job-id
	# (FIXME: CMS/COBRA specific!).  We pick the newest files first
	# -- if we are moving files in streaming mode and fall behind,
	# it's better to keep up with the stream the best we can rather
	# than disrupt everyone else trying to slug our way through the
	# old files.  (FIXME: Custom file priority rules?)
	my $total = $row->[0] || 0;
	my $stmt = $dbh->prepare(qq{
		select ts.guid, m1.value
		from t_transfer_state ts
		left join t_replica_metadata m1
		  on ts.guid = m1.guid and m1.attribute = 'filesize'
		left join t_replica_metadata m2
		  on ts.guid = m2.guid and m2.attribute = 'POOL_jobid'
		where ts.to_node = '$mynode' and ts.to_state = 0
		order by ts.from_state desc, trunc (ts.to_time_stamp/86400) desc, m2.value});
	$stmt->execute();
	while (my $file = $stmt->fetchrow_arrayref())
        {
	    my ($guid, $size) = @$file;

	    $total += $size;
	    last if ($total > $self->{WANT_LIMIT});

	    $dbh->do(qq{
		update t_transfer_state
		set to_state = 1, to_time_stamp = $now
		where guid = '$guid' and to_node = '$mynode'});
  	}

	$dbh->commit();
    };

    do { &alert ("failed to mark files wanted: $@"); $dbh->rollback() } if $@;
}

# Get the list of N guids that should be transferred next.  Returns
# a list of arrays with members GUID, ALLOC_TIMESTAMP, FROM_NODE,
# FROM_CATALOGUE, TO_CATALOGUE plus a hash of POOL file attributes.
sub nextFiles
{
    my ($self, $dbh, $n) = @_;

    # FIXME: sort by filegroup, descending time?
    my @result = ();
    eval
    {
	my $mynode = $self->{MYNODE};
	my $stmt = $dbh->prepare (qq{
		select ts.guid,
		       m.value,
		       ts.from_time_stamp,
		       ts.from_node,
		       n1.catalogue_contact,
		       n1.host_string,
		       n2.catalogue_contact
		from t_transfer_state ts
		left join t_replica_metadata m
		  on m.guid = ts.guid
		  and m.attribute = 'filesize'
		left join t_nodes n1
		  on n1.node_name = ts.from_node
		left join t_nodes n2
		  on n2.node_name = ts.to_node
		where ts.to_node = '$mynode'
		  and ts.to_state < 2
	  	  and ts.from_state = 1});
        $stmt->execute();
	while (my $row = $stmt->fetchrow_arrayref())
	{
	    last if ! $n--;
	    my $attrs = {};
	    foreach my $m (@{$dbh->selectall_arrayref(qq{
				select m.attribute, m.value
				from t_replica_metadata m
				where m.guid = '$row->[0]'
				  and m.attribute like 'POOL%'})})
	    {
		my ($key, $value) = @$m;
		$key =~ s/^POOL_//;
		$attrs->{$key} = $value;
	    }

	    push(@result, [ @$row, $attrs ]);
	}
    };
    &alert ("failed to select files for transfer: $@") if $@;
    return @result;
}

# Pick up files to download from the database
sub idle
{
    my ($self, @pending) = @_;
    if (my $dbh = &connectToDatabase ($self))
    {
        # FIXME: Pick up and process messages to me

	# If there are no pending jobs, make sure database ghosts
	my $now = time();
	if (scalar @{$self->{JOBS}} == 0 && $self->{LAST_RESET} < $now - 900)
	{
	    my $tmpdir = $self->{DROPDIR};
	    unlink <$tmpdir/*-*-*-*>;
	    $self->resetPendingTransfers($dbh);
	    $self->{LAST_RESET} = $now;
        }

        # Request some more files
        $self->markFilesWanted ($dbh);

	# If there's still space in job queue, start new transfers.
	while (scalar @{$self->{JOBS}} < $self->{NJOBS})
	{
	    $self->maybeStop ();
	    $self->pumpJobs ();
	    last if ! $self->{BACKEND}->consumeFiles ($self, $dbh);
	}

        # Disconnect from the database
        $dbh->disconnect if $dbh;
        undef $dbh;
    }

    # Keep jobs going while resting a bit.
    my $target = time() + $self->{WAITTIME};
    do {
        $self->maybeStop();
	$self->pumpJobs ();
	select (undef, undef, undef, .1);
    } while (time() < $target);
}
