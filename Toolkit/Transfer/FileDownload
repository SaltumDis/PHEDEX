#!/usr/bin/env perl

##H Parallel reliable file download agent.
##H
##H This agent monitors TMDB for files assigned to the agent, and
##H manages their download.  A separate backend (Globus, SRM, ...)
##H handles the actual transfer steps.  A configurable number of
##H files is always kept in transfer, using subprocesses to manage
##H transfers and other tasks as appropriate.  The again maintains
##H a local pool of transfer work and uploads file transfer report
##H to the database every once in a while.
##H
##H The agent creates batches of transfer "jobs" out of its queue
##H of files to transfer.  As files complete transfer, the agent
##H verifies transfer completeness, and then uploads the full
##H transfer report back to the database where the central file
##H mover manages the rest.
##H
##H Interaction with the site infrastructure is through scripts
##H named in command line arguments.  The required tools are
##H described in detail in the manual.  The general syntax is
##H CMD[,ARG...], where CMD is the actual command to execute,
##H followed by comma-separated list of arguments to pass to it.
##H
##H Usage:
##H   FileDownload
##H      -state DIRECTORY -nodes PATTERN[,PATTERN...]
##H      -db FILE[:SECTION] [-log OUT] [-verbose]
##H      [-ignore NODE[,NODE...]] [-accept NODE[,NODE...]]
##H      [-validate CMD[,ARG...]] [-delete CMD[,ARG...]]
##H      -backend TYPE [OPTIONS]
##H
##H Options to all backends:
##H      [-protocols NAME[,NAME...]] [-timeout SECS]
##H
##H Options to Null backend:
##H      (none)
##H
##H Options to Globus backend:
##H      [-command CMD[,ARG...]] [-jobs NJOBS]
##H
##H Options to SRM backend:
##H      [-command CMD[,ARG...]] [-batch-files N] [-jobs NJOBS]
##H
##H Options to FTS backend:
##H      -passfile FILE -server CONTACT [-batch-files N]
##H
##H -state       agent state directory
##H -nodes       patterns for the node names for which this agent runs
##H -db          database connection configuration parameter file
##H -log         where to redirect logging information
##H -verbose     include more information about internal algorithms
##H
##H -ignore      comma-separated list of nodes to ignore transfers from
##H -accept      comma-separated list of nodes to accept transfers from
##H -validate    command to verify file transfer success; if not specified
##H               uses transfer command exit code, normally a bad idea
##H -delete      command to delete file on failure
##H
##H -backend     the transfer backend to use: Globus, SRM, FTS, DCCP; all
##H               options that follow are passed to the backend as such
##H
##H -protocols   comma-separated list of storage protocols to accept
##H -batch-files number of files per transfer batch (SRM, FTS; default: 30)
##H -jobs        maximum number of concurrent transfer jobs (default: 5)
##H -timeout     execution time limit for commands (default: 3600)
##H -command     override the transfer command, for instance
##H               globus-url-copy,-p,3,-tcp-bs,2097152
##H -passfile    file containing FTS proxy password
##H -server      FTS server to use

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args = (CMDLINE => [ @ARGV ]);
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
	     "verbose"     => \$args{VERBOSE},
             "nodes=s"     => sub { push(@{$args{NODES}},
					 split(/,/, $_[1])) },

             "ignore=s"    => sub { push(@{$args{IGNORE_NODES}},
					 split(/,/, $_[1])) },
             "accept=s"    => sub { push(@{$args{ACCEPT_NODES}},
					 split(/,/, $_[1])) },

             "validate=s"  => sub { push(@{$args{VALIDATE_COMMAND}},
					 split(/,/, $_[1])) },
             "delete=s"    => sub { push(@{$args{DELETE_COMMAND}},
					 split(/,/, $_[1])) },

             "backend=s"   => sub { $args{BACKEND_TYPE} = $_[1]; die "!FINISH"; },

	     "help|h"      => sub { &usage() });

die "Insufficient parameters, use -h for help.\n"
    if (!$args{DROPDIR} || !$args{NODES} || !$args{DBCONFIG}
	|| !$args{BACKEND_TYPE});

(new FileDownload (%args, BACKEND_ARGS => [ @ARGV ]))->process();

######################################################################
# Routines specific to this agent.
package FileDownload; use strict; use warnings; use base 'UtilsAgent';
use List::Util qw(min max sum);
use File::Path qw(mkpath rmtree);
use UtilsLogging;
use UtilsTiming;
use UtilsDB;
use POSIX;

our $TASK_CODE_SUCCESS = 0;
our $TASK_CODE_LOST = 1;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
		  NODES => undef,		# Nodes to operate for
	  	  IGNORE_NODES => [],		# TMDB nodes to ignore
	  	  ACCEPT_NODES => [],		# TMDB nodes to accept

		  VALIDATE_COMMAND => undef,	# Post-download test command
		  DELETE_COMMAND => undef,	# Delete bad files command
		  TIMEOUT => 600,		# Maximum execution time
		  NJOBS => 10,			# Max number of utility processes
		  WAITTIME => 15,		# Nap length between cycles
		  VERBOSE => 0,			# Verbosity

		  BACKEND_TYPE => undef,	# Backend type
		  BACKEND_ARGS => undef,	# Options to the backend

		  NODE => {},			# Node bandwidth parameters
		  LINKS => {},			# Per-link transfer parameters
		  FILES => {},			# Files to transfer

		  FIRST_ACTION => undef,	# Time of first action
		  ACTIONS => [],		# Future actions

		  TASK_DIR => "$$self{DROPDIR}/tasks", # Tasks to do

		  LAST_SYNC_PUSH => 0,		# Last time we pushed status
		  LAST_SYNC_PULL => 0,		# Last time we pulled tasks
		  DBH_LAST_USE => 0,		# Last time we used database
	  	  );
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    $$self{BACKEND} = eval ("use UtilsDownload$args{BACKEND_TYPE};"
	    		    . " new UtilsDownload$args{BACKEND_TYPE}"
			    . " (MASTER => \$self, \@_)");
    do { chomp ($@); die "Failed to create backend: $@\n" } if $@;
    -d $$self{TASK_DIR} || mkdir($$self{TASK_DIR}) || -d $$self{TASK_DIR}
        || die "$$self{TASK_DIR}: cannot create: $!\n";
    bless $self, $class;
    return $self;
}

# If stopped, tell backend to stop, then wait for all the pending
# utility jobs to complete.  All backends just abandon the jobs, and
# we try to pick up on the transfer again if the agent is restarted.
# Utility jobs usually run quickly so we let them run to completion.
sub stop
{
    my ($self) = @_;

    # Tell backend to stop.
    $$self{BACKEND}->stop();

    # Now wait for utility processes to finish.
    if (@{$$self{JOBS}})
    {
        &logmsg ("waiting pending jobs to finish...");
        while (@{$$self{JOBS}})
        {
            $self->pumpJobs();
	    select(undef, undef, undef, .1);
        }
        &logmsg ("all pending jobs finished, ready to exit");
    }
    else
    {
        &logmsg ("no pending jobs, ready to exit");
    }

    # Clear to exit.
}

######################################################################
# Schedule an action to execute at some future time.  The actions are
# added and executed in the order added, except that no action is
# executed before the given time.  Each action carries a tag, and if
# a new action with the same tag is added, the one with earlier time
# is kept in the queue.
sub setAction
{
    my ($self, $time, $action) = @_;

    # If this action is before our current first action, register.
    $$self{FIRST_ACTION} = $time
        if (! defined $$self{FIRST_ACTION} || $$self{FIRST_ACTION} > $time);

    # See if we there is an existing earlier or later action.
    for (my $i = 0; $i < scalar @{$$self{ACTIONS}}; ++$i)
    {
	next if $$self{ACTIONS}[$i]{ACTION} ne $action;
	return if $$self{ACTIONS}[$i]{TIME} <= $time;
	$$self{ACTIONS}[$i]{TIME} = $time;
	$$self{ACTIONS}[$i]{ACTION} = $action;
	return;
    }

    # No previous entry, add this.
    push(@{$$self{ACTIONS}}, { ACTION => $action, TIME => $time });
}

# Run all actions which were scheduled to run by now.
sub flushActions
{
    my ($self) = @_;
    my $now = &mytimeofday();
    my $old = $$self{ACTIONS};
    my ($new, $run) = ([], []);

    # Split "old" action list into "run" (run now) and "new" (to retain
    # for a future flush time).  Keep the actions in the order they were
    # in.
    my $first = undef;
    while (@$old)
    {
        my $a = shift(@$old);
	if ($$a{TIME} > $now)
	{
	    push(@$new, $a);
	    $first = $$a{TIME} if (! defined $first || $first > $$a{TIME});
        }
	else
	{
	    push(@$run, $a);
        }
    }

    # Swap the new action list into place.  This guarantees that new
    # actions added by calling those in "run" list will go to the
    # end of the list by default.
    $$self{ACTIONS} = $new;
    $$self{FIRST_ACTION} = $first;

    # Now execute the actions in "run" list.
    foreach my $a (@$run)
    {
	&logmsg("executing action $$a{ACTION} scheduled at $$a{TIME}")
	    if $$self{VERBOSE};
        $$a{ACTION}->($self);
    }
}

######################################################################
sub fetchMoreFiles
{
    my ($self) = @_;
}

sub createTransferJob
{
}

sub checkTransferJobs
{
}

sub startFileTransfer
{
}

sub finishFileTransfer
{
}

sub uploadTransferStatus
{
}

# Compare local transfer pool with database and reset everything one
# or the other doesn't know about.  We assume this is the only agent
# managing transfers for the links it's given.  If database has a
# locally unknown transfer, we mark the database one lost.  If we
# have a local transfer unknown to the database, we trash the local.
sub purgeLostTransfers
{
    my ($self) = @_;
    my (%inlocal, %indb) = ();

    # Get the local transfer pool.  All we need is the task ids.
    $inlocal{$_} = 1 for &getdir($$self{TASK_DIR});

    # Get the database transfer pool.  Again, just task ids.
    my ($dest, %dest_args) = &myNodeFilter ($self, "xt.to_node");
    my ($filter, %filter_args) = &otherNodeFilter ($self, "xt.from_node");
    my $qpending = &dbexec($$self{DBH}, qq{
      select xt.id from t_xfer_task xt where $dest $filter
        and exists (select 1 from t_xfer_task_inxfer where task = xt.id)
        and not exists (select 1 from t_xfer_task_done where task = xt.id)},
      %dest_args, %filter_args);
    $indb{$_} = 1 while (($_) = $qpending->fetchrow());

    # Calculate differences.
    my @lostlocal = grep(! exists $indb{$_}, keys %inlocal);
    my @lostdb    = grep(! exists $inlocal{$_}, keys %indb);

    # Mark locally unknown tasks as lost in database.
    my $now = &mytimeofday();
    my $qlost = &dbprep($$self{DBH}, qq{
      insert into t_xfer_task_done
        (task, report_code, time_update)
	values (:task, $TASK_CODE_LOST, :now)});
    &dbbindexec($qlost, ":now" => $now, ":task" => $_) for @lostdb;

    # Remove locally known tasks forgotten by database.
    unlink(map { "$$self{TASK_DIR}/$_" } @lostlocal);
}

sub refreshLinks
{
    my ($self) = @_;
    my $to;
	# Disable and enable links as necessary
	my $bytes = sum(0, map { $$_{BYTES} } values %{$$self{LINKS}{$to}});
	foreach my $from (keys %{$$self{LINKS}{$to}})
	{
	    my $link = $$self{LINKS}{$to}{$from};
	    my $linkcap = $$link{PARAMS}{BANDWIDTH_CAP};
	    my $nodecap = $$self{NODE}{$to}{BANDWIDTH_CAP};

	    if ($$link{ERRORS} > 100)
	    {
		&logmsg ("excessive errors in transfers on link"
			 . " from $from to $to,"
			 . " suspending transfers on this link")
		    if ! $$link{DOWN};
		$$link{DOWN} = 1;
	    }
	    elsif (defined $linkcap && $$link{BYTES} > $linkcap * 3600)
	    {
		&logmsg ("transfer rate on link from $from to $to"
			 . " exceeded link bandwidth cap "
			 . sprintf ("%.1f", $linkcap/(1024**2))
			 . " MB/s, suspending transfers on this link")
		    if ! $$link{DOWN};
		$$link{DOWN} = 2;
	    }
	    elsif (defined $nodecap && $bytes > $nodecap * 3600)
	    {
		&logmsg ("aggregate transfer rate on all links to $to"
			 . " exceeded node bandwidth cap "
			 . sprintf ("%.1f", $nodecap/(1024**2))
			 . " MB/s, suspending transfers on all links")
		    if ! $$link{DOWN};
		$$link{DOWN} = 3;
	    }
	    elsif ($$link{DOWN})
	    {
		&logmsg ("reactivating transfers on suspended link"
			 . " from $from to $to");
		$$link{DOWN} = 0;
	    }
	}
}

# Update file transfer work list.
#
# The full work list is refreshed from the database only every few
# minutes, or if the agent is running out of work.  The agent will
# therefore take a little time to adjust to behind-the-scenes changes
# to the database.  Files already in transfer by the agent are not
# refreshed from the database.
#
# The work queue contains all files available for transfer on active
# links.  The lists exclude files not yet available for transfer,
# missing either source or destination name, in error states, on
# suspended links, expired or having experienced too many errors.
sub refreshFiles
{
    my ($self, $dbh) = @_;
    my $now = &mytimeofday ();

    # Remove transfers on links that are now down
    delete $$self{FILES}{$$_{FILEID}}
	for grep($$_{TO_STATE} != 2
		 && $$self{LINKS}{$$_{TO_NODE}}{$$_{FROM_NODE}}{DOWN},
		 values %{$$self{FILES}});

    # Avoid refresh if we've read recently and have enough to chew on
    return if (($$self{LAST_FILES} > $now - 300 && scalar keys %{$$self{FILES}})
	       || $$self{LAST_FILES} > $now - 15);

    # Reset old state
    delete $$self{FILES}{$$_{FILEID}}
      for grep($$_{TO_STATE} != 2, values %{$$self{FILES}});

    # Fetch transfer backlog
    my ($dest, %dest_args) = &myNodeFilter ($self, "xs.to_node");
    my ($filter, %filter_args) = &otherNodeFilter ($self, "xs.from_node");

    my $q = &dbexec ($dbh, qq{
	select
	    xs.fileid, f.logical_name, f.filesize, f.checksum, b.name inblock,
	    xs.priority, xs.weight, xs.age,
	    xs.to_node to_node_id, nd.name to_node, xs.to_state, xs.to_pfn,
	    xs.from_node from_node_id, ns.name from_node, xs.from_pfn,
	    xs.time_assign, xs.time_available
	from t_xfer_state xs
	  join t_xfer_file f on f.id = xs.fileid
	  join t_node nd on nd.id = xs.to_node
	  join t_node ns on ns.id = xs.from_node
          join t_dps_block b on b.id = f.inblock
	where $dest
	  and xs.from_state = 1
 	  and xs.to_pfn is not null
	  and xs.time_expire > :now
	  and xs.errors < 5
	  $filter},
	":now" => $now,
	%dest_args,
	%filter_args);

    while (my $row = $q->fetchrow_hashref())
    {
	# Remember the file if it's not on a link that's down
	$$self{FILES}{$$row{FILEID}} = $row
	    if (! exists $$self{FILES}{$$row{FILEID}}
		&& ! $$self{LINKS}{$$row{TO_NODE}}{$$row{FROM_NODE}}{DOWN});
    }

    &logmsg ("@{[scalar keys %{$$self{FILES}}]} files in refreshed list")
	if $$self{VERBOSE};
    $$self{LAST_FILES} = $now;
}

# Issue file transfers.
sub issueTransfers
{
    my ($self, $dbh, $to, $nxfers) = @_;

    # Divide up the $nxfers transfer share by link and priority using
    # a "corrected auto-tuning fair share algorithm" as described at
    # http://www.psc.edu/publications/tech_reports/fairshare/fairshare.html.
    #
    # The essence of the algorithm is:
    # 1. P = the total size of the pool, allocated and unallocated
    #    D(i) = the amount of buffer space desired by connection i
    #    B(i) = the amount of buffer space allotted to connection i so
    #           far (zero initially).
    # 2. U = set of all connections i for which B(i) < D(i)
    #    N = the number of connections in U.
    #    current share = (P - sum(B(i)) for all i) / N
    # 3. For all i in U,
    #      if D(i) < B(i) + current share then B(i) = D(i)
    #      otherwise, increase B(i) by the current share
    # 4. Repeat from step 2 until U is empty, or the current share
    # falls to a certain threshold (which may be zero).
    #
    # In our implementation, each link has a share for each priority
    # level, typically four values n:m:o:p.  We sum these up over all
    # links for which there are transfers at that priority level into
    # Sn:Sm:So:Sp.  To balance the distribution of shares fairly over
    # time, both over links and priorities, we use as dead weight
    # momentum the transfer history from the last hour.  In other
    # words, our pool size P is ((# of transfers in last hour) +
    # $nxfers), and initialise B to the transfers of last hour.
    #
    # We run the fair share algorithm to determine the fraction of
    # $nxfers each priority level receives on this round, then
    # distribute each priority share to the links according to
    # their relative fraction n:Sn.
    #
    # The variables below have the following meanings:
    #  @files:      files ready for transfer
    #  @levels:     the priority levels we have transfers for
    #  %queues:     @files divided into per-priority, per-link queues
    #  %shares:     per-priority sum of link shares
    #  %old:        usage of bandwidth share in last hour
    #  %desired:    the transfers desired by priority level "i" (D(i))
    #  %allocated:  %old + transfers allocated to priority level "i" (B(i))
    #  %issue:      transfers to issue by priority level (%allocated - %old)
    #  "P"           $oldbias + $nxfers

    my (@files, @levels, %queues, %shares);
    my (%old, %desired, %allocated, %issue);

    # Divide files into by priority, then by source node (link).
    @files = grep($$_{TO_NODE} eq $to && $$_{TO_STATE} <= 1,
		  values %{$$self{FILES}});
    push(@{$queues{$$_{PRIORITY}}{$$_{FROM_NODE}}}, $_) for @files;
    @levels = sort { $b <=> $a } keys %queues;

    # Compute total share allocation for links with transfers.
    foreach my $prio (@levels)
    {
	$shares{$prio} = $desired{$prio} = $issue{$prio} = 0;

	foreach my $from (keys %{$queues{$prio}})
	{
	    $shares{$prio} += $$self{LINKS}{$to}{$from}{SHARES}{$prio} || 1;
	    $old{$prio} += $$self{LINKS}{$to}{$from}{XFERS};
	    $desired{$prio} += scalar @{$queues{$prio}{$from}};
	}

	$desired{$prio} += $old{$prio};
	$allocated{$prio} = $old{$prio};
    }

    # Run the fair share algorithm.
    my $oldbias = sum(0, values %old);
    while (1)
    {
	&logmsg ("nxfers=$nxfers"
		 . join(" ", map { ("(level=$_"
				    . " desired=$desired{$_}"
				    . " allocated=$allocated{$_})") }
			@levels))
	    if $$self{VERBOSE};

	# Select priorities which haven't reached their quota yet.
	my @unallocated = grep($allocated{$_} < $desired{$_}, @levels);
	last if ! @unallocated;

	# Calculate current weighted share.  If too little left, give up.
	my $n = sum(map { $shares{$_} } @unallocated);
	my $share = ($oldbias + $nxfers - sum(0, values %allocated)) / $n;
	last if $share < 0.01;

	# Allocate the share to each priority level according to
	# the relative weight of the level.
	foreach (@levels)
	{
	    my $pshare = $share * $shares{$_};
	    if ($desired{$_} < $allocated{$_} + $pshare)
	    {
		my $issue = $desired{$_} - $allocated{$_};
		$issue{$_} += $issue;
		$allocated{$_} += $issue;
	    }
	    else
	    {
		$issue{$_} += $pshare;
		$allocated{$_} += $pshare;
	    }
	}
    }

    &logmsg ("nxfers=$nxfers"
	     . join(" ", map { "(level=$_ issue=$issue{$_})" } @levels))
	if $$self{VERBOSE};

    # Now divide the priority slots by participating link, from the
    # lowest priority to the highest.  Each priority level takes its
    # integer share of the available transfers; leftovers are given
    # to the highest remaining priority level.
    my %xfers;
    while (@levels)
    {
	# Share of this priority level from the total
	my $prio = shift(@levels);
	my $pshare = (@levels ? int($issue{$prio}) : $nxfers);
	$nxfers -= $pshare;
	next if ! $pshare;

	&logmsg ("priority $prio: issuing $pshare/$nxfers transfers to $to")
	    if $$self{VERBOSE};

	# Same division as we just did with priority shares, but
	# dividing priority-level share to links in proportion to
	# their relative weights.
	my @links = keys %{$queues{$prio}};
	while (@links)
	{
	    # Share of this link from the priority level
	    my $from = shift(@links);
	    my $factor = (($$self{LINKS}{$to}{$from}{SHARES}{$prio} || 1) / $shares{$prio});
	    my $lshare = (@links ? int($factor*$pshare) : $pshare);
	    $pshare -= $lshare;
	    next if ! $lshare;

	    &logmsg ("priority $prio: issuing $lshare/$pshare/$nxfers"
		     . " transfers to $to from $from")
		if $$self{VERBOSE};

	    # Rank the transfers, then pick $lshare first ones.
	    my @new = sort { (int($$a{AGE}/3600) <=> int($$b{AGE}/3600)
			      || $$a{WEIGHT} <=> $$b{WEIGHT}
			      || $$a{INBLOCK} cmp $$b{INBLOCK}
			      || $$a{LOGICAL_NAME} cmp $$b{LOGICAL_NAME}) }
		@{$queues{$prio}{$from}};
	    push(@{$xfers{$from}}, splice(@new, 0, $lshare));
	}
    }

    # Finally, issue the transfers
    $self->startTransfer ($dbh, map { @$_ } values %xfers);
    $$self{BACKEND}->transfer(@$_) for values %xfers;
}

# Mark the files in transfer in the database.
sub startTransfer
{
    my ($self, $dbh, @files) = @_;
    my $now = &mytimeofday();
    foreach my $file (@files)
    {
	&dbexec ($dbh, qq{
	    update t_xfer_state
	    set to_state = 2, time_xfer_start = :now
	    where fileid = :fileid and to_node = :node},
	    ":now" => $now, ":fileid" => $$file{FILEID},
	    ":node" => $$file{TO_NODE_ID});
	$$file{TIME_START} = $now;
    }
    $dbh->commit();
    $$_{TO_STATE} = 2 for @files;
}

# Record completion for the transfer.  Mark files either transferred
# or sends them to cool-off failure state.
sub completeTransfer
{
    my ($self, @files) = @_;
    my $dbh = &connectToDatabase ($self, 0);

    # Have a connection to the database, update status
    my $now = &mytimeofday ();
    foreach my $file (@files)
    {
	if ($$file{FAILURE})
	{
	    $$file{TO_STATE} = 100;
	    &alert ("transfer failed: $$file{FAILURE};"
		    . " to=$$file{TO_NODE}"
		    . " from=$$file{FROM_NODE}"
		    . " fileid=$$file{FILEID}"
		    . " lfn=$$file{LOGICAL_NAME}"
		    . " from_pfn=$$file{FROM_PFN}"
		    . " to_pfn=$$file{TO_PFN}");
	    &dbexec ($dbh, qq{
	        update t_xfer_state
	        set to_state = :state,
		    errors = errors + 1,
		    last_error = :errmsg,
		    time_xfer_start = null,
		    time_request = null,
		    time_error_start = :error_start,
		    time_error_end = :error_end,
		    time_error_total = nvl(time_error_total,0)
				       + :error_end - :error_start
	        where fileid = :fileid and to_node = :node},
	        ":state" => $$file{TO_STATE},
		":errmsg" => (length($$file{FAILURE}) <= 4000 ? $$file{FAILURE}
			      : (substr($$file{FAILURE}, 0, 3975) . "... (rest truncated)")),
	        ":error_start" => $now,
	        ":error_end" => $now + 60*(30 + rand(30)),
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID});
	}
	else
	{
	    $$file{TO_STATE} = 3;
	    &dbexec ($dbh, qq{
	        update t_xfer_state
	        set to_state = :state, time_xfer_end = :now
	        where fileid = :fileid and to_node = :node},
	        ":now" => $now,
	        ":state" => $$file{TO_STATE},
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID});

	    &dbexec ($dbh, qq{
                insert into t_xfer_replica
	        (id, fileid, node, state, time_create, time_state)
	        values (seq_xfer_replica.nextval, :fileid, :node, 1, :now, :now)},
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID},
	        ":now" => $now);

	    &dbexec ($dbh, qq{
	        delete from t_xfer_state
	        where fileid = :fileid and to_node = :node},
	        ":fileid" => $$file{FILEID},
	        ":node" => $$file{TO_NODE_ID});
	}

	# Log transfer stats
	&logmsg ("xstats:"
		 . " to=$$file{TO_NODE}"
		 . " from=$$file{FROM_NODE}"
		 . " fileid=$$file{FILEID}"
		 . " state=$$file{TO_STATE}"
		 . " size=$$file{FILESIZE}"
		 . join("", map { sprintf(" time_%s=%.2f", $$_[0], $$_[2]-$$_[1]) }
			[ 'assigned', $$file{TIME_ASSIGN}, $now ],
			[ 'all', $$file{TIME_START}, $now ],
			@{$$file{TIMING}})
		 . " lfn=$$file{LOGICAL_NAME}"
		 . " from_pfn=$$file{FROM_PFN}"
		 . " to_pfn=$$file{TO_PFN}");

	# Record in my recent history
	push(@{$$self{RECENT}{$$file{TO_NODE}}{$$file{FROM_NODE}}},
	     { TIME => $now,
	       STATE => $$file{TO_STATE},
	       PRIORITY => $$file{PRIORITY},
	       FILESIZE => $$file{FILESIZE} });

	# Remove from work list.  If it errored out, we will reread eventually.
	delete $$self{FILES}{$$file{FILEID}};
    }

    $dbh->commit ();
    &disconnectFromDatabase ($self, $dbh);
}

######################################################################
# Reconnect the agent to the database.  If the database connection
# has been shut, create a new connection.  Update agent status.  Set
# $$self{DBH} to database handle and $$self{NODES_ID} to hash of the
# (node name, id) pairs.
sub reconnect
{
    my ($self) = @_;
    my $now = &mytimeofday();
    
    # Schedule myself to reconnect again later to indicate liveness.
    $self->setTask($now + 3600, \&reconnect);

    # Now connect.
    my ($dbh, @nodes) = &expandNodesAndConnect($self);

    # Indicate to file router which links are "live."
    my ($dest, %dest_args) = &myNodeFilter ($self, "l.to_node");
    my ($src, %src_args) = &otherNodeFilter ($self, "l.from_node");
    my @protos = $$self{BACKEND}->protocols();

    &dbexec($dbh, qq{
	delete from t_xfer_sink l where $dest $src},
	%dest_args, %src_args);
    &dbexec($dbh, qq{
	insert into t_xfer_sink (from_node, to_node, protocols, time_update)
	select l.from_node, l.to_node, :protocols, :now
	from t_xfer_link l where $dest $src},
	":protocols" => "@protos", ":now" => $now, %dest_args, %src_args);

    $$self{DBH_LAST_USE} = $now;
}

# Push status of completed tasks to the database.
sub doSyncPush
{
    my ($self) = @_;
    my $now = &mytimeofday();

    # Update our task status data structures.
    my @done = $self->collectTasks("REPORT_CODE");

    # Get us connected to the database if we aren't yet.
    $self->reconnect() if ! $$self{DBH};

    # Upload status of completed tasks.
    $self->updateTaskStatus($_) for @done;
    $$self{DBH}->commit();

    # Prune recent history.
    $$self{HISTORY} = [ grep($now - $$_{TIME} > 3600, @{$$self{HISTORY}}) ];

    # Remember the time stamp we did this.
    $$self{LAST_SYNC_PUSH} = $now;
    $$self{DBH_LAST_USE} = $now;
}

# Pull more tasks from the database.
sub doSyncPull
{
    my ($self) = @_;

    # Get us connected to the database if we aren't yet.
    $self->reconnect() if ! $$self{DBH};

    # Upload status of completed tasks.
    $self->fetchNewTasks();
    $$self{DBH}->commit();

    # Remember the time stamp we did this.
    my $now = &mytimeofday();
    $$self{LAST_SYNC_PULL} = $now;
    $$self{DBH_LAST_USE} = $now;
}

# Kill ghost transfers in the database every once in a while.  This
# action is executed first when the agent connects to the database,
# then periodically after that.  This action opportunistically uses
# an existing database connection: it never causes a connection to
# be created, but instead just schedules itself to run later.
sub doPurge
{
    my ($self) = @_;
    my $now = &mytimeofday();

    if (! $$self{DBH})
    {
	# No database connection.  Try again later.
	$self->setAction($now + 60, \&doPurge);
    }
    else
    {
	# Ask to run again later.
        $self->setAction($now + 3600, \&doPurge);

	# Remove ghosts in the database and locally.
        $self->purgeLostTransfers();
	# my $tmpdir = $$self{DROPDIR};
	# unlink grep(-f $_ && ! /\.log$/, <$tmpdir/[0-9]*>);
	# unlink grep(-f $_ && (stat(_))[9] < $now-3*86400, <$tmpdir/*.log>);

	# Remember we used the database connection.
	$$self{DBH_LAST_USE} = $now;
    }
}

# Disconnect action.  Check if the agent database connection has been
# idle for some time (at least one minute), and if it looks like the
# agent has enough work for some time, disconnect from the database.
sub doDisconnect
{
    my ($self) = @_;
    my $now = &mytimeofday();
    $$self->setAction($now+300, \&doDisconnect);
    $$self->disconnectFromDatabase($$self{DBH}, 1)
        if (defined $$self{DBH}
	    && $now - $$self{DBH_LAST_USE} > 60
	    && $$self{BACKEND}->isBusy());
}

######################################################################
# Initialisation hook.  Install standard actions.
sub init
{
    my ($self) = @_;
    my $now = &mytimeofday();
    $$self{BACKEND}->init ();
    $self->setAction (0, \&doPurge);
    $self->setAction ($now+300, \&doDisconnect);
}

# Run agent main loop.
sub idle
{
    # FIXME: when backend reaps transfers, it should notify front-end
    # FIXME: when front-end notes task done, it should update link error stats
    # FIXME: think how to handle links that are down, and how to fetch tasks
    #   evenly for links when some links progress slower than others, together
    #   with links being forced down when they have had too many recent errors
    my ($self, @pending) = @_;

    # Run scheduled actions.
    eval { $self->flushActions(); };
    do { chomp ($@); &alert ($@);
	 eval { $$self{DBH}->rollback() } if $$self{DBH}; } if $@;

    # Keep working, resting a bit at a time.  Break out of the loop
    # if we have reached the time the first action should execute.
    my $target = &mytimeofday() + $$self{WAITTIME};
    while (1)
    {
	# Check if we reached our nap time limit.
	my $now = &mytimeofday();
	last if $now >= $target;

	# Pump jobs
        $self->maybeStop ();
	$self->pumpJobs ();
	$$self{BACKEND}->pumpJobs ();

	# Stop if we reached time to execute the first action.
	last if (defined $$self{FIRST_ACTION} && $now >= $$self{FIRST_ACTION});
	select (undef, undef, undef, .1);
    }
}
