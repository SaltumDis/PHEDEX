#!/usr/bin/env perl

##H Produce transfer file names (TURLs) for exported files.
##H
##H This agent generates the export file name for outbound file
##H transfers.  It monitors the database for wanted or staged
##H files that do not yet have a transfer name, and generates
##H one.  The site storage map catalogue is used to generate
##H the export names for the files.
##H
##H Usage:
##H   FilePFNExport
##H      -state DIRECTORY -nodes PATTERN[,PATTERN...]
##H      -db FILE[:SECTION] [-log OUT]
##H      [-ignore NODE[,NODE...]] [-accept NODE[,NODE...]]
##H      -protocols PROTO[,PROTO...] -storagemap PATH
##H
##H -state       agent state directory
##H -nodes       patterns for the node names for which this agent runs
##H -db          database connection configuration parameter file
##H -log         where to redirect logging information
##H -ignore      comma-separated list of nodes to ignore transfers from
##H -accept      comma-separated list of nodes to accept transfers from
##H -protocols   comma-separated list of protocols to accept
##H -storagemap  storage mapping catalogue

BEGIN {
  use strict; use warnings; $^W=1;
  our $me = $0; $me =~ s|.*/||;
  our $home = $0; $home =~ s|/[^/]+$||; $home ||= "."; $home .= "/../../Toolkit/Common";
  unshift(@INC, $home);
}

######################################################################
my %args;
use Getopt::Long;
use UtilsHelp;
&GetOptions ("state=s"     => \$args{DROPDIR},
	     "log=s"       => \$args{LOGFILE},
             "db=s"        => \$args{DBCONFIG},
             "nodes=s"     => sub { push(@{$args{NODES}}, split(/,/, $_[1])) },
             "ignore=s"    => sub { push(@{$args{IGNORE_NODES}}, split(/,/, $_[1])) },
             "accept=s"    => sub { push(@{$args{ACCEPT_NODES}}, split(/,/, $_[1])) },
             "protocols=s" => sub { push(@{$args{PROTOCOLS}}, split(/,/, $_[1])) },
	     "storagemap=s"=> \$args{STORAGEMAP},
	     "help|h"      => sub { &usage() });


if (@ARGV || !$args{DROPDIR} || !$args{NODES} || !$args{DBCONFIG}
    || !$args{STORAGEMAP} || !$args{PROTOCOLS})
{
    die "Insufficient parameters, use -h for help.\n";
}

(new FilePFNExport (%args))->process();

######################################################################
# Routines specific to this agent.
package FilePFNExport; use strict; use warnings; use base 'UtilsAgent';
use UtilsCommand;
use UtilsCatalogue;
use UtilsLogging;
use UtilsTiming;
use UtilsDB;

sub new
{
    my $proto = shift;
    my $class = ref($proto) || $proto;
    my $self = $class->SUPER::new(@_);
    my %params = (DBCONFIG => undef,		# Database configuration file
	  	  MYNODE => undef,		# My TMDB node name
	  	  NODES => [],			# Patterns for nodes to run for
	  	  IGNORE_NODES => [],		# TMDB nodes to ignore
	  	  ACCEPT_NODES => [],		# TMDB nodes to accept
		  WAITTIME => 60 + rand(10),	# Agent activity cycle
	  	  STORAGEMAP => undef,		# Storage path mapping rules
		  LAST_LIVE => 0,		# Last time we indicated liveness
	  	  LAST_UPDATE => -1, 		# Timestamp of file catalogue file
		  PROTOCOLS => undef);		# Protocols to accept
    my %args = (@_);
    map { $$self{$_} = $args{$_} || $params{$_} } keys %params;
    bless $self, $class;
    return $self;
}

######################################################################
sub checkCatalogueChange
{
    my ($self, $exists) = @_;
    my $now = &mytimeofday ();

    # If we don't find a catalogue, warn.
    &warn ("trivial catalogue $$self{STORAGEMAP} has vanished") if (!$exists);

    # Unchanged if this is no different from our last check
    my $stamp = (stat(_))[9] if $exists;
    return 0 if (defined $stamp && $$self{LAST_UPDATE} == $stamp);

    # By default changed
    $$self{LAST_UPDATE} = $stamp if defined $stamp;
    return 1;
}

sub checkLivenessUpdate
{
    my ($self) = @_;
    my $now = &mytimeofday ();
    return 0 if $now - $$self{LAST_LIVE} > 5400;
    $$self{LAST_LIVE} = $now;
    return 1;
}

sub idle
{
    my ($self, @pending) = @_;
    my $dbh = undef;
    
    eval
    {
	# Check whether we need to update catalogue, or just liveness
	# info.  We update liveness once every hour and a half or so,
	# and catalogue whenever it changes.  The agent cycle is quite
	# small so we react to catalogue changes quickly. If we don't
        # find a local catalogue file, delete it from TMDB.

	my $valid = -e $$self{STORAGEMAP};
	my $liveness = $self->checkLivenessUpdate ();
	my $changed  = $self->checkCatalogueChange ($valid);
	
	if ($changed || $liveness)
	{
	    my $now = &mytimeofday ();
	    my ($dbh, @nodes) = &expandNodesAndConnect ($self);
	    my ($filter, %filter_args) = &otherNodeFilter ($self, "xs.from_node");

	    # Statement to upload rules.
	    my $stmt = &dbprep ($dbh, qq{
		insert into t_xfer_catalogue
		(node, rule_index, rule_type, protocol, chain,
		 destination_match, path_match, result_expr)
		values (:node, :rule_index, :type, :protocol, :chain,
			:destination, :path, :result)});

	    foreach my $node (@nodes)
	    {
		# Upload new catalogue if it changed and
                # we have a local storage catalogue.
		if ($changed)
		{
		    # Delete old catalogue for this node.
		    &dbexec($dbh, qq{
			delete from t_xfer_catalogue where node = :node},
			":node" => $$self{NODES_ID}{$node});

		    # Upload current catalogue rules.
		    my $index = 0;
		    
		    foreach my $kind (qw(lfn-to-pfn pfn-to-lfn))
		    {
			next if !$valid;
			my $rules = {};
			# Protect against corrupted storage catalogues
			eval
			{
			    $rules = &storageRules($$self{STORAGEMAP}, $kind);
			};
			do { chomp ($@); $valid = 0;
			     &alert ("error parsing storagemap $$self{STORAGEMAP}: $@");
			 } if $@;

			while (my ($proto, $ruleset) = each %$rules)
			{
			    foreach my $rule (@$ruleset)
			    {
				&dbbindexec($stmt,
					    ":node" => $$self{NODES_ID}{$node},
					    ":rule_index" => $index++,
					    ":type" => $kind,
					    ":protocol" => $proto,
					    ":chain" => $$rule{'chain'},
					    ":destination" => $$rule{'destination-match'},
					    ":path" => $$rule{'path-match'},
					    ":result" => $$rule{'result'});
			    }
			}
		    }
		}

		# Remove source status on links we manage.
		&dbexec($dbh, qq{
		    delete from t_xfer_source xs where xs.to_node = :me $filter},
		    ":me" => $$self{NODES_ID}{$node}, %filter_args);

		# Mark current set of managed links as live.
		&dbexec ($dbh, qq{
		    insert into t_xfer_source (from_node, to_node, protocols, time_update)
		    select xs.from_node, xs.to_node, :protocols, :now from t_adm_link xs
		    where xs.from_node = :me $filter},
		    ":me" => $$self{NODES_ID}{$node},
		    ":protocols" => "@{$$self{PROTOCOLS}}",
		    ":now" => &mytimeofday(),
		    %filter_args) if $valid;
	    }

	    $dbh->commit();
	}

	if ($changed && $valid)
	{
	    &logmsg ("trivial file catalogue rules published");
	}
	elsif ($changed && !$valid)
	{
	    &logmsg ("trivial file catalogue rules removed");
	}
	elsif ($liveness)
	{
	    &logmsg ("refreshed export liveness (no local changes to publish)");
	}
    };

    do { chomp ($@); &alert ("database error: $@");
	 eval { $dbh->rollback() } if $dbh; } if $@;
    
    # Disconnect from the database
    &disconnectFromDatabase ($self, $dbh) if $dbh;
    
    # Have a little nap
    $self->nap ($$self{WAITTIME});
}
