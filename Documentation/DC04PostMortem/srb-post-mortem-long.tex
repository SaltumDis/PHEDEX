\documentclass{cmspaper}
\def\RCS$#1: #2 ${\expandafter\def\csname RCS#1\endcsname{#2}}
\RCS$Revision: 1.0 $
\RCS$Date: 2004/07/07 15:58:00 $

\usepackage{graphicx}
\DeclareGraphicsExtensions{.eps,.pdf,.png,.mps}

\begin{document}
\begin{titlepage}
  %\whitepaper 
  \internalnote{2004/XXX} % \cmsnote{2005/000} % \conferencereport{2005/000}
  \date{Revision \RCSRevision, \RCSDate}
  \title{Experience with SRB from the 2004 CMS data challenge}

  \begin{Authlist}
  \end{Authlist}

  %\Instfoot{cern}{CERN, Geneva, Switzerland}
  %\Anotfoot{a}{On leave from prison}
  %\collaboration{CMS collaboration}

  \begin{abstract}
	This paper discusses the use of SRB (Storage Resource Broker) during the 2004 CMS
	data challenge. It highlights the problems experienced and details lessons that 
	have been learnt which apply to more general cases.
  \end{abstract} 

  %\conference{Presented at {\it Physics Rumours}, Coconut Island, April 1, 2005}
  %\submitted{Submitted to {\it Physics Rumours}}
  \note{Preliminary DRAFT version}
\end{titlepage}

\setcounter{page}{2}

\section{Introduction}
During March and April 2004 CMS carried out the first of a series of large scale data challenges (DC04), 
essentially a "test beam" of the computing infrastructure. The aim of DC04 was to distribute data 
to six Tier 1 sites at a rate comparable to 25\% of the start up luminosity of the LHC.
Of the six Tier 1 sites three (RAL, IN2P3 and FZK) used SDSC's SRB software to manage transfer and 
archiving of the data.
((Diagram of the SRB chain))

\subsection{SRB}

The SRB system is made of three components; a central metadata catalogue (the MCat), a group of 
storage resources running the SRB server software and client tools used for transferring, 
modifying and listing data.

The MCat is used to store authentication information, resource details, file location and 
metadata, and user information. All resources must be registerred in the MCat, which makes 
combining two or more resources into one logical resource possible. For DC04 the MCat database 
was hosted at RAL on a Oracle 9i RAC Enterprise Edition Database (v 9.2.0.5), an identical 
setup at Daresbury laboratory was available for failover situations. The MCat server software 
ran at RAL on a dual Intel P3 (1.4GHz) machine running Red Hat 7.3. These machines had been used
throughout 2003 for the Pre-Challenge Production.

Within SRB version 2 the MCat, or metadata catalogue, represents a single point of failure. All 
user authentications, replica and metadata lookups are undertaken using this single service 
(it should be noted that this is not true in version 3 of SRB).

Lightweight server software is installed on machines providing access to storage resources 
(disk, tape or a combination of both). Data is stored in a unique path so one server can host 
many replicas of the same file. As well as storing the data these servers act as portals to 
client software to SRB space. 

In total 11Tb of disk space was managed via SRB. Of particular note is the resources used at 
CERN to form the export buffer. This was made up of 4 machines each hosting 10 RAID disks. These
disks were combined into a single logical resource via the MCat, making transfer to and from the 
EB considerably easier.

\subsection{Agents}
All transfers were controlled by an "Agent" communicating to other agents in the system via a central
database ((ref Tims paper)). These agents replicated files from Castor MSS at CERN (the T0) through a 
series of intermediate disk buffers and onto MSS at the appropriate T1. This happened entirely within 
SRB at RAL, FZK had no SRB interface to their tape store and so the final step was done using dCache,
IN2P3 preferrerd the reliability of using the native HPSS commands combined with an Sregister
to manage their MSS resources.

\subsection{GMCat}
To maintain consistency between the LCG and SRB realms the GUID (Grid Unique Identifier, used 
by the LCG software to identify replicas of files) for files placed onto the SRB Export Buffer 
was attached as SRB metadata. Further replications of this file within SRB maintained this
piece of metadata so successive replicas could be identified.

In parrallel to the metadata inserts, a service named GMCat periodically updated the 
LCG-RLS (Replica Location Service) with SRB guid:physical file names mappings, thus 
providing a bridge between the two namespaces.

\section{Experience}

\subsection{SRB chain}
Change all of this.....

The performance of the SRB chain was severely hampered by technical
issues: first with availability of the MCat metadata catalogue, hosted
at RAL, and secondly with a small number of bugs in SRB client and
server, and Oracle Linux implementations. Experience with the SRB
chain has highlighted the vital importance of production-quality
service and support at the T1s, especially if they are to be
responsible for mission-critical services during experiment running.

However, SRB had been used successfully by CMS production for a year
beforehand.  In addition, LCG-2 resources were not available at
several T1s. More critically still, for RAL and IN2P3 SRB represented
the only mechanism available for placing files in mass storage with
consistent catalogue information in an automated manner.

Unfortunately during DC04 there came a serious break in service
quality as staff at RAL left and replacements were sought, and then
brought up to speed.  As a result the MCat suffered serious problems
on 22 of 56 days. Problems ranged from slight loss of performance
(lengthy directory query times) to severe loss of performance
(transactions taking so long that transfer agents had no choice but to
continually time out) and unannounced reboots, geographical shifts of
the database service and core dumps.

The SRB chain comprised an export buffer at the T0 and import buffers
at RAL, IN2P3 and Karlsruhe T1s. Files were copied from Castor onto
the export buffer, where they were inserted (Sput) into SRB space. The
files GUID was then added as additional SRB metadata Tier 1 sites then
transferred the files to the relevant import buffers and mass storage
systems, either using Sreplicate or Sget/Sput.

While transfer speeds from T0-T1 were respectable %(fig. \ref{fig:RAL-files})
 the initial entry of the file onto the export buffer was
particularily troublsome.  Much of these problems were caused by the
strategy chosen when writing the agent, based on the knowledge we had
before DC04. Sput is particularily efficient with large files and so
was chosen as the tool to enter data into the export buffer.

During DC04 the file sizes were orders of magnitude smaller than
expected. Had this small filesize been aparent before the data
challenge a different system would have been developed. In retrospect
an export buffer agent based on the drop boxes used in the tier 0
would be a preferred system, separating transfers from the GUID
metadata insertion.  Since the data challenge came to an end a series
of tests have been done in an attempt to replicate and solve the
problems found during the DC, as many components of the CMS grid will
rely on similar backends (eg Oracle databases) it is essential to
understand the problems experienced.

The data challenge highlighted issues of the SRB CLI (return codes were not 
reliable, at times returning 0 despite failure, and some killed transfers continued to 
run in the background) and many problems can be attributed to using the CLI in
a high load production environment. The combination of parrallel processes and
many authentication steps in the CLI tools meant that the load on the MCat was 
sustained at high levels. Indeed on days of particularily high load the MCat
had over 200,000 connection requests.This level of load could have been 
eased by using the bulk commands available in newer versions of SRB or by 
writing our own tools against the API's, however time constraints meant that 
neither were possible.

Even with administration problems, transfers to IN2P3 reached 80 Mbps
over a period of hours, although typically they averaged only 30
Mbps over a whole day. The SRB chain was not able to take part in a
large filesize stress test at the end of DC04; these transfer rates
are due to small file sizes, and thus compare favourably with other
chains. Prior to the data challenge testing indicated that transfer
rates could be sustained at a rate greater than this.

\subsection{Agents}
The simple structure of the agents led to a (neccessarily) rapid development cycle, which 
in turn led to a reduced amount of testing, eg. functionality was tested but little stress 
testing was carried out. Had more stress testing been possible the high load on the MCat 
server caused by the rapid use of CLI tools would have been identified and a workaround 
determined. 

Clearly a despoke tool written against one of the SRB API's would have been considerably more 
efficent than using generic CLI tools, indeed annecdotal evidence from DC04 shows this clearly; 
towards the end of the DC a tool combining the transfer and metadata insert was developed. 
Under initial testing it demostrated a order of magnitude better performance, due to reducing 
the amount of authentication steps involved and parralelising the transfer and metadata 
interaction. However the DC was coming to an end and this was not deployed, nor was it tested 
under significant strain.

Consideration of the number of database interactions suggests that a system of "drop box" agents 
(as used in the T0 system) would be a more appropriate manner to use the SRB CLI. A load of 
files could be staged into one drop box, uploaded as a group (reducing the amount of 
authentication involved), then successful Sput's could be dropped into a second box for the 
metadata insert. Due to that nature of GMCat the metadata updates can be asynchronous to the 
Sputs, GMCat just sees a delete and an insert. This system would also enable a level of failover 
and recover not available in the DC04 system.


\subsection{GMCat}
Details on the performance of GMCat...
Diagram? Graph?

As no RLI was deployed for the data challenge the GMCat service had to synchronise the files 
added and deleted from SRB, instead of responding to individual queries. This meant that the 
query on the MCat had to run over all files in SRB. Response to this query increased linearly 
with the number of files. The updates to the LRC (using the java/wsdl API) at CERN uniformly 
took 0.3s and posed no significant overhead.As the difficulties with the MCat increased GMCat 
was stopped as the system no longer reponded in a useful timescale, a smaller program to add
files added between two times was then run manually to update the RLS.

Clearly this is a sub-optimum solution and integration to the RLS via the RLI would have been 
preferred. Running the large query on the MCat was unlikely to scale beyond the scope of the 
data challenge. MORE HERE

\subsection{Manpower \& Readiness}
Ooh not sure about this one....

\section{Conclusions}
Recent versions of SRB have included new features specifically as a 
result of CMS experience, for which we would like to thank the developers
at SDSC.

It should be noted that SRB is not integrated into the LCG analysis
system, and that while this is possible it may not be desirable to create a "grid of grids" 
without some significant additional desired functionality.  It is likely that CMS
use of SRB will be phased out before DC05.
