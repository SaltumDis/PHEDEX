\documentclass{cmspaper}
\def\RCS$#1: #2 ${\expandafter\def\csname RCS#1\endcsname{#2}}
\RCS$Revision: 1.0 $
\RCS$Date: 2004/07/07 15:58:00 $

\usepackage{graphicx}
\DeclareGraphicsExtensions{.eps,.pdf,.png,.mps}

\begin{document}
\begin{titlepage}
  %\whitepaper 
  \internalnote{2004/XXX} % \cmsnote{2005/000} % \conferencereport{2005/000}
  \date{Revision \RCSRevision, \RCSDate}
  \title{Experience with SRB from the 2004 CMS data challenge}

  \begin{Authlist}
  \end{Authlist}

  %\Instfoot{cern}{CERN, Geneva, Switzerland}
  %\Anotfoot{a}{On leave from prison}
  %\collaboration{CMS collaboration}

  \begin{abstract}
	This paper discusses the use of SRB (Storage Resource Broker) during the 2004 CMS
	data challenge. It highlights the problems experienced and details lessons that 
	have been learnt which apply to more general cases.
  \end{abstract} 

  %\conference{Presented at {\it Physics Rumours}, Coconut Island, April 1, 2005}
  %\submitted{Submitted to {\it Physics Rumours}}
  \note{Preliminary DRAFT version}
\end{titlepage}

\setcounter{page}{2}

\section{Introduction}
During March and April 2004 CMS carried out the first of a series of large scale data challenges (DC04), 
essentially a "test beam" of the computing infrastructure. The aim of DC04 was to distribute data 
to six Tier 1 sites at a rate comparable to 25\% of the start up luminosity of the LHC.
Of the six Tier 1 sites three (RAL, IN2P3 and FZK) used SDSC's SRB software to manage transfer and 
archiving of the data.
((Diagram of the SRB chain))

\subsection{SRB}

The SRB system is made of three components; a central metadata catalogue (the MCat), a group of storage 
resources running the SRB server software and client tools used for transferring, modifying and listing data.

The MCat is used to store authentication information, resource details, file location and metadata, 
and user information. All resources must be registerred in the MCat, which makes combining two or 
more resources into one logical resource possible. For DC04 the MCat was hosted at RAL on a ((details of 
the machine here)).

Lightweight server software is installed on machines providing access to storage resources (disk, tape or 
a combination of both). Data is stored in a unique path so one server can host many replicas of the same file.
As well as storing the data these servers act as portals to client software to SRB space. ((details of the server here))

\subsection{Agents}
All transfers were controlled by an "Agent" communicating to other agents in the system via a central
database ((ref Tims paper)). These agents replicated files from Castor MSS at CERN (the T0) through a 
series of intermediate disk buffers and onto MSS at the appropriate T1. This happened entirely within 
SRB at two T1's, FZK had no SRB interface to their tape store and so the final step was done using dCache 
(though still automated via an agent).

\subsection{GMCat}
To maintain consistency between the LCG and SRB realms the GUID (Grid Unique Identifier, used 
by the LCG software to identify replicas of files) for files placed onto the SRB Export Buffer 
was attached as SRB metadata. Further replications of this file within SRB maintained this
piece of metadata so successive replicas could be identified.

In parrallel to the metadata inserts, a service named GMCat periodically updated the 
LCG-RLS (Replica Location Service) with SRB guid:physical file names mappings, thus 
providing a bridge between the two namespaces.

\section{Experience}

\subsection{SRB chain}
The performance of the SRB chain was severely hampered by technical
issues: first with availability of the MCat metadata catalogue, hosted
at RAL, and secondly with a small number of bugs in SRB client and
server, and Oracle Linux implementations. Experience with the SRB
chain has highlighted the vital importance of production-quality
service and support at the T1s, especially if they are to be
responsible for mission-critical services during experiment running.

Within SRB version 2 the MCat, or metadata catalogue, represents a
single point of failure. All user authentications, replica and
metadata lookups are undertaken using this single service (it should
be noted that this is not true in version 3 of SRB).

However, SRB had been used successfully by CMS production for a year
beforehand.  In addition, LCG-2 resources were not available at
several T1s. More critically still, for RAL and IN2P3 SRB represented
the only mechanism available for placing files in mass storage with
consistent catalogue information in an automated manner.

Unfortunately during DC04 there came a serious break in service
quality as staff at RAL left and replacements were sought, and then
brought up to speed.  As a result the MCat suffered serious problems
on 22 of 56 days. Problems ranged from slight loss of performance
(lengthy directory query times) to severe loss of performance
(transactions taking so long that transfer agents had no choice but to
continually time out) and unannounced reboots, geographical shifts of
the database service and core dumps.

The SRB chain comprised an export buffer at the T0 and import buffers
at RAL, IN2P3 and Karlsruhe T1s. Files were copied from Castor onto
the export buffer, where they were inserted (Sput) into SRB space. The
files GUID was then added as additional SRB metadata Tier 1 sites then
transferred the files to the relevant import buffers and mass storage
systems, either using Sreplicate or Sget/Sput.

While transfer speeds from T0-T1 were respectable %(fig. \ref{fig:RAL-files})
 the initial entry of the file onto the export buffer was
particularily troublsome.  Much of these problems were caused by the
strategy chosen when writing the agent, based on the knowledge we had
before DC04. Sput is particularily efficient with large files and so
was chosen as the tool to enter data into the export buffer.

During DC04 the file sizes were orders of magnitude smaller than
expected. Had this small filesize been aparent before the data
challenge a different system would have been developed. In retrospect
an export buffer agent based on the drop boxes used in the tier 0
would be a preferred system, separating transfers from the GUID
metadata insertion.  Since the data challenge came to an end a series
of tests have been done in an attempt to replicate and solve the
problems found during the DC, as many components of the CMS grid will
rely on similar backends (eg Oracle databases) it is essential to
understand the problems experienced.

The data challenge highlighted issues of the SRB CLI (return codes were not 
reliable, at times returning 0 despite failure, and some killed transfers continued to 
run in the background) and many problems can be attributed to using the CLI in
a high load production environment. The combination of parrallel processes and
many authentication steps in the CLI tools meant that the load on the MCat was 
sustained at high levels. Indeed on days of particularily high load the MCat
had over 200,000 connection requests.This level of load could have been 
eased by using the bulk commands available in newer versions of SRB or by 
writing our own tools against the API's, however time constraints meant that 
neither were possible.

Even with administration problems, transfers to IN2P3 reached 80 Mbps
over a period of hours, although typically they averaged only 30
Mbps over a whole day. The SRB chain was not able to take part in a
large filesize stress test at the end of DC04; these transfer rates
are due to small file sizes, and thus compare favourably with other
chains. Prior to the data challenge testing indicated that transfer
rates could be sustained at a rate greater than this.

\subsection{Agents}
Mistakes made with the agents....

The simple structure of the agents led to a (neccessarily) rapid development cycle, which 
in turn led to a reduced amount of testing, eg. functionality was tested but little stress 
testing was carried out. Had more stress testing been possible the high load on the MCat 
server caused by the rapid use of CLI tools would have been identified and a workaround 
determined. Clearly a despoke tool written against one of the SRB API's would have 
been considerably more efficent than using generic CLI tools, indeed annecdotal evidence from 
DC04 shows this clearly. Towards the end of the DC a tool combining the transfer and metadata
insert was developed. Under initial testing it demostrated a order of magnitude better
performance, due to reducing the amount of authentication step involved and parralelising the 
transfer and metadata interaction. However the DC was coming to an end and this was not deployed.

\subsection{GMCat}
Details on the performance of GMCat...

As the difficulties with the MCat increased GMCat was stopped as the system 
no longer reponded in a useful timescale.

\subsection{Manpower}
Ooh not sure about this one....

\section{Conclusions}
Recent versions of SRB have included new features specifically as a 
result of CMS experience, for which we would like to thank the developers
at SDSC.

It should be noted that SRB is not integrated into the LCG analysis
system, and that while this is possible %\cite{gmcat} 
it may not be desirable to create a "grid of grids" without some
significant additional desired functionality.  It is likely that CMS
use of SRB will be phased out before DC05.