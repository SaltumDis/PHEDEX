\documentclass{cmspaper}
\def\RCS$#1: #2 ${\expandafter\def\csname RCS#1\endcsname{#2}}
\RCS$Revision: 1.3 $
\RCS$Date: 2004/05/27 12:46:29 $

\usepackage{graphicx}

\begin{document}
\begin{titlepage}
  %\whitepaper 
  \internalnote{2004/XXX} % \cmsnote{2005/000} % \conferencereport{2005/000}
  \date{Revision \RCSRevision, \RCSDate}
  \title{Software agents in data and workflow management during DC04- post-mortem}

  \begin{Authlist}
  \end{Authlist}

  %\Instfoot{cern}{CERN, Geneva, Switzerland}
  %\Anotfoot{a}{On leave from prison}
  %\collaboration{CMS collaboration}

  \begin{abstract}
	This paper collates the experience gained in data distribution during
	DC04. It details the structure used, and discusses the overall behaviour
	of each of the components.
  \end{abstract} 

  %\conference{Presented at {\it Physics Rumours}, Coconut Island, April 1, 2005}
  %\submitted{Submitted to {\it Physics Rumours}}
  \note{Preliminary DRAFT version}
\end{titlepage}

\setcounter{page}{2}

\section{Introduction}
During the first half of 2004 CMS undertook a large scale data challenge (DC04) with the goal of sustained data distribution over an extended period, with data streaming from the T0 as if from the CMS detector running at 25\% of startup luminosity (25 Hz, or events per second).

In the preparations for DC04 it became apparent that the Grid tools available did not meet the requirements of the experiment. More specifically, Grid projects had developed useful APIs to meet certain needs- e.g. for point-to-point file transfer using gsiftp- however, the tools were coupled together to form a replica management system that did not meet overall experiment requirements for data transfer during DC04: namely, that transfer should be scheduled and directed in a large-scale manner, at dataset granularity, rather than in a point-to-point opportunistic manner at file granularity.

A significant part of DC04 preparation and running was therefore the design and development of a working directed replica management system for CMS. This paper outlines the experience gained in designing, developing and using the system. It does not examine the apparent mismatch between requirements published and received between the experiment and Grid developers.

CMS developed a system that to meet the following requirements of DC04:
\begin{list}{}{}
\item Data streams from the T0 as if from the CMS detector running at 25Hz.
\item Datasets will be allocated and delivered to certain T1s.
\item The system must manage the propagation fo files to the T1s automatically.
\item The system must be scalable (requiring a minimum of supervision and intervention).
\end{list}

The system designed was based on a structure of semi-autonomous software agents collaborating by sharing information in a global space. It was was rapidly prototyped and put into production during the months of February and March 2004.

The system proved that it was possible to transfer 25 Hz reconstructed data, and to reach sustained aggregate data rates of 30+ MBps. The system was also used to analyse data in "real time", exhibiting a median lag of only 20 minutes between files being ready for distribution and the analysis results being available at a Tier 1.

\section{The data distribution system}
The CMS data distribution system for DC04 was a file transfer management structure, drawing on aspects of blackboard and multi-agent system design \cite{FG96,C03,Setal03}. Persistent, stateful agents were deployed at a number of geographically distributed sites, and handled point to point propagation of files through the system. Communication between agents (typically announcing the presence of files at some point in the system) was limited by design; propagation of information through the agent system required the agents to post information at a central ``blackboard''. This blackboard was implemented in Oracle and named the Transfer Management Database, or TMDB. 

This architecture enabled us to maintain a coherent picture of system state in a central location, making it relatively easy to diagnose and solve problems. It also meant that agent development was characterised by short timescales (the agents could be implemented in a local language of choice), and simplicity. 

Agent development was simple because the exchange of messages was used to define the behaviour of a small range of agents, which could then be implemented in a way that suited local developer groups. By doing this any need for local functionality were easily met by implementing a new agent, in whatever language was suitable. Strictly defined message passing encouraged the localisation of complex functionality within single agents \cite{B03}.

\subsection{Components and data flow}
The complete system comprised a data source (the Reconstruction farm at CERN), a common transfer/state database (TMDB) and a hierarchy of agents that managed anddistributed the data. Other packages- like a web front end to manage the TMDB and monitor the status of the disk buffer from which files are distributed- were also required. 

The distribution system drew files from Castor stage disk and streamed them to a number of T1s, where they were placed in mass storage. The transfer of data through the system was handled by a series of agents of limited  responsibility.

Files were placed in the Castor stage area by Reconstruction jobs. To trigger  distribution an XML catalogue fragment and checksum file were placed in a dropbox for an agent to find. These agents then published file information in the RLS and the TMDB.

A Configuration agent at the T0 allocated files to specific T1s, acting as a simple replica manager.

Export Buffer agents (dedicated to a number of T1s, all using the same distribution tool) scanned the TMDB for newly allocated files. Generically, they drew these files from the Castor stage area and placed them on an Export Buffer dedicated to a specific distribution tool.

At each T1 an agent scanned the TMDB looking for files that were newly available on the Export Buffer. It transferred these files to the T1. Further agents ensured that the files were placed in mass storage.

As files appeared at T1s, other agents submitted the files to analysis and published the results. In some cases, further agents replicated files to T2 sites for analysis.

\begin{figure}[tbp]
\centering
\includegraphics[angle = 90]{T0-flow.eps} 
\label{fig:flow1}
\caption{Making data available to distribution during DC04.}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[angle = 90]{T0-flow-2.eps} 
\label{fig:flow2}
\caption{Insertion of data into distribution chains during DC04.}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[angle = 90]{T1-flow.eps} 
\label{fig:flow3}
\caption{Transfer of files to T1 mass storage during DC04}
\end{figure}

\subsection{DC04 distribution infrastructure}
During DC04 data was distributed via three "chains", corresponding to three flavours of distribution middleware. These middleware tools were LCG, SRM + dCache and SRB (fig. \ref{fig:chains}).

\begin{figure}[tbp]
\centering
\includegraphics[angle = 90]{chains.eps} 
\label{fig:chains}
\caption{Overall view of the distribution chains deployed during DC04.}
\end{figure}

The LCG chain variously used the Replica Manager Java command line tools and Globus globus-url-copy to make transfers using gsiFTP, and the Local Replica Catalogue C++ API to query the RLS and make updates. The chain deployed various flavours of LCG and European DataGrid (EDG) Storage Elements (SEs) as file buffers.

All chains used the LCG (LHC Computing Grid) Replica Location Service (RLS) as a repository of filename and metadata information. For future reference, the RLS deployed during DC04 comprised a single Local Replica Catalogue and Replica Metadata Catalogue deployed at CERN. The distribution chains used LCG tools- either LRC tools or POOL File Catalog tools- to query and make entries in the catalogue.

SRM is an acronym denoting the Storage Resource Management interface, which provides generic access to any storage resource. Superficially SRM provides a defined two step interaction through which a file can be obtained from any storage medium for which the user has a Storage URL (SURL), which contains the hostname of the medium. The SRM chain deployed disk buffers coupled by dCache, a disk pool and tape storage management software as file buffers, and used gsiFTP to transfer files between them.

SRB is an acronym for the Storage Resource Broker, another middleware that presents a uniform generic interface to files. SRB differs from the SRM and LCG in that it presents a uniform global interface to the user; that is, a user is typically unaware of where a file is within SRB filespace. The SRB filespace and file metadata is stored within a metadata catalogue named the MCat. During DC04 the SRB chain deployed SRB vaults as file buffers, and used Sput and Sreplicate commands to transfer files. The SRB and LCG file catalogues were syncronised by a standalone application named GMCat.

More detailed descriptions of each chain, and experience with it during DC04, can be found below.

\section{Experience}
Development was rapid, and mostly straightforward. Several versions of export buffer, T1 transfer and mass storage agents were implemented within several weeks of the first examples being made available. This rapid development was due in part to the extremely limited responsibility of each agent, and the easy breakdown of agent workflow into chunks of functionality like ``publish state in database'', ``check for new guids'' (defined by TMDB interactions).

Agents were implemented in a variety of languages: at first, a scripting approach was taken, and agents were implemented in Perl and Bash scripts. The agents relied on command line tools- often provided only as examples by developers- to access the Oracle TMDB and POOL File Catalogues, and to make file transfers. Experience during DC04 modified this approach, and agents implemented in lower level languages (C++) appeared. 

Further, command line tools proved to be too generic to support intensive bulk use, meaning that agent developers moved naturally to using the underlying component APIs to effectively write their own focused distribution tools.

Each chain of agents was coded and trivially tested within six weeks, and developed and tested over the whole of DC04 (eight weeks). 

It was found that the collaborative approach- sharing information between agents on a blackboard- enabled the developeent of new agents with functionality that was not foreseen. By way of example, a cleaning agent was implemented at the T0 to manage Castor stage space. It required a somewhat more sophisticated strategy than just checking whether files were safe at MSS before migrating them to tape. Instead, using information already in the TMDB, it was able to rank files and thereby efficiently prioritise them for migration.

This sort of evolution was not without difficulty- the original design of the TMDB meant that adding new information schema to the database was somewhat costly. It is however something that can easily be overcome in future versions of the database. (see below).

Coordination and management of people and agents proved highly inefficient, and it became clear that global mechanisms for suspending, restarting and monitoring agents needed to be deployed in future versions. During DC04, almost all system shutdown/restarts required an email sent to each operator, with the action only taken when all replies were received. In a similar vein, the presentation of global sysetm state must be improved: during DC04 a limited amount of all information was available via a simple web page. In the future some interpretive interface must be created to highlight current problems and warnings, and reveal more sophisticated information (file transfer rates, etc) in a more immediately intuitive and useful way.

\subsection{Overall behaviour}
One might use the lifetime of files in the system as a useful metric for measuring system performance, defining the lifetime of files as the period between appearance and migration to Castor at CERN. However, the mechanism for migrating to Castor was only implemented late in DC04, and migration was hampered by technical issues with the SRB chain which meant that a large backlog of files that had not been made safe at T1s appeared. Studying the comparison of lifetime with time of appearance, it is clear that files that appeared early remained in distribution for a long time, while files that appeared late had a correspondingly shorter lifetime; the information gained from such a study is negligible.

It is more meaningful to ask how long it was before files were available for analysis at a T1: figures for this vary from distribution chain to distribution chain, with the LCG chain to CNAF taking between 10 minutes and a day and a half, and the LCG chain to PIC taking only 20 minutes consistently (fig. \ref{fig:PIC-RTA}). More detailed information on real time analysis during DC04 is available ELSEWHERE (TODO: fix this).

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{PIC-RTA.eps}
\label{fig:PIC-RTA}
\caption{During the last four days of DC04 PIC were able to implement a real time analysis system based on CNAF's system. Turnaround time for analysis at  PIC was much shorter and more consistent than CNAF at a median of 20 minutes.}
\end{figure} 

Transfer rates varied greatly, and are treated in more detail below. Generally however, transfer rates at the start of DC04 were low- individual transfers reached maybe a MBps, with aggregate rates pushing 10MBps. These low transfer rates were due to the small size of files being pushed through the system. This is an example of poor initial coupling between the data source and the distribution system. Here the coupling is described as poor because it is easy for reconstruction to produce small files, but difficult for distribution to distribute them due to the overheads associated with each file, independent in most cases of size. These overheads include catalogue registration (which takes the same time despite file size); startup times for Java Virtual Machines; and inefficient ramping of TCP/IP window sizes for small file sizes.

During DC04 this poor coupling was mostly dealt with by working around it. For example, poor transfer speeds were remedied by making as many parallel transfers as possible; file and metadata catalogue accesses were limited as far as possible; and the use of Java command line tools was replaced by the use of C++ APIs.

At the very end of DC04 this poor coupling was improved to some extent by the implementation of a "merging" step (detailed below) which added files to one of a series of queues which expired and were written after either a certain aggregate size or time limit was reached. File sizes were typically ~500k median at the start of DC04.

(TODO: table of filesizes).

Agent uptime was not monitored directly, and is difficult to estimate. Some agent types would not publish themselves as "available" if they could not complete any file transfers- as happened for example several times when the load on Castor was high. Typically the agents implemented as Bash scripts suffered this problem- the C agents were implemented in a slightly different way.  Interpretation of information about agent uptime is therefore difficult to interpret from DC04 logs. This is an area for improvement in future versions of the distribution system.

Due to bugs in the reconstruction code large numbers of files were deemed of limited use for analysis. It was decided that these files should be marked as bad, and effectively removed from distribution. To do this a new state- or rather set of states- indexed 90 and above was introduced. As agents weren't looking for files in those states, the files were never transferred. The system was therefore flexible enough to meet new requirements during development.

\subsection{The TMDB}
In contrast to other databases used, the TMDB proved to be remarkably stable, only experiencing a critical problem when log entries exceeded the initial tablespace allocated.

Development of the TMDB continued throughout DC04 as new use cases were discovered. The management of changes to the databases provided useful experience for the future. Most changes were easily accomodated thanks to a carefully designed database schema (for example, the addition of new agents was handled gracefully) (fig. \ref{fig:schema}).

\begin{figure}[tbp]
\centering
\includegraphics{v1_tmdb.eps}
\label{fig:schema}
\caption{The Transfer Management Database schema deployed during DC04.}
\end{figure} 

Some changes- notably the addition of some forms of metadata to the list of files for transfer- caused significant slow down in access. During DC04 it became apparent that some specific metadata- an entry denoting the filegroup, and some indication of cleaning priority- was required. Adding this new metadata was seen as a "one off", and was implemented as new fields in an existing table. At that stage, however, the TMDB had several hundred thousand files registered in it. The addition of new fields and entries to the database took many hours, and impaired access for the agents.

Future versions of the TMDB will require that metadata be stored in a separate table referencing filefortransfer.guid, thus avoiding this problem.

The discussion of which metadata to add to the database- from LFNs through replica to application metadata- continued throughout DC04. Addition of metadata to the TMDB was vigorously opposed to avoid it being seen as a permanent repository of metadata (in principle all information in the TMDB is transient). It became apparent that the ability to add metadata to replica entries- even changing the schema of metadata, or having a varying schema for each replica- was essential, especially during development.

\subsection{The DC04 Replica Location System (RLS)}
A reduced set of components from the LCG-2 RLS was deployed as a global service for lfn:guid:pfn and metadata lookups during DC04: a Local Replica Catalogue (LRC) and Replica Metadata Catalogue (RMC) were deployed at CERN. These catalogues were accessed in a variety of ways- for example, using the LRC C++ API for direct access to the LRC, or through POOL File Catalogue tools (the LRC-RMC combination can be represented as a single POOL File Catalogue).

Note that some sites had separate MySQL POOL catalogues that partially replciated information in the RLS. These catalogues were not fully syncronised, although information was published to them from the RLS.

The RLS proved to be a critical component during DC04, even if only judged on the number of processes accessing it (table \ref{table:rls}).

\begin{table}
\begin{tabular}[tbp]{|l|l|l|}
\hline User & Use of RLS & Tool
\\ \hline Publishing Agent & Registration on appearance & FCpublish
\\ Configuration Agent & Query the RLS metadata & FClistMeta
\\ Export Buffer Agents & Register new mappings & FCaddReplica
\\ & & C++ LRC API
\\ GMCat & Maintain SRB : LRC filespace mappings & LRC java API
\\ Tier-1 Agents & Register new mappings & FCaddReplica,FCpublish
\\ & Publish sub-RLS into local MySQL POOL catalogue & C++ LRC-API
\\ LCG Analysis jobs & Through Resource Broker &
\\ & Register the private output data & 
\\ Humans & trying to figure what was going on & FC commands
\\ \hline
\end{tabular}
\label{table:rls}
\caption{Summary of the many uses of the RLS during DC04. FC* commands are those provided by POOL.}
\end{table}

The LRC at CERN was found to be a critical bottleneck early in the challenge, and a significant amount of effort was expended in developing it so that performance was brought close to requirements. Even at the end of DC04 however it was still not ebing used generally- POOL XML catalogues were transferred alongside the data, and analysis either used these directly, or accessed the information via a local MySQL POOL catalogue if uploaded upon reacihng the destniation.

In general inserting information into the LRC was slow, and using the RMC was slower. Looking up file information by GUID met performance requirements, but queries, including queries by GUID, took a long time. Throughout DC04 discussions on performance were held with the catalogue developers, and as a result a new optimized version was released just after DC04.

Over one of the earliest busy periods of DC04- when the LRC and RMC began to fail- there were around $10^6$ interactions with LRC Oracle backend, and $3\times10^6$ interactions with the RMC per day. 

In total $~570$k lfns were stored in the RLS during DC04, each with 5-10 pfns and 9 metadata attributes. Inserting information into RLS was fast enough if  the appropriate tools were used. A variety of more appropriate tools were developed during DC04 leading to improvements in performance- for example, registering a new mapping using the LRC C++ API took 0.1-0.2 s per mapping, while using the POOL CLI with GUID took seconds per mapping.

Inserting files with metadata into the RLS was slow in comparison, although approximately usable at around 3 s per file. There were periods, however, where intervention was required on the server and insertion times dramatically reduced (fig. \ref{fig:rls}). Interventions included cleaning of log partitions, switching backend nodes, and optimizing queries.

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{rls.eps}
\label{fig:rls}
\caption{The time taken to register all files in drops as they appeared at the end of Reconstruction jobs shows some variation, with periods of high data production associated with longer registration times (near the middle of the plot). Typically there were 16 files per drop.}
\end{figure} 

Querying information from RLS was in general very slow. Looking up file information by GUID seemed sufficiently fast, but bulk queries by GUID took seconds per file. Queries on metadata were too slow- for instance, querying for all files of a specific "owner" took hours. In contrast, it took two minutes to dump the entire catalogue to file after gaining access to the Oracle backend.

The problem was also not POOL-related: for comparison, a similar dumps from POOL MySQL or XML catalogues were a minimum factor 10 faster than a dump from a POOL RLS catalogue. It is entirely possible that a POOL MySQL catalogue could have met the demands of DC04: however, some transfers were undertaken using LCG tools that required an LCG RLS.

In summary, several workarounds or solutions were provided to speed up the access to RLS during DC04: use of the Java Replica Manager CLI was replaced by use of the C++ API; POOL developed improvements and workarounds; and CERN IT were able to index some meta data attributes in RLS (by default, none of the RLS tables are indexed). There were a number of requirements that were not met at the start of DC04: transactional access; a small voerhead compared to to direct RDBMS catalogues; and fast queries.

Each of these issues needs to be met if the RLS is to be used in the future as the file catalogue of choice. Wheher it remains the ideal solution to storing replica metadata also needs to be examined.

\subsection{Dropbox agents}
+ Performed well, typically 0.01s per drop (O(10) files).

+ Found needed bulk access to POOL RLS...

+ Numbers from Lassi.

\subsection{SRM chain}
The SRM distribution chain comprised an SRM Export Buffer at CERN providing access to a dCache disk pool, and an SRM import buffer at FNAL providing access to Enstore, again via dCache. 

At the T0, the SRM export buffer handled staging of files from Castor to dCache disk pool, where they were pinned until transferred.

The transfer agent copied files from the export buffer to the T1 import buffer by initiating a third aprty SRM transaction to receive a TURL from the export buffer, then using gridFTP to make the transfer.

The use of storage media that presents a uniform SRM interface to the outside world is a model for the way components of the distribution system should behave in future versions. Use of SRM throughout would enable us to create generic transfer agents, and make implementing them simpler, as operations like file staging, etc would be handled automatically \footnote{Although whether the SRM method for handling staging is best for us is not clear. If we only use SRM to handle this, then ideally SRM gives the transfer agents a way to trigger a stage and return simply and gracefully when necessary.}.

Initially problems were seen with repeated authentications: the agents were developed so that multiple streams, with multiple files in each stream, could be used to transfer files, reducing the number of authentications required. Transfer rates of over 10 MBps could be sustained for long periods (fig. \ref{fig:FNAL-network}).

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{FNAL-network.eps}
\label{fig:FNAL-network}
\caption{The SRM chain was able to sustain transfer rates of more than 10 MBps into FNAL for long periods.}
\end{figure} 

In contrast to other techniques, the use of SRM eased agent development with features like automatic directory creation, file migration and staging, and automatic failure recovery. Similar features were successfully implemented as parts of the distribution system in, for example, the LCG chain.

However, during DC04 it proved difficult to install monitoring technology, which meant that hardware failures had to be identified by a human operator, and restarts handled manually. In common with other distribution chains the SRM chain had difficulties with the large number of small files, which necessitated an increase ni the number of tapes available and the deployment of a larger namespace service.

FNAL also deployed a MySQL POOL catalogue to enable access to the transferred data in the US; they found its performance more than adequate for the task. Initially the publishing of entries from the RLS to the FNAL POOL catalogue proved difficult, with the RLS queries taking a long time (see above). Close work between agent developers at CERN and FNAL brought about a 100-fold increase in performance.

All data access at FNAL was attempted through dCache via a ROOT plugin- so that COBRA based applciations could trivially access teh data. Read performance was shown to be "dramatically increased"- however, opening files became a dominant bottleneck, exacerbated by the large number of small files. The software environment consisted of access to applications over AFS at CERN, which proved quite stable.

Access to the transferred data was found to be logistically difficult: as the files ina  dataset were reconstructed at a range of times through DC04, they were stored on a large number of tapes. Making data available for analysis therefore meant a large number of tape stages.

Data was also transferred to UFL and Caltech T2s toward the end of the challenge. UFL was able to use the same software environment as the T1 to analyse data so transferred.

\subsection{SRB chain}
The performance of the SRB chain was severely hampered by technical issues:
first with availability of the MCat metadata catalogue, hosted at RAL, and
secondly with a small number of bugs in SRB client and server, and Oracle
Linux implementations. Experience with the SRB chain has highlighted the vital importance of production-quality service and support at the T1s, especially if they are to be responsible for mission-critical services during experiment running.

Within SRB version 2 the MCat, or metadata catalogue, represents a single point of failure. All user authentications, replica and metadata lookups are undertaken using this single service (it should be noted that this is not true in version 3 of SRB). 

However, SRB had been used successfully by CMS production for a year beforehand. In addition, LCG-2 resources were not available at several T1s. More critically still, for RAL and IN2P3 SRB represented the only mechanism available for placing files in mass storage.

Unfortunately during DC04 there came a serious break in service quality as staff at RAL left and replacements were sought, and then brought up to speed. As a result the MCat suffered serious problems on 22 of 56 days. Problems ranged from slight loss of performance (lengthy directory query times) to severe loss of performance (transactions taking so long that transfer agents had no choice but to continually time out) and unannounced reboots, geographical shifts of the database service and core dumps.

The SRB chain comprised an export buffer at the T0 and import buffers at RAL, IN2P3 and Karlsruhe T1s. Files were copied from Castor onto the export buffer, where they were injected (Sput) into SRB space. An application developed in the UK (GMCat) linked the name spaces of SRB and the other chains by publishing SRB replica information into the LRC at CERN periodically. Performance...

Even with administration problems, transfers to IN2P3 reached 10 MBps over a period of hours, although typically they averaged only 30 Mbps over a whole day. The SRB chain was not able to take part in a large filesize stress test at the end of DC04; these transfer rates are due to small file sizes, and thus compare favourably with other chains.

\subsection{LCG chain}
The LCG components required for distribution were the LRC at CERN, the dedicated information index (bdII), LCG-2 VO tools, a GridICE node at CNAF, Castor SEs at CNAF and PIC and "classic" disk SEs at CERN, CNAF, PIC, Legnaro, CIEMAT and Taiwan.

At the start of DC04, the LCG chain had no Export Buffer from which to export data. A temporary storage element (SE) was deployed at CERN on March 3rd for testing. Ten days later a "classic" (e.g. disk based) LCG SE was deployed, and was joined by two other nodes several days later. This deployment represents intense effort by CERN IT/EIS and was critical in getting the LCG chain moving data.

The SE was managed as a distribution buffer by an EB agent, at first making third party transfers before being hosted by the SE node for reasons of efficiency.

Files from the SE EB were drained down to Castor SE buffers at CNAF and PIC T1s, then replicated to disk-SEs to grant data access for ``fake-analysis''. Throughout DC04 the SE agent and the PIC and CNAF T1 agents were all able to match pace with the generation of files at the T0 (fig. \ref{fig:PIC-FT}).

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{PIC-FT.eps}
\label{fig:PIC-FT}
\caption{Throughout DC04 the number of files apparently at the T0 and on the SE (classed as NEW or IN\_BUFFER) was negligible for PIC and CNAF (the diagram here is for PIC, but is equally applicable to CNAF), suggesting that the SE and T1 agents were able to keep up with the rate of generation of files at the T0. }
\end{figure} 

At the start of DC04 both used the Java RM API to make transfers and catalogue lookups/registrations. This was found to be inefficient. Interestingly, the T1s chose subtly different methods to transfer from the T0 after moving away from the Java API.

CNAF chose to use the Java EDG Replica Manager command line tools to transfer and register files with some very limited use of the LRC C++ API to query the LRC for filenames to start transfers. Use of the RM API meant that their transfers and catalogue registrations were integral operations, and therefore more fully transactional. 

PIC chose to use globus-url-copy and the LRC C++ API to register replicas. In principle recovering from failed transfers was more difficult for PIC, as the transfer and register operations were not integral. This however should be weighed against CNAF's use of the slower Java RM tools- although the use of the Java RM could be supplanted by the use of the EDG C++ API.

Subtle variations in approach aside, transfer rates from the SE EB could be sustained at over 2 MBps, peaking at 16 MBps (fig. \ref{fig:SE-EB-network}), with transfer rates into CNAF and PIC reaching a sustained 30 MBps for hours during a large-filesize stress test at the end of DC04 (fig's. \ref{fig:PIC-stress}, \ref{fig:CNAF-stress}). Internal monitoring at CNAF revealed a bottleneck on site, with an internal link to the SE operating at over 80\% capacity during the final stress test.

\begin{figure}[tbp]
\centering
\includegraphics[width=15cm]{SE-EB-network.eps}
\label{fig:SE-EB-network}
\caption{Transfer rates out of the SE EB could be sustained at over 2 MBps toward the end of DC04, and peaked at 16MBps.}
\end{figure}

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{PIC-stress.eps}
\label{fig:PIC-stress}
\caption{During a large filesize stress test at the end of DC04, files of 600MB and larger were put into distribution. PIC sustained transfer rates of 30 MBps for 6 hours.}
\end{figure} 

\begin{figure}[tbp]
\centering
\includegraphics[width=10cm]{CNAF-stress.eps}
\label{fig:CNAF-stress}
\caption{During a large filesize stress test at the end of DC04 (the period from 0 at the right of the plot), files of 600MB and larger were put into distribution. CNAF sustained transfer rates of 40 MBps for 5 hours.}
\end{figure} 

At CNAF issues with the Castor tape stager were found to be due to the high number of entries in its database (a consequence of the small file size). In general tape performance was slow due to inefficient utilization, meaning a delay in psoting files as being ``safe''. CNAF have since been working on an import buffer and related agents to group files more reasonably for their tape system on arrival.

In contrast PIC experienced no problems with their Castor tape.

PIC and CNAF were also able to distribute data on to T2s (CIEMAT and Legnaro respectively) for analysis. To manage these further transfers, as well as real-time analysis, they effetively created a new set of distribution chains using a MySQL TMDB for collaboration information.

\section{Conclusions and future plans}
Sources of throttling: access to tape/stage, catalogues, tmdb(?), intermediate links (no control over these?), buffer read/write, tape migration...

Gap in middleware: developers have implemented APIs. They have also developed command line tools which are good as examples but suffer from impaired performance when used in a large scale system. From our point of view the amount of "grid itegration" work to is alrger than expected: many expected the middleware developers to develop tools rather than APIs.

A general remark about "more warning and planning so that operational issues could be anticipated".

Data sources and distribution need to be coupled properly- for example, reconstruction produced many small files, whereas distribution requires large files to function efficiently. Impedance mismatches?

Most "big pushes" planned over the weekend! Not the best of ideas? Could it be done any other way?

Management of agents in a global sense, and notification of local supervisors needs to be dramatically improved in future versions of the system, as email does not provide an efficient and reliable mechanism for coordinating agent shutdown and restart, and other global system actions.

IT was shown that it is possible to analyse data as it arrives, with only a 20 minute period elapsing between the file being made available for distribution and it being used in analysis. This turnaround time gives us confidence that future versions will be able to handle this sort of analysis consistently and for sustained periods. The variability in eprformance between deifferent deployments of the real-time analysis agents and structure however shows that there is some exploraty work to be done to determine the best practice for deployment and use.

The distribution system/ agents require easy access to metadata, both dedicated distribution metadata and more experiment specific emtadata. The mechanism for handling this was poorly developed for DC04, and needs to be improved for future versions. However, it is important that people remember that the TMDB is not a permanent repository of metadata, as the information it contains will probably be deleted once the files are no longer in transit.

Logging of low level metrics- file transfer rates, etc- was handled very well by a number of tools- MonaLISA, GridICE, Lemon among them. Logging of higher level distribution component metrics however was not so well developed. Toward the end of DC04 the number fo files in each state was monitored using MonaLISA, but only for a predefined set of states (e.g. the total set of states was not dynamically determined). In the future this monitoring needs to be extended to cover agent uptime, transition of files through states, files handled per agent per second and no doubt others.

Log entries should not be made in the TMDB- instead they should be handled locally by a tool such as netlogger- note that remote access to these logs on demand is still necessary; it should be pssible to query the log repository remotely.

\bibliographystyle{plain}
\bibliography{dc04-post-mortem}

\end{document}