* Forming a production datagrid from simple, collaborative components

** Introduction
*** Context
High Energy Physics (HEP) experiments have historically been large consumers and producers of data. Analysis of data from the most recent experiments will require efficient transfer and management of tens of petabytes of experimental data, and associated complex metadata, a year.

Typically HEP experiments have been able to rely on manpower-intensive mechanisms for distributing data to geographically dispersed collaborators. The new era of LHC computing however necessitates a move to more scalable data distribution and management systems; many experiments have identified Grid computing as a suitable basis for creating such systems.

CMS- one of the four LHC experiments currently being built at CERN- has made important progress toward having a functioning grid to distribute its experimental and simulated data by developing a robust data management layer. It has a simple twofold goal: to manage the prioritized transfer of files from multiple sources to multiple sinks; and to provide answers to questions of transfer latency and speed at a range of granularities. It bridges the gap between "traditional" HEP data distribution- large scale scheduled transfers between major sites- and more recent grid data distribution, which focuses more on optimized replications in response to user demand.

The data management layer- named PhEDEx, for Physics Experiment Data Export- is composed of a series of agents, which in the current version are robust, persistent, stateful processes. These agents share information about replica and transfer state through a blackboard architecture. The blackboard also hosts some higher-level information about network routing, dataset subscriptions and other infrastructural information.

In contrast to some existing grid structures PhEDEx accommodates a not-completely connected distribution network- for example the tiered, hierarchical structure common to many HEP experiments. Existing Grid tools have tended to view the structure as completely connected, and all transfers as simply single file point-to-point. A better approximation to the HEP environment is a somewhat static core infrastructure in which transfers are continuous and directed from a central site through successive sites of decreasing resources. This core infrastructure is intended to store multiple secure tape copies of invaluable experimental data within a certain time of generation, and relies on highly robust and available services. Such services are only available at the experimental centre, and at around 10 large, geographically distributed regional centres.

Around this core infrastructure exists a dynamic infrastructure of sites that associate with regional centres. These smaller sites typically make resources available on something close to a best-effort basis- the extreme example being a physicists laptop. These resources are intended for rapidly changing analysis tasks, typically with much fewer resources (storage and bandwidth). This dynamic environment is more ideally suited to management by a grid that optimizes the storage and availability of data, and is driven by end-user analysis tasks.

The key problem is the seamless blending of very high priority, continuous large-scale data transfers between highly available resources with smaller scale, lower priority transfers between smaller sites for analysis or reanalysis, and therefore make the infrastructure manageable by a small number of people. The data management layer needs to do this in an environment in which even the relatively static, stable core infrastructure is considered unreliable- and must therefore handle failover and retry of transfers to a given destination, or even reallocation of high-priority transfers to other resources based on collaboration policy. The data management layer should provide sufficient functionality to allow a person or process to specify some group of files and a number of destinations, and then have the layer manage the efficient replication of data from wherever it currently resides to those destinations.

By devising a system in this manner, we meet the immediate needs of HEP experiments- which are typically the traditional large-scale directed transfers- by plugging in a simple replica management algorithm that takes subscriptions to datasets and manages transfers as they become possible, without having a manager oversee the entire transfer process. We also make it possible to add to this simple replica management a more sophisticated optimizing replica manager as the system matures.

** Overall use cases for a data management layer in HEP
In HEP there are two broad use cases for a data management layer: "push" and "stream" (or "pull"). Push represents the transfer of invaluable raw experimental data from the detector to tape store at regional centres. In this sense, the collaboration chooses which regional centres store which data, and pushes the data onto them

The streaming use case represents another on-going process instigated by either a regional centre, or a relatively stable smaller site. In this case, the site can subscribe to a given dataset; as files are created- wherever they are created in the infrastructure- they need to be transferred to the subscribing site. Similarly, these sites must make the data they create available for streaming to other sites. The pull use case is a subset of stream, and represents a small-scale one shot operation, in which say a single physicist is interested in some part of a dataset, and wishes to transfer it either to his University or even his laptop. In this case the end user would just connect his laptop to the data management layer, specify the data required, wait for the data management layer to transfer the necessary files and then remove the laptop from the layer.

Note that the names of these use cases are not intended to describe low-level transfer activity. In all cases we implement point to point transfers as third party- although in the majority of cases they are instigated by an agent running near the destination rather than the source.

The simplest requirements placed on the system is that it is required to be manageable at a high level by an individual, and that low level deployment and support take close to zero effort. To meet these requirements we devised an architecture based on quite simple, focused, robust processes that were able to handle failover and retry of transfers automatically, and which could collaborate to solve the problem of transferring data over multiple hops to multiple destinations.

Performance requirements ...

The system should also scale by number of files in transfer. It should have a robust, internet-like design where no one part can bring entire transfer network down. The system should be able to saturate any hardware (network, storage) on which it is deployed. It should also operates completely asynchronously from other systems, for example: data management systems; data production processes; or analysis tasks running on batch computing nodes.

** Current architecture/ system
*** Introduction
HEP environments are typically heterogenous at all levels of operation, although some standards in transfer tools and storage management are beginning to emerge. To develop PhEDEx we rely on a core set of tools and services that are most widely available. These tools and services are broadly cataegorized as part of storage management and transfer tools.

For storage management we rely on SRM (Storage Resource Management), which provides generic access to any storage resource. Superficially SRM provides a defined two step interaction through which a file can be obtained from any storage medium for which the user has a Storage URL (SURL), which contains the hostname of the medium. During an SRM transaction the client presents an SRM server with the SURL, is is returned a Transport URL (TURL) which indicates the current (possibly temporary) location of the file and a transport protocol that can be used to access it. This means that someone wishing to access a file at a given site does not need to know on which physical resource the file resides.

Catalogues map some global replica set identifer to some local identifer that can be used to access the file (SURL). 

A number of tools are used for point-to-point transfer of data. WAN transfers are made with tools that overlie gsiFTP- basically FTP with X509 certificate authentication. Such tools are globus-url-copy and srmcp. PhEDEx also uses a number of site-local transfer and tape management commands. This variety of tools is accomodated with a plug-in component design that allows the system to provide generic components that can be easily configured to use local tools when necessary.

*** System design
PhEDEx design and development is guided by a number of architectural principles. The key principle is that complex functionality should be kept in discrete units, and that the handover of responsibility for sections of the transfer workflow should be handed between those units using as few messages- containing minimal information- as possible. As the system is active, with stateful components, it is reasonable to model these units after agents rather than as passive stateless services.

The PhEDEx design also maintains abstractions at different layers. Our experience with data management tools has led us to view many operations and tools as unreliable. To manage this we wrap unreliable tools and systems in more robust layers to build a reliable data management system.

< phedex-layers.eps here? >

In addition, PhEDEx allows site managers to keep local information local, and allow as much local management of files as possible. This makes the system robust during local infrastructural change, but means that local and global information can become unsyncronised after deletion operations.

To guide development we decided to avoid creating servers that aren't necessary, generally by accepting a small increase in complexity on the "client" transfer components. To make message passing simple we ustilise a blackboard architecture in which transfer components post state information. The blackboard was deployed as a single high availability database. We also leverage data chunking to imporove performance by avoiding somewhat traditional file to site mappings, instead mapping files to filegroups and filegroups to sites; we don't however, require a chunk to exist anywhere in entirety. 

*** Overview of transfer operations
The core element of the data management system is a node- a logical entity to which persistent processes named agents are associated. Typical examples might be a regional centre's transfer node, with agents to, for example, manage transfers from the centre's neighbours to the centre's disk buffers; or a mass storage node with an agent to manage the clustering of requests from other nodes for files for transfer.

Transfer workflows are divided into functionally coherent units- file transfer, for example, or migration to tape. Within those units workflow stages are defined as internal state transitions. Global state transitions like the handover of responsibility for a replication between units are seen as messages passed between components. By way of example the internal state transitions of a file transfer agent are: predelete, transfer, verify, postdelete, publish catalogue. The global state transitions are for operations like marking a number of files as wanted, as being ready for transfer or migrate, which represent the collaboration of system components to achieve their goals.

These global state transitions also enable efficient error handling: error states can be easily posted, and agents encountering error states know to adopt some defined strategy in dealing with them. 

The blackboard space that agents use to exchange messages/state information was envisaged as a database schema. This database schema tracks the list of files currently of interest to the data management system, useful metadata concerning the nodes in the system (names, routing information) and high level subscription information (which sites subscribe to which datasets, and which files are currently allocated to which specific node). It also tracks the current state of point to point transfers, and maintains an RLI like structure that maps global replica set identifiers to nodes in the distribution network.

Agents are based around an agent toolkit, which wraps much of the low level functionality in common to all the agents- for example, handling database connections in a robust manner, processing job queues etc. An explicit decision was made at the start of the project to wrap core message passing/ database access in a toolkit rather than to provide services that overlay the databases. The advantage of making agent code simpler through the deployment of services to handle chunks of complex functionality was deemed lower priority than the minimisation of maintenance and support effort required to maintain a performant service- especially when robust services to handle database interactions (e.g. Oracle and MySQL both provide services to allow remote database interaction- to a large extent there is no need to create a new service over the top, however thin). This approach was taken in interactions with both the TMDB and, where possible, local File Catalogues.

** Development, deployment and operations
Experience: summary of number of sites, types of components, bandwidth available, storage available... deployment and access to catalogues.

Development (mostly) as perl scripts that call other transfer tools. 

Summary of actual transfer experience- typical speeds, typical issues that get in the way etc (local deployment of research components like SRM, dCache, tools that create zero size files and not overwrite etc ...)

** Evaluation of future architectures, necessary improvements to the system
Explicit handling of "priority" at various granularities: what does priority mean, why do we need it, moving to priority based replica management (straightforward extension of simple scheduling/subscription replica management).

Queueing: filling trucks with parcels headed toward the same destination... simple attempts at this.

Peer to peer: what does it mean in this case; would it be useful? How close are we to this already? What information is global, which is local? What information might we accept a cost for/ sliding cost based on need?

Optimizing replica management? Coupling to workload management systems/ computational grids?

Conclusions

